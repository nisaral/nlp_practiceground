{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pandas requests beautifulsoup4 scrapy datasets pdfminer.six clean-text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:14:53.257325Z","iopub.execute_input":"2025-03-09T13:14:53.257673Z","iopub.status.idle":"2025-03-09T13:15:08.406358Z","shell.execute_reply.started":"2025-03-09T13:14:53.257641Z","shell.execute_reply":"2025-03-09T13:15:08.404825Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\nCollecting scrapy\n  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting pdfminer.six\n  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\nCollecting clean-text\n  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.1.31)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\nCollecting Twisted>=21.7.0 (from scrapy)\n  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (44.0.1)\nCollecting cssselect>=0.9.1 (from scrapy)\n  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\nCollecting itemloaders>=1.0.1 (from scrapy)\n  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\nCollecting parsel>=1.5.0 (from scrapy)\n  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (25.0.0)\nCollecting queuelib>=1.4.2 (from scrapy)\n  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\nCollecting service-identity>=18.1.0 (from scrapy)\n  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\nCollecting w3lib>=1.17.0 (from scrapy)\n  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting zope.interface>=5.1.0 (from scrapy)\n  Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\nCollecting itemadapter>=0.1.0 (from scrapy)\n  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2)\nCollecting tldextract (from scrapy)\n  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.3.0)\nRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\nCollecting PyDispatcher>=2.0.5 (from scrapy)\n  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nCollecting emoji<2.0.0,>=1.0.0 (from clean-text)\n  Downloading emoji-1.7.0.tar.gz (175 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy<7.0,>=6.0 (from clean-text)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\nRequirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\nCollecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\nCollecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\nCollecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\nCollecting requests-file>=1.4 (from tldextract->scrapy)\n  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\nRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nDownloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading clean_text-0.6.0-py3-none-any.whl (11 kB)\nDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\nDownloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\nDownloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\nDownloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\nDownloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\nDownloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\nDownloading service_identity-24.2.0-py3-none-any.whl (11 kB)\nDownloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\nDownloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\nDownloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\nDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=d22e3a1cf1d6d0a4b92eac85d1f8a4310c009a124932920059c879685df5064f\n  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\nSuccessfully built emoji\nInstalling collected packages: PyDispatcher, emoji, zope.interface, w3lib, queuelib, protego, itemadapter, incremental, hyperlink, ftfy, cssselect, constantly, automat, Twisted, requests-file, parsel, clean-text, tldextract, service-identity, pdfminer.six, itemloaders, scrapy\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.14.1\n    Uninstalling emoji-2.14.1:\n      Successfully uninstalled emoji-2.14.1\nSuccessfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 clean-text-0.6.0 constantly-23.10.4 cssselect-1.2.0 emoji-1.7.0 ftfy-6.3.1 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 parsel-1.10.0 pdfminer.six-20240706 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom datasets import load_dataset\nfrom pdfminer.high_level import extract_text\nimport os\nimport re\nfrom cleantext import clean\n\n# Languages and target sample size per language\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\"]\nTARGET_PER_LANG = 750000  # Aiming for ~5.25 million total\nMIN_WORDS = 50\n\n# Function to filter text by word count\ndef filter_text_length(text):\n    if not isinstance(text, str):  # Handle non-string inputs\n        return False\n    words = text.split()\n    return len(words) >= MIN_WORDS\n\n# Function 1: Load from Public Datasets\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    corpus = []\n    \n    # Try IndicCorp (corrected name or fallback)\n    try:\n        # Note: IndicCorp might need manual download from AI4Bharat; using OSCAR as a fallback\n        dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_hi\", split=\"train\")  # Hindi example\n        df = pd.DataFrame(dataset)\n        filtered_df = df[df['text'].apply(filter_text_length)].sample(n=min(TARGET_PER_LANG, len(df)), random_state=42)\n        filtered_df = filtered_df[['text']].assign(language=\"Hindi\")\n        corpus.append(filtered_df)\n        print(\"Loaded Hindi from OSCAR\")\n    except Exception as e:\n        print(f\"Error loading OSCAR for Hindi: {e}\")\n    \n    # Add other languages from OSCAR (example for Tamil)\n    try:\n        dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_ta\", split=\"train\")\n        df = pd.DataFrame(dataset)\n        filtered_df = df[df['text'].apply(filter_text_length)].sample(n=min(TARGET_PER_LANG, len(df)), random_state=42)\n        filtered_df = filtered_df[['text']].assign(language=\"Tamil\")\n        corpus.append(filtered_df)\n        print(\"Loaded Tamil from OSCAR\")\n    except Exception as e:\n        print(f\"Error loading OSCAR for Tamil: {e}\")\n\n    # Manually add mappings for others if OSCAR works (Marathi: mr, Gujarati: gu, Bengali: bn, Kannada: kn)\n    oscar_langs = {\"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"kn\": \"Kannada\"}\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\")\n            df = pd.DataFrame(dataset)\n            filtered_df = df[df['text'].apply(filter_text_length)].sample(n=min(TARGET_PER_LANG, len(df)), random_state=42)\n            filtered_df = filtered_df[['text']].assign(language=lang)\n            corpus.append(filtered_df)\n            print(f\"Loaded {lang} from OSCAR\")\n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n\n    # Sindhi might not be in OSCAR easily; skip or source elsewhere\n    \n    # Combine and save\n    if corpus:\n        combined_df = pd.concat(corpus).drop_duplicates(subset=\"text\")\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Saved {len(combined_df)} samples from datasets to {output_file}\")\n        return combined_df\n    else:\n        print(\"No datasets loaded successfully.\")\n        return pd.DataFrame(columns=[\"text\", \"language\"])  # Return empty DataFrame\n\n# Function 2: Web Scraping\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"]\n    }\n    \n    corpus = []\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    \n    for lang, urls in sites.items():\n        lang_texts = []\n        for url in urls:\n            try:\n                response = requests.get(url, headers=headers, timeout=10)\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                paragraphs = soup.find_all(\"p\")\n                for p in paragraphs:\n                    text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True)\n                    if filter_text_length(text):\n                        lang_texts.append({\"text\": text, \"language\": lang})\n            except Exception as e:\n                print(f\"Error scraping {url}: {e}\")\n        \n        lang_df = pd.DataFrame(lang_texts).sample(n=min(TARGET_PER_LANG, len(lang_texts)), random_state=42)\n        corpus.append(lang_df)\n    \n    if corpus:\n        combined_df = pd.concat(corpus).drop_duplicates(subset=\"text\")\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Saved {len(combined_df)} samples from web to {output_file}\")\n        return combined_df\n    return pd.DataFrame(columns=[\"text\", \"language\"])\n\n# Function 3: OCR from PDFs\ndef extract_from_pdfs(pdf_dir=\"pdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    corpus = []\n    \n    for lang in LANGUAGES:\n        lang_texts = []\n        lang_dir = os.path.join(pdf_dir, lang.lower())\n        if not os.path.exists(lang_dir):\n            print(f\"No PDFs found for {lang}, skipping...\")\n            continue\n        \n        for pdf_file in os.listdir(lang_dir):\n            if pdf_file.endswith(\".pdf\"):\n                try:\n                    text = extract_text(os.path.join(lang_dir, pdf_file))\n                    paragraphs = text.split(\"\\n\\n\")\n                    for para in paragraphs:\n                        cleaned_text = clean(para, no_line_breaks=True, no_urls=True, no_emails=True)\n                        if filter_text_length(cleaned_text):\n                            lang_texts.append({\"text\": cleaned_text, \"language\": lang})\n                except Exception as e:\n                    print(f\"Error processing {pdf_file}: {e}\")\n        \n        lang_df = pd.DataFrame(lang_texts).sample(n=min(TARGET_PER_LANG, len(lang_texts)), random_state=42)\n        corpus.append(lang_df)\n    \n    if corpus:\n        combined_df = pd.concat(corpus).drop_duplicates(subset=\"text\")\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Saved {len(combined_df)} samples from PDFs to {output_file}\")\n        return combined_df\n    return pd.DataFrame(columns=[\"text\", \"language\"])\n\n# Main function to build the corpus\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    \n    dataset_df = load_from_datasets()\n    scraped_df = scrape_from_web()\n    pdf_df = extract_from_pdfs()\n    \n    all_dfs = [df for df in [dataset_df, scraped_df, pdf_df] if not df.empty]\n    if not all_dfs:\n        print(\"No data collected!\")\n        return\n    \n    combined_df = pd.concat(all_dfs).drop_duplicates(subset=\"text\")\n    \n    balanced_corpus = []\n    for lang in LANGUAGES:\n        lang_df = combined_df[combined_df['language'] == lang]\n        sampled_df = lang_df.sample(n=min(TARGET_PER_LANG, len(lang_df)), random_state=42)\n        balanced_corpus.append(sampled_df)\n    \n    final_df = pd.concat(balanced_corpus).sample(frac=1, random_state=42)\n    \n    if len(final_df) < 5000000:\n        print(f\"Warning: Only {len(final_df)} samples collected, below 5 million!\")\n    else:\n        print(f\"Success: Collected {len(final_df)} samples!\")\n    \n    final_df.to_csv(output_file, index=False)\n    print(f\"Corpus saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:18:57.177084Z","iopub.execute_input":"2025-03-09T13:18:57.177364Z","iopub.status.idle":"2025-03-09T13:37:09.663927Z","shell.execute_reply.started":"2025-03-09T13:18:57.177341Z","shell.execute_reply":"2025-03-09T13:37:09.660992Z"}},"outputs":[{"name":"stdout","text":"Building the corpus...\nLoading from public datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff9d41716e74f52980d2c0b0af10a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b21052f3b34e27876aa3f828ea6fd8"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for oscar contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/oscar.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d46243b0568452fb9a10cc2673d6d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f3b4bd8a02413e8898e0c0a1320daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/409M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1288f145083848d381bbb2bd9d28760d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/406M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"549830f99635441599c864c257a4518a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae58116899444cf9b0d00349abc15c8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e96cb5622cc24657aa41703282e52537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1909387 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9485788c53049bb95bd0925653a919c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"327dee508bb14e7197a37fc7434b70e4"}},"metadata":{}},{"name":"stdout","text":"Loaded Hindi from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/246 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5f91935d4844799b57cfbcbe0e95d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/342M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796727a98e7a4b20acbaf6840f4a74f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4efc7adb4d9847a4b3ea0a2574a13219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/285M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83be01213b344672856654af72ddf2ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/833101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"999b471058464cd8a9871e52ca7a2aa7"}},"metadata":{}},{"name":"stdout","text":"Error loading OSCAR for Tamil: Cannot take a larger sample than population when 'replace=False'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabf2658d0594422aa9ae03b782d0648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae432162bfa4fb0b015bb8b1a96a73a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/212556 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"211a78b03a0242c993142496f8c78bea"}},"metadata":{}},{"name":"stdout","text":"Error loading OSCAR for Marathi: Cannot take a larger sample than population when 'replace=False'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f0d866bf2b48f090b42d82fd364479"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/163M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af310bfa2dd4aa392bd746744154f8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/169834 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471ad9daef9248908534d9341e86787b"}},"metadata":{}},{"name":"stdout","text":"Error loading OSCAR for Gujarati: Cannot take a larger sample than population when 'replace=False'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/328 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e9ff21944f740b1b469b35447f4429c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed931619baff44e6a7460a3c28cbe01b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64932e460ced464b8d8b0541592b9e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/391M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b228cf8e9b42b7bcf21d3a27f56230"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/86.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b71cecd85481492da7ecc8a2cceb2dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1114481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0652ca08a41d48aba7b2d78f144e15bd"}},"metadata":{}},{"name":"stdout","text":"Loaded Bengali from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baff5b3777a04ff5a17f20074cc4914f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7204daca0ad14a368f8e093fde1a1353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/251064 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b69f8d5096c4c758df8775b3465588c"}},"metadata":{}},{"name":"stdout","text":"Error loading OSCAR for Kannada: Cannot take a larger sample than population when 'replace=False'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-de12626dcbc3>\u001b[0m in \u001b[0;36m<cell line: 182>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mbuild_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-de12626dcbc3>\u001b[0m in \u001b[0;36mbuild_corpus\u001b[0;34m(output_file)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Building the corpus...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mdataset_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mscraped_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_from_web\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mpdf_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_from_pdfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-de12626dcbc3>\u001b[0m in \u001b[0;36mload_from_datasets\u001b[0;34m(output_file)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mcombined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved {len(combined_df)} samples from datasets to {output_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcombined_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 )\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3965\u001b[0m         )\n\u001b[1;32m   3966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3967\u001b[0;31m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[1;32m   3968\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineterminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         )\n\u001b[0;32m-> 1014\u001b[0;31m         \u001b[0mcsv_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_need_to_save_header\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstart_i\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslicer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_number_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         libwriters.write_csv_rows(\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mwriters.pyx\u001b[0m in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;31mError\u001b[0m: need to escape, but no escapechar set"],"ename":"Error","evalue":"need to escape, but no escapechar set","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}