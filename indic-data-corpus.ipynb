{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10679429,"sourceType":"datasetVersion","datasetId":6615670}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:16.708605Z","iopub.execute_input":"2025-03-20T14:00:16.708897Z","iopub.status.idle":"2025-03-20T14:00:16.807475Z","shell.execute_reply.started":"2025-03-20T14:00:16.708867Z","shell.execute_reply":"2025-03-20T14:00:16.806643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T08:31:30.538537Z","iopub.execute_input":"2025-03-20T08:31:30.538900Z","iopub.status.idle":"2025-03-20T08:31:31.855574Z","shell.execute_reply.started":"2025-03-20T08:31:30.538868Z","shell.execute_reply":"2025-03-20T08:31:31.853998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get update && apt-get install -y poppler-utils tesseract-ocr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:28.518368Z","iopub.execute_input":"2025-03-20T14:00:28.518650Z","iopub.status.idle":"2025-03-20T14:00:48.221644Z","shell.execute_reply.started":"2025-03-20T14:00:28.518629Z","shell.execute_reply":"2025-03-20T14:00:48.220553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pandas requests beautifulsoup4 scrapy datasets pdfminer.six clean-text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:48.222872Z","iopub.execute_input":"2025-03-20T14:00:48.223121Z","iopub.status.idle":"2025-03-20T14:01:03.547210Z","shell.execute_reply.started":"2025-03-20T14:00:48.223099Z","shell.execute_reply":"2025-03-20T14:01:03.546155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:01:03.548831Z","iopub.execute_input":"2025-03-20T14:01:03.549173Z","iopub.status.idle":"2025-03-20T14:01:09.534313Z","shell.execute_reply.started":"2025-03-20T14:01:03.549142Z","shell.execute_reply":"2025-03-20T14:01:09.533216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\n\n# Languages and target sample size per language\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\"]\nTARGET_PER_LANG = 250000  # Reduced from 750000 to prevent memory issues\nMIN_WORDS = 50\n\n# Function to filter text by word count\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    words = text.split()\n    return len(words) >= MIN_WORDS\n\n# Preprocess image for OCR\ndef preprocess_image(image):\n    \"\"\"Preprocess scanned images to improve OCR accuracy.\"\"\"\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    processed_image = cv2.dilate(binary, kernel, iterations=1)\n    return Image.fromarray(processed_image)\n\n# Extract text from PDF using OCR with better memory management\ndef extract_text_from_pdf(pdf_path, batch_size=10):  # Reduced batch size\n    \"\"\"Extract text from scanned PDFs using OCR in batches.\"\"\"\n    try:\n        # Convert only the first 20 pages to prevent memory issues\n        images = convert_from_path(pdf_path, dpi=200, first_page=1, last_page=20)  \n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ) + \"\\n\"\n            # Force garbage collection after each batch\n            del batch_images\n            processed_image = None\n            gc.collect()\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\n# Detect language based on Unicode ranges\ndef detect_language_unicode(text):\n    \"\"\"Detects and labels language based on Unicode ranges.\"\"\"\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),  # Devanagari\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'), # Devanagari (overlap with Hindi)\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),  # Arabic script\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),    # Arabic script\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    \n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    \n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    \n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\n# Function to save dataframe in chunks to prevent memory issues\ndef save_df_in_chunks(df, output_file, chunk_size=10000):\n    \"\"\"Save large dataframe in chunks to avoid memory issues.\"\"\"\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, \n                    mode=mode, \n                    header=header, \n                    index=False, \n                    escapechar='\\\\', \n                    encoding='utf-8-sig')\n        # Clear memory\n        del chunk\n        gc.collect()\n    print(f\"Saved {len(df)} samples to {output_file} in chunks\")\n\n# Function 1: Load from Public Datasets (OSCAR) with memory optimization\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    all_samples = 0\n    oscar_langs = {\n        \"hi\": \"Hindi\",\n        \"mr\": \"Marathi\",\n        \"gu\": \"Gujarati\",\n        \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\",\n        \"kn\": \"Kannada\"\n    }\n    \n    # Clear any existing file\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    \n    for code, lang in oscar_langs.items():\n        try:\n            # Load and process in smaller chunks\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = min(10000, TARGET_PER_LANG)\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            \n            total_lang_samples = 0\n            for i in range(num_chunks):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, len(dataset))\n                \n                chunk_data = dataset[start_idx:end_idx]\n                chunk_df = pd.DataFrame(chunk_data)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                \n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    # Write chunk to file\n                    mode = 'a' if i > 0 or all_samples > 0 else 'w'\n                    header = i == 0 and all_samples == 0\n                    filtered_df.to_csv(output_file, \n                                      mode=mode, \n                                      header=header, \n                                      index=False, \n                                      escapechar='\\\\', \n                                      encoding='utf-8-sig')\n                    \n                    samples_added = len(filtered_df)\n                    total_lang_samples += samples_added\n                    all_samples += samples_added\n                \n                # Clear memory\n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                \n                # Stop if we've collected enough samples\n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n                    \n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n            \n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    \n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\n# Function 2: Web Scraping with memory optimization\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"]\n    }\n    \n    # Clear any existing file\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    \n    for lang, urls in sites.items():\n        lang_texts = []\n        max_samples_per_url = TARGET_PER_LANG // len(urls)\n        \n        for url in urls:\n            try:\n                response = requests.get(url, headers=headers, timeout=10)\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                paragraphs = soup.find_all(\"p\")\n                \n                for p in paragraphs:\n                    if len(lang_texts) >= max_samples_per_url:\n                        break\n                        \n                    text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True)\n                    if filter_text_length(text):\n                        lang_texts.append({\"text\": text, \"language\": lang})\n                        \n                # Save immediately if we have enough data\n                if len(lang_texts) >= max_samples_per_url:\n                    break\n                    \n            except Exception as e:\n                print(f\"Error scraping {url}: {e}\")\n        \n        # Save language data\n        if lang_texts:\n            lang_df = pd.DataFrame(lang_texts)\n            mode = 'a' if all_samples > 0 else 'w'\n            header = all_samples == 0\n            lang_df.to_csv(output_file, \n                          mode=mode, \n                          header=header, \n                          index=False, \n                          escapechar='\\\\', \n                          encoding='utf-8-sig')\n            \n            all_samples += len(lang_df)\n            print(f\"Scraped {lang}: {len(lang_df)} samples\")\n            \n            # Clear memory\n            del lang_df, lang_texts\n            gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\n# Function 3: OCR from PDFs using Kaggle path with memory optimization\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    \n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    \n    # Clear any existing file\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    # Limit to 10 PDFs max to prevent memory issues\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:10]\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        \n        try:\n            # Try pdfminer for searchable PDFs first\n            text = extract_text(pdf_path)\n            if not text.strip():  # If empty, use OCR\n                text = extract_text_from_pdf(pdf_path, batch_size=5)  # Reduced batch size\n            \n            if not text:\n                continue\n            \n            # Split into paragraphs\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            \n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, lang_probs, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:  # Only include target languages\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            \n            # Save immediately after each PDF to prevent memory buildup\n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, \n                             mode=mode, \n                             header=header, \n                             index=False, \n                             escapechar='\\\\', \n                             encoding='utf-8-sig')\n                \n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                \n                # Clear memory\n                del pdf_df, pdf_corpus\n                gc.collect()\n            \n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n        \n        # Clear more memory\n        text = None\n        paragraphs = None\n        gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\n# Main function to build the corpus with memory optimizations\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    \n    # Process each source individually and merge at the end\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    # If we have data from any source, merge and balance\n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        \n        # Create a new output file\n        open(output_file, 'w', encoding='utf-8-sig').close()\n        \n        # Process each language separately\n        for lang in LANGUAGES:\n            lang_samples = []\n            \n            # Process each source file in chunks to avoid memory issues\n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    # Read in chunks\n                    chunk_size = 10000\n                    for chunk in pd.read_csv(source_file, \n                                            chunksize=chunk_size, \n                                            encoding='utf-8-sig', \n                                            escapechar='\\\\'):\n                        # Filter for current language\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            \n                            # If we have enough samples, stop reading\n                            total_samples = sum(len(df) for df in lang_samples)\n                            if total_samples >= TARGET_PER_LANG:\n                                break\n                    \n                    # Clear memory\n                    gc.collect()\n            \n            # Combine all chunks for this language\n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                available_samples = len(combined_lang)\n                \n                # Sample if we have more than needed\n                if available_samples > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                \n                # Save this language to the final file\n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, \n                                    mode=mode, \n                                    header=header, \n                                    index=False, \n                                    escapechar='\\\\', \n                                    encoding='utf-8-sig')\n                \n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                \n                # Clear memory\n                del combined_lang, lang_samples\n                gc.collect()\n            else:\n                print(f\"No samples found for {lang}\")\n        \n        # Get final count\n        total_lines = 0\n        with open(output_file, 'r', encoding='utf-8-sig') as f:\n            for _ in f:\n                total_lines += 1\n        \n        total_lines -= 1  # Subtract header line\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        \n        # Clean up temporary files\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:06:57.658914Z","iopub.execute_input":"2025-03-10T17:06:57.659297Z","iopub.status.idle":"2025-03-10T17:44:04.813424Z","shell.execute_reply.started":"2025-03-10T17:06:57.659268Z","shell.execute_reply":"2025-03-10T17:44:04.811934Z"}}},{"cell_type":"markdown","source":"df = pd.read_csv(\"indic_corpus.csv\", encoding='utf-8-sig', escapechar='\\\\')","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:46:28.961732Z","iopub.execute_input":"2025-03-10T17:46:28.962301Z","iopub.status.idle":"2025-03-10T17:48:35.670113Z","shell.execute_reply.started":"2025-03-10T17:46:28.962263Z","shell.execute_reply":"2025-03-10T17:48:35.668976Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install -y tesseract-ocr tesseract-ocr-hin tesseract-ocr-mar tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-kan tesseract-ocr-eng tesseract-ocr-urd tesseract-ocr-chi-sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:31.700099Z","iopub.execute_input":"2025-03-20T14:02:31.700473Z","iopub.status.idle":"2025-03-20T14:02:38.713557Z","shell.execute_reply.started":"2025-03-20T14:02:31.700442Z","shell.execute_reply":"2025-03-20T14:02:38.712489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdf2image pytesseract opencv-python pillow clean-text langdetect datasets requests beautifulsoup4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:38.715217Z","iopub.execute_input":"2025-03-20T14:02:38.715600Z","iopub.status.idle":"2025-03-20T14:02:42.161361Z","shell.execute_reply.started":"2025-03-20T14:02:38.715560Z","shell.execute_reply":"2025-03-20T14:02:42.160350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdfminer pdf2image pytesseract langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:42.163299Z","iopub.execute_input":"2025-03-20T14:02:42.163627Z","iopub.status.idle":"2025-03-20T14:03:07.039191Z","shell.execute_reply.started":"2025-03-20T14:02:42.163592Z","shell.execute_reply":"2025-03-20T14:03:07.038381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdfminer.six==20231228\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:07:00.074390Z","iopub.execute_input":"2025-03-20T14:07:00.074773Z","iopub.status.idle":"2025-03-20T14:07:04.442186Z","shell.execute_reply.started":"2025-03-20T14:07:00.074748Z","shell.execute_reply":"2025-03-20T14:07:04.440969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install cleantext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:03:10.534805Z","iopub.execute_input":"2025-03-20T14:03:10.535056Z","iopub.status.idle":"2025-03-20T14:03:14.196236Z","shell.execute_reply.started":"2025-03-20T14:03:10.535032Z","shell.execute_reply":"2025-03-20T14:03:14.195167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:12:23.074577Z","iopub.execute_input":"2025-03-10T15:12:23.074905Z","iopub.status.idle":"2025-03-10T15:12:27.721704Z","shell.execute_reply.started":"2025-03-10T15:12:23.074883Z","shell.execute_reply":"2025-03-10T15:12:27.720648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\n# Fix for pdfminer import issue\ntry:\n    from pdfminer.high_level import extract_text\nexcept ImportError:\n    # Create a fallback function if pdfminer fails to import properly\n    def extract_text(pdf_path):\n        print(f\"Using fallback extraction for {pdf_path} due to pdfminer import issue\")\n        return \"\"\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\nimport concurrent.futures\nimport time\n\n# Expanded languages\nLANGUAGES = [\n    \"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\",\n    \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"\n]\nTARGET_PER_LANG = 250000  # Reduced for faster runtime\nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    return len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    return Image.fromarray(cv2.dilate(binary, kernel, iterations=1))\n\ndef extract_text_from_pdf(pdf_path, batch_size=5):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        start_time = time.time()\n        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=10)  # Reduced DPI and pages\n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ).replace('\\0', '')[:100000] + \"\\n\"  # Clean NULL, truncate\n            del batch_images\n            gc.collect()\n        print(f\"Finished {pdf_path} in {time.time() - start_time:.2f}s\")\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\ndef detect_language_unicode(text):\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Telugu': sum(1 for char in text if '\\u0C00' <= char <= '\\u0C7F'),\n        'Malayalam': sum(1 for char in text if '\\u0D00' <= char <= '\\u0D7F'),\n        'Punjabi': sum(1 for char in text if '\\u0A00' <= char <= '\\u0A7F'),\n        'Odia': sum(1 for char in text if '\\u0B00' <= char <= '\\u0B7F'),\n        'Assamese': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\ndef save_df_in_chunks(df, output_file, chunk_size=5000):\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n        del chunk\n        gc.collect()\n\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    oscar_langs = {\n        \"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\",\n        \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"\n    }\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    for code, lang in oscar_langs.items():\n        try:\n            print(f\"Loading OSCAR for {lang}...\")\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = 5000\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            total_lang_samples = 0\n            \n            for i in range(num_chunks):\n                chunk_data = dataset[i * chunk_size:(i + 1) * chunk_size]\n                chunk_df = pd.DataFrame(chunk_data)\n                chunk_df['text'] = chunk_df['text'].apply(lambda x: x.replace('\\0', '')[:100000] if isinstance(x, str) else x)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    mode = 'a' if all_samples > 0 else 'w'\n                    header = all_samples == 0\n                    filtered_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                    total_lang_samples += len(filtered_df)\n                    all_samples += len(filtered_df)\n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\ndef scrape_url(url, lang, max_samples):\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    lang_texts = []\n    try:\n        response = requests.get(url, headers=headers, timeout=5)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paragraphs = soup.find_all(\"p\")\n        for p in paragraphs:\n            if len(lang_texts) >= max_samples:\n                break\n            text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True).replace('\\0', '')[:100000]\n            if filter_text_length(text):\n                lang_texts.append({\"text\": text, \"language\": lang})\n        print(f\"Scraped {url}: {len(lang_texts)} samples\")\n    except Exception as e:\n        print(f\"Error scraping {url}: {e}\")\n    return lang_texts\n\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"],\n        \"Telugu\": [\"https://eenadu.net\", \"https://sakshi.com\"],\n        \"Malayalam\": [\"https://mathrubhumi.com\", \"https://www.manoramaonline.com\"],\n        \"Punjabi\": [\"https://punjabitribuneonline.com\", \"https://www.ajitjalandhar.com\"],\n        \"Odia\": [\"https://sambad.in\", \"https://dharitri.com\"],\n        \"Assamese\": [\"https://asomiyapratidin.in\", \"https://pratidintime.com\"]\n    }\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        for lang, urls in sites.items():\n            max_samples_per_url = TARGET_PER_LANG // len(urls)\n            future_to_url = {executor.submit(scrape_url, url, lang, max_samples_per_url): url for url in urls}\n            lang_texts = []\n            for future in concurrent.futures.as_completed(future_to_url):\n                lang_texts.extend(future.result())\n            if lang_texts:\n                lang_df = pd.DataFrame(lang_texts)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                lang_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                all_samples += len(lang_df)\n                print(f\"Scraped {lang}: {len(lang_df)} samples\")\n                del lang_df, lang_texts\n                gc.collect()\n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:5]  # Limit to 5 PDFs\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        try:\n            # Modified to handle pdfminer error more gracefully\n            try:\n                text = extract_text(pdf_path)\n            except Exception as e:\n                print(f\"Primary PDF extraction failed: {e}, using backup method\")\n                text = \"\"\n                \n            if not text.strip():\n                text = extract_text_from_pdf(pdf_path)\n            if not text:\n                continue\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, _, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                del pdf_df, pdf_corpus\n                gc.collect()\n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()\n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()\n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()\n    \n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        open(output_file, 'w', encoding='utf-8-sig').close()\n        for lang in LANGUAGES:\n            lang_samples = []\n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    for chunk in pd.read_csv(source_file, chunksize=5000, encoding='utf-8-sig', escapechar='\\\\'):\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            if sum(len(df) for df in lang_samples) >= TARGET_PER_LANG:\n                                break\n                    gc.collect()\n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                if len(combined_lang) > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                del combined_lang, lang_samples\n                gc.collect()\n        total_lines = sum(1 for _ in open(output_file, 'r', encoding='utf-8-sig')) - 1  # Subtract header\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:16:57.249315Z","iopub.execute_input":"2025-03-20T14:16:57.249660Z","iopub.status.idle":"2025-03-20T15:07:41.554729Z","shell.execute_reply.started":"2025-03-20T14:16:57.249638Z","shell.execute_reply":"2025-03-20T15:07:41.533771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gzip\nimport pandas as pd\n\n# Decompress and clean NULL bytes\nwith gzip.open('/kaggle/working/dataset_corpus.csv.gz', 'rb') as f_in:\n    with open('/kaggle/working/dataset_corpus_clean.csv', 'wb') as f_out:\n        # Read raw bytes, replace NULL bytes\n        data = f_in.read().replace(b'\\0', b'')\n        f_out.write(data)\n\n# Load cleaned CSV\ndf = pd.read_csv('/kaggle/working/dataset_corpus_clean.csv', engine='python', escapechar='\\\\')\nprint(f\"OSCAR samples: {len(df)}\")\nscraped_df = scrape_from_web()\npdf_df = extract_from_pdfs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T10:38:56.438622Z","iopub.execute_input":"2025-03-20T10:38:56.438913Z","execution_failed":"2025-03-20T10:44:10.381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nimport gc\nimport time\n\nLANGUAGES = [\n    \"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\",\n    \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"\n]\nTARGET_TOTAL = 500000  # 500K from PDFs\nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    return len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    return Image.fromarray(cv2.dilate(binary, kernel, iterations=1))\n\ndef extract_text_from_pdf(pdf_path, batch_size=5):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        start_time = time.time()\n        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=20)  # More pages\n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ).replace('\\0', '')[:100000] + \"\\n\"\n            del batch_images\n            gc.collect()\n        print(f\"Finished {pdf_path} in {time.time() - start_time:.2f}s\")\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\ndef detect_language_unicode(text):\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Telugu': sum(1 for char in text if '\\u0C00' <= char <= '\\u0C7F'),\n        'Malayalam': sum(1 for char in text if '\\u0D00' <= char <= '\\u0D7F'),\n        'Punjabi': sum(1 for char in text if '\\u0A00' <= char <= '\\u0A7F'),\n        'Odia': sum(1 for char in text if '\\u0B00' <= char <= '\\u0B7F'),\n        'Assamese': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv.gz\"):\n    print(\"Extracting from PDFs...\")\n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    for f in os.listdir(\"/kaggle/working\"):\n        if f.endswith((\".csv\", \".gz\")):\n            os.remove(os.path.join(\"/kaggle/working\", f))\n    if os.path.exists(output_file):\n        os.remove(output_file)\n    all_samples = 0\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")]  # Process all\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        try:\n            text = extract_text(pdf_path)\n            if not text.strip():\n                text = extract_text_from_pdf(pdf_path)\n            if not text:\n                continue\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, _, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig', compression='gzip')\n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                del pdf_df, pdf_corpus\n                gc.collect()\n                if all_samples >= TARGET_TOTAL:\n                    break\n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\nif __name__ == \"__main__\":\n    extract_from_pdfs()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}