{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10679429,"sourceType":"datasetVersion","datasetId":6615670}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install poppler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:01:44.867533Z","iopub.execute_input":"2025-03-10T18:01:44.867953Z","iopub.status.idle":"2025-03-10T18:01:46.342049Z","shell.execute_reply.started":"2025-03-10T18:01:44.867919Z","shell.execute_reply":"2025-03-10T18:01:46.340533Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement poppler (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for poppler\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install pandas requests beautifulsoup4 scrapy datasets pdfminer.six clean-text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:05:42.244066Z","iopub.execute_input":"2025-03-10T17:05:42.244536Z","iopub.status.idle":"2025-03-10T17:05:59.071229Z","shell.execute_reply.started":"2025-03-10T17:05:42.244491Z","shell.execute_reply":"2025-03-10T17:05:59.070119Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\nCollecting scrapy\n  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting pdfminer.six\n  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\nCollecting clean-text\n  Downloading clean_text-0.6.0-py3-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.1.31)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\nCollecting Twisted>=21.7.0 (from scrapy)\n  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (44.0.1)\nCollecting cssselect>=0.9.1 (from scrapy)\n  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting itemloaders>=1.0.1 (from scrapy)\n  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\nCollecting parsel>=1.5.0 (from scrapy)\n  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (25.0.0)\nCollecting queuelib>=1.4.2 (from scrapy)\n  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\nCollecting service-identity>=18.1.0 (from scrapy)\n  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\nCollecting w3lib>=1.17.0 (from scrapy)\n  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting zope.interface>=5.1.0 (from scrapy)\n  Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\nCollecting itemadapter>=0.1.0 (from scrapy)\n  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.2)\nCollecting tldextract (from scrapy)\n  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.3.0)\nRequirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.7.1)\nCollecting PyDispatcher>=2.0.5 (from scrapy)\n  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nCollecting emoji<2.0.0,>=1.0.0 (from clean-text)\n  Downloading emoji-1.7.0.tar.gz (175 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy<7.0,>=6.0 (from clean-text)\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\nRequirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\nCollecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\nCollecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\nCollecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\nCollecting requests-file>=1.4 (from tldextract->scrapy)\n  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\nRequirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->pandas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->pandas) (2024.2.0)\nDownloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading clean_text-0.6.0-py3-none-any.whl (11 kB)\nDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\nDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\nDownloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\nDownloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\nDownloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\nDownloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\nDownloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\nDownloading service_identity-24.2.0-py3-none-any.whl (11 kB)\nDownloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\nDownloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\nDownloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\nDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=309618ea2b8e60430a628a361af55640cdf7b89f067903ff7b66cba94de65211\n  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\nSuccessfully built emoji\nInstalling collected packages: PyDispatcher, emoji, zope.interface, w3lib, queuelib, protego, itemadapter, incremental, hyperlink, ftfy, cssselect, constantly, automat, Twisted, requests-file, parsel, clean-text, tldextract, service-identity, pdfminer.six, itemloaders, scrapy\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.14.1\n    Uninstalling emoji-2.14.1:\n      Successfully uninstalled emoji-2.14.1\nSuccessfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 clean-text-0.6.0 constantly-23.10.4 cssselect-1.3.0 emoji-1.7.0 ftfy-6.3.1 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 parsel-1.10.0 pdfminer.six-20240706 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom datasets import load_dataset\nfrom pdfminer.high_level import extract_text\nimport os\nimport re\nfrom cleantext import clean\n\n# Languages and target sample size per language\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\"]\nTARGET_PER_LANG = 750000  # Aiming for ~5.25 million total\nMIN_WORDS = 50\n\n# Function to filter text by word count\ndef filter_text_length(text):\n    if not isinstance(text, str):  # Handle non-string inputs\n        return False\n    words = text.split()\n    return len(words) >= MIN_WORDS\n\n# Function 1: Load from Public Datasets\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    corpus = []\n    \n    # Try IndicCorp (corrected name or fallback)\n    try:\n        # Note: IndicCorp might need manual download from AI4Bharat; using OSCAR as a fallback\n        dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_hi\", split=\"train\")  # Hindi example\n        df = pd.DataFrame(dataset)\n        filtered_df = df[df['text'].apply(filter_text_length)].sample(n=min(TARGET_PER_LANG, len(df)), random_state=42)\n        filtered_df = filtered_df[['text']].assign(language=\"Hindi\")\n        corpus.append(filtered_df)\n        print(\"Loaded Hindi from OSCAR\")\n    except Exception as e:\n        print(f\"Error loading OSCAR for Hindi: {e}\")\n    \n    # Add other languages from OSCAR (example for Tamil)\n    try:\n        dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_ta\", split=\"train\")\n        df = pd.DataFrame(dataset)\n        filtered_df = df[df['text'].apply(filter_text_length)].sample(n=min(TARGET_PER_LANG, len(df)), random_state=42)\n        filtered_df = filtered_df[['text']].assign(language=\"Tamil\")\n        corpus.append(filtered_df)\n        print(\"Loaded Tamil from OSCAR\")\n    except Exception as e:\n        print(f\"Error loading OSCAR for Tamil: {e}\")\n\n    # Manually add mappings for others if OSCAR works (Marathi: mr, Gujarati: gu, Bengali: bn, Kannada: kn)\n    oscar_langs = {\"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"kn\": \"Kannada\"}\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\")\n            df = pd.DataFrame(dataset)\n            filtered_df = df[df['text'].apply(filter_text_length)].sample(n=min(TARGET_PER_LANG, len(df)), random_state=42)\n            filtered_df = filtered_df[['text']].assign(language=lang)\n            corpus.append(filtered_df)\n            print(f\"Loaded {lang} from OSCAR\")\n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n\n    # Sindhi might not be in OSCAR easily; skip or source elsewhere\n    \n    # Combine and save\n    if corpus:\n        combined_df = pd.concat(corpus).drop_duplicates(subset=\"text\")\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Saved {len(combined_df)} samples from datasets to {output_file}\")\n        return combined_df\n    else:\n        print(\"No datasets loaded successfully.\")\n        return pd.DataFrame(columns=[\"text\", \"language\"])  # Return empty DataFrame\n\n# Function 2: Web Scraping\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"]\n    }\n    \n    corpus = []\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    \n    for lang, urls in sites.items():\n        lang_texts = []\n        for url in urls:\n            try:\n                response = requests.get(url, headers=headers, timeout=10)\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                paragraphs = soup.find_all(\"p\")\n                for p in paragraphs:\n                    text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True)\n                    if filter_text_length(text):\n                        lang_texts.append({\"text\": text, \"language\": lang})\n            except Exception as e:\n                print(f\"Error scraping {url}: {e}\")\n        \n        lang_df = pd.DataFrame(lang_texts).sample(n=min(TARGET_PER_LANG, len(lang_texts)), random_state=42)\n        corpus.append(lang_df)\n    \n    if corpus:\n        combined_df = pd.concat(corpus).drop_duplicates(subset=\"text\")\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Saved {len(combined_df)} samples from web to {output_file}\")\n        return combined_df\n    return pd.DataFrame(columns=[\"text\", \"language\"])\n\n# Function 3: OCR from PDFs\ndef extract_from_pdfs(pdf_dir=\"pdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    corpus = []\n    \n    for lang in LANGUAGES:\n        lang_texts = []\n        lang_dir = os.path.join(pdf_dir, lang.lower())\n        if not os.path.exists(lang_dir):\n            print(f\"No PDFs found for {lang}, skipping...\")\n            continue\n        \n        for pdf_file in os.listdir(lang_dir):\n            if pdf_file.endswith(\".pdf\"):\n                try:\n                    text = extract_text(os.path.join(lang_dir, pdf_file))\n                    paragraphs = text.split(\"\\n\\n\")\n                    for para in paragraphs:\n                        cleaned_text = clean(para, no_line_breaks=True, no_urls=True, no_emails=True)\n                        if filter_text_length(cleaned_text):\n                            lang_texts.append({\"text\": cleaned_text, \"language\": lang})\n                except Exception as e:\n                    print(f\"Error processing {pdf_file}: {e}\")\n        \n        lang_df = pd.DataFrame(lang_texts).sample(n=min(TARGET_PER_LANG, len(lang_texts)), random_state=42)\n        corpus.append(lang_df)\n    \n    if corpus:\n        combined_df = pd.concat(corpus).drop_duplicates(subset=\"text\")\n        combined_df.to_csv(output_file, index=False)\n        print(f\"Saved {len(combined_df)} samples from PDFs to {output_file}\")\n        return combined_df\n    return pd.DataFrame(columns=[\"text\", \"language\"])\n\n# Main function to build the corpus\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    \n    dataset_df = load_from_datasets()\n    scraped_df = scrape_from_web()\n    pdf_df = extract_from_pdfs()\n    \n    all_dfs = [df for df in [dataset_df, scraped_df, pdf_df] if not df.empty]\n    if not all_dfs:\n        print(\"No data collected!\")\n        return\n    \n    combined_df = pd.concat(all_dfs).drop_duplicates(subset=\"text\")\n    \n    balanced_corpus = []\n    for lang in LANGUAGES:\n        lang_df = combined_df[combined_df['language'] == lang]\n        sampled_df = lang_df.sample(n=min(TARGET_PER_LANG, len(lang_df)), random_state=42)\n        balanced_corpus.append(sampled_df)\n    \n    final_df = pd.concat(balanced_corpus).sample(frac=1, random_state=42)\n    \n    if len(final_df) < 5000000:\n        print(f\"Warning: Only {len(final_df)} samples collected, below 5 million!\")\n    else:\n        print(f\"Success: Collected {len(final_df)} samples!\")\n    \n    final_df.to_csv(output_file, index=False)\n    print(f\"Corpus saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:18:57.177084Z","iopub.execute_input":"2025-03-09T13:18:57.177364Z","iopub.status.idle":"2025-03-09T13:37:09.663927Z","shell.execute_reply.started":"2025-03-09T13:18:57.177341Z","shell.execute_reply":"2025-03-09T13:37:09.660992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:06:50.072328Z","iopub.execute_input":"2025-03-10T17:06:50.072692Z","iopub.status.idle":"2025-03-10T17:06:54.660196Z","shell.execute_reply.started":"2025-03-10T17:06:50.072664Z","shell.execute_reply":"2025-03-10T17:06:54.658951Z"}},"outputs":[{"name":"stdout","text":"Collecting langdetect\n  Using cached langdetect-1.0.9-py3-none-any.whl\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\nInstalling collected packages: langdetect\nSuccessfully installed langdetect-1.0.9\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\n\n# Languages and target sample size per language\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\"]\nTARGET_PER_LANG = 250000  # Reduced from 750000 to prevent memory issues\nMIN_WORDS = 50\n\n# Function to filter text by word count\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    words = text.split()\n    return len(words) >= MIN_WORDS\n\n# Preprocess image for OCR\ndef preprocess_image(image):\n    \"\"\"Preprocess scanned images to improve OCR accuracy.\"\"\"\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    processed_image = cv2.dilate(binary, kernel, iterations=1)\n    return Image.fromarray(processed_image)\n\n# Extract text from PDF using OCR with better memory management\ndef extract_text_from_pdf(pdf_path, batch_size=10):  # Reduced batch size\n    \"\"\"Extract text from scanned PDFs using OCR in batches.\"\"\"\n    try:\n        # Convert only the first 20 pages to prevent memory issues\n        images = convert_from_path(pdf_path, dpi=200, first_page=1, last_page=20)  \n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ) + \"\\n\"\n            # Force garbage collection after each batch\n            del batch_images\n            processed_image = None\n            gc.collect()\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\n# Detect language based on Unicode ranges\ndef detect_language_unicode(text):\n    \"\"\"Detects and labels language based on Unicode ranges.\"\"\"\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),  # Devanagari\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'), # Devanagari (overlap with Hindi)\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),  # Arabic script\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),    # Arabic script\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    \n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    \n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    \n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\n# Function to save dataframe in chunks to prevent memory issues\ndef save_df_in_chunks(df, output_file, chunk_size=10000):\n    \"\"\"Save large dataframe in chunks to avoid memory issues.\"\"\"\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, \n                    mode=mode, \n                    header=header, \n                    index=False, \n                    escapechar='\\\\', \n                    encoding='utf-8-sig')\n        # Clear memory\n        del chunk\n        gc.collect()\n    print(f\"Saved {len(df)} samples to {output_file} in chunks\")\n\n# Function 1: Load from Public Datasets (OSCAR) with memory optimization\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    all_samples = 0\n    oscar_langs = {\n        \"hi\": \"Hindi\",\n        \"mr\": \"Marathi\",\n        \"gu\": \"Gujarati\",\n        \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\",\n        \"kn\": \"Kannada\"\n    }\n    \n    # Clear any existing file\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    \n    for code, lang in oscar_langs.items():\n        try:\n            # Load and process in smaller chunks\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = min(10000, TARGET_PER_LANG)\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            \n            total_lang_samples = 0\n            for i in range(num_chunks):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, len(dataset))\n                \n                chunk_data = dataset[start_idx:end_idx]\n                chunk_df = pd.DataFrame(chunk_data)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                \n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    # Write chunk to file\n                    mode = 'a' if i > 0 or all_samples > 0 else 'w'\n                    header = i == 0 and all_samples == 0\n                    filtered_df.to_csv(output_file, \n                                      mode=mode, \n                                      header=header, \n                                      index=False, \n                                      escapechar='\\\\', \n                                      encoding='utf-8-sig')\n                    \n                    samples_added = len(filtered_df)\n                    total_lang_samples += samples_added\n                    all_samples += samples_added\n                \n                # Clear memory\n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                \n                # Stop if we've collected enough samples\n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n                    \n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n            \n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    \n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\n# Function 2: Web Scraping with memory optimization\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"]\n    }\n    \n    # Clear any existing file\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    \n    for lang, urls in sites.items():\n        lang_texts = []\n        max_samples_per_url = TARGET_PER_LANG // len(urls)\n        \n        for url in urls:\n            try:\n                response = requests.get(url, headers=headers, timeout=10)\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                paragraphs = soup.find_all(\"p\")\n                \n                for p in paragraphs:\n                    if len(lang_texts) >= max_samples_per_url:\n                        break\n                        \n                    text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True)\n                    if filter_text_length(text):\n                        lang_texts.append({\"text\": text, \"language\": lang})\n                        \n                # Save immediately if we have enough data\n                if len(lang_texts) >= max_samples_per_url:\n                    break\n                    \n            except Exception as e:\n                print(f\"Error scraping {url}: {e}\")\n        \n        # Save language data\n        if lang_texts:\n            lang_df = pd.DataFrame(lang_texts)\n            mode = 'a' if all_samples > 0 else 'w'\n            header = all_samples == 0\n            lang_df.to_csv(output_file, \n                          mode=mode, \n                          header=header, \n                          index=False, \n                          escapechar='\\\\', \n                          encoding='utf-8-sig')\n            \n            all_samples += len(lang_df)\n            print(f\"Scraped {lang}: {len(lang_df)} samples\")\n            \n            # Clear memory\n            del lang_df, lang_texts\n            gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\n# Function 3: OCR from PDFs using Kaggle path with memory optimization\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    \n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    \n    # Clear any existing file\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    # Limit to 10 PDFs max to prevent memory issues\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:10]\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        \n        try:\n            # Try pdfminer for searchable PDFs first\n            text = extract_text(pdf_path)\n            if not text.strip():  # If empty, use OCR\n                text = extract_text_from_pdf(pdf_path, batch_size=5)  # Reduced batch size\n            \n            if not text:\n                continue\n            \n            # Split into paragraphs\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            \n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, lang_probs, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:  # Only include target languages\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            \n            # Save immediately after each PDF to prevent memory buildup\n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, \n                             mode=mode, \n                             header=header, \n                             index=False, \n                             escapechar='\\\\', \n                             encoding='utf-8-sig')\n                \n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                \n                # Clear memory\n                del pdf_df, pdf_corpus\n                gc.collect()\n            \n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n        \n        # Clear more memory\n        text = None\n        paragraphs = None\n        gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\n# Main function to build the corpus with memory optimizations\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    \n    # Process each source individually and merge at the end\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    # If we have data from any source, merge and balance\n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        \n        # Create a new output file\n        open(output_file, 'w', encoding='utf-8-sig').close()\n        \n        # Process each language separately\n        for lang in LANGUAGES:\n            lang_samples = []\n            \n            # Process each source file in chunks to avoid memory issues\n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    # Read in chunks\n                    chunk_size = 10000\n                    for chunk in pd.read_csv(source_file, \n                                            chunksize=chunk_size, \n                                            encoding='utf-8-sig', \n                                            escapechar='\\\\'):\n                        # Filter for current language\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            \n                            # If we have enough samples, stop reading\n                            total_samples = sum(len(df) for df in lang_samples)\n                            if total_samples >= TARGET_PER_LANG:\n                                break\n                    \n                    # Clear memory\n                    gc.collect()\n            \n            # Combine all chunks for this language\n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                available_samples = len(combined_lang)\n                \n                # Sample if we have more than needed\n                if available_samples > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                \n                # Save this language to the final file\n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, \n                                    mode=mode, \n                                    header=header, \n                                    index=False, \n                                    escapechar='\\\\', \n                                    encoding='utf-8-sig')\n                \n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                \n                # Clear memory\n                del combined_lang, lang_samples\n                gc.collect()\n            else:\n                print(f\"No samples found for {lang}\")\n        \n        # Get final count\n        total_lines = 0\n        with open(output_file, 'r', encoding='utf-8-sig') as f:\n            for _ in f:\n                total_lines += 1\n        \n        total_lines -= 1  # Subtract header line\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        \n        # Clean up temporary files\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:06:57.658914Z","iopub.execute_input":"2025-03-10T17:06:57.659297Z","iopub.status.idle":"2025-03-10T17:44:04.813424Z","shell.execute_reply.started":"2025-03-10T17:06:57.659268Z","shell.execute_reply":"2025-03-10T17:44:04.811934Z"}},"outputs":[{"name":"stdout","text":"Building the corpus...\nLoading from public datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee654646b61141ad8f792589ecd8ab66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbe1172c4c6d4d4baa8588302f168345"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c13c5121cb45049ab2fc2dd1af57bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548fae06eb964195a59226f99835a0c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/409M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c657d722284738bfd275fb9e3123e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/406M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255b057c9c8e4ee9a7d1defe38a33f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c4b30dc679462a867e679070720ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1826dbf9c2b48e08bb40f51c67e4200"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1909387 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309d709d98f9472a8a900adeccd6c47f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4e61196eff4548a0be8b0525d4b923"}},"metadata":{}},{"name":"stdout","text":"Loaded 206704 Hindi samples from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce64f16e46ee4ae2b3b8f8f3c3f4b955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92700e20b0514163a3bbb9154c3f7100"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/212556 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"229e6b3eee9c43c99b39e9bacf2b093c"}},"metadata":{}},{"name":"stdout","text":"Loaded 177439 Marathi samples from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4985628f0c04374a4e8489c203fcffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/163M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c00e346e54b4fca831b70cc60a6c4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/169834 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f31cdae5864ef39f8bd702f0c5c236"}},"metadata":{}},{"name":"stdout","text":"Loaded 117012 Gujarati samples from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/328 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af6ea66ebcf4c4988d959188986111b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d59e5b1201f49bba47c6001ecd2595d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6729303e0a54146af5f5bfb97b86b5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/391M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990440793f394353af1f7348e38fd214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/86.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd908a8477a94d3e91124c97ec073126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1114481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38134142d9d7477fb48cbac2bba4abf9"}},"metadata":{}},{"name":"stdout","text":"Loaded 214656 Bengali samples from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/246 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891ecdb67f734d07a44cdd6eb77db756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/342M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"164457a1ffe14fff8b5d22143b5350c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"640b3a023bef44448dae27c8814b7305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/285M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7465dd1ffc2942d893bed00abf8b934b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/833101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f205fd7576d4d2890c66300fe0db661"}},"metadata":{}},{"name":"stdout","text":"Loaded 190633 Tamil samples from OSCAR\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee786655ee4a4f98a0dee12b68cd55dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012ef6aad301483d81d76e1381816cbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/251064 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649ca2d7a7ed4eeb97142127527f81ee"}},"metadata":{}},{"name":"stdout","text":"Loaded 177725 Kannada samples from OSCAR\nSaved total of 1084169 samples from datasets to temp_dataset_corpus.csv\nScraping from web...\nError scraping https://hindi.bbc.com: HTTPSConnectionPool(host='hindi.bbc.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7fcdb933cd90>: Failed to resolve 'hindi.bbc.com' ([Errno -2] Name or service not known)\"))\nSaved total of 0 samples from web to temp_scraped_corpus.csv\nExtracting from PDFs...\nError extracting text from /kaggle/input/pdddffs/allpdfs/170502_46th_BoG_Minutes.pdf: Unable to get page count. Is poppler installed and in PATH?\nError extracting text from /kaggle/input/pdddffs/allpdfs/471 (TO).pdf: Unable to get page count. Is poppler installed and in PATH?\nError extracting text from /kaggle/input/pdddffs/allpdfs/Extension-of-Ahdoc-Employees.pdf: Unable to get page count. Is poppler installed and in PATH?\nError extracting text from /kaggle/input/pdddffs/allpdfs/181010_51-BOG-MINUTES.pdf: Unable to get page count. Is poppler installed and in PATH?\nSaved total of 0 samples from PDFs to temp_pdf_corpus.csv\nMerging and balancing corpus...\nAdded 206704 Hindi samples to final corpus\nAdded 177439 Marathi samples to final corpus\nNo samples found for Sindhi\nAdded 117012 Gujarati samples to final corpus\nAdded 214656 Bengali samples to final corpus\nAdded 190633 Tamil samples to final corpus\nAdded 177725 Kannada samples to final corpus\nFinal corpus saved to indic_corpus.csv with 9199912 samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv(\"indic_corpus.csv\", encoding='utf-8-sig', escapechar='\\\\')","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:46:28.961732Z","iopub.execute_input":"2025-03-10T17:46:28.962301Z","iopub.status.idle":"2025-03-10T17:48:35.670113Z","shell.execute_reply.started":"2025-03-10T17:46:28.962263Z","shell.execute_reply":"2025-03-10T17:48:35.668976Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:48:42.174498Z","iopub.execute_input":"2025-03-10T17:48:42.174907Z","iopub.status.idle":"2025-03-10T17:48:42.208453Z","shell.execute_reply.started":"2025-03-10T17:48:42.174877Z","shell.execute_reply":"2025-03-10T17:48:42.207270Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                      text language\n0        'आइटम गर्ल' बनकर हिट हुई थीं राखी सावंत, आज कर...    Hindi\n1        नई दिल्ली: देश में अगस्त में थोक मूल्य सूचकांक...    Hindi\n2        वरिष्ठ समाजवादी नेता शरद यादव ने अगले साल लोकस...    Hindi\n3        बेंगलुरु के आसमान में इंडिगो के 2 विमान टकराने...    Hindi\n4        थोक कारखाने - उच्च वापस निर्माताओं और आपूर्तिक...    Hindi\n...                                                    ...      ...\n1084164  ಹಿಂದೂ ಧರ್ಮದಲ್ಲಿ ಸಂಪತ್ತಿನ ದೇವತೆಯೆಂದೇ ಪೂಜಿಸಲ್ಪಡು...  Kannada\n1084165  ಬೆಳಗಾವಿ, ಜೂ.24- ಅನಾರೋಗ್ಯದಿಂದ ಬಳಲಿ ದಯಾಮರಣಕ್ಕೆ ಅ...  Kannada\n1084166  ಕೇರಳ ಶಿಕ್ಷಕರ ಅರ್ಹತಾ ಪರೀಕ್ಷೆ (ಕೆ.ಟಿ.ಟಿ.ಟಿ) ಯನ್ನ...  Kannada\n1084167  ದೇವದುರ್ಗ: ತಾಲ್ಲೂಕಿನ ಕೆ. ಇರಬಗೇರಾ ಗ್ರಾಮ ಪಂಚಾಯಿತಿ...  Kannada\n1084168  ಸಂತ ತೆರೇಸಮ್ಮನವರ ದೇವಾಲಯ: ಭಾನುವಾರ ‘ಜೋಸೆಫ್ ಮತ್ತು ...  Kannada\n\n[1084169 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>'आइटम गर्ल' बनकर हिट हुई थीं राखी सावंत, आज कर...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>नई दिल्ली: देश में अगस्त में थोक मूल्य सूचकांक...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>वरिष्ठ समाजवादी नेता शरद यादव ने अगले साल लोकस...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>बेंगलुरु के आसमान में इंडिगो के 2 विमान टकराने...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>थोक कारखाने - उच्च वापस निर्माताओं और आपूर्तिक...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1084164</th>\n      <td>ಹಿಂದೂ ಧರ್ಮದಲ್ಲಿ ಸಂಪತ್ತಿನ ದೇವತೆಯೆಂದೇ ಪೂಜಿಸಲ್ಪಡು...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084165</th>\n      <td>ಬೆಳಗಾವಿ, ಜೂ.24- ಅನಾರೋಗ್ಯದಿಂದ ಬಳಲಿ ದಯಾಮರಣಕ್ಕೆ ಅ...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084166</th>\n      <td>ಕೇರಳ ಶಿಕ್ಷಕರ ಅರ್ಹತಾ ಪರೀಕ್ಷೆ (ಕೆ.ಟಿ.ಟಿ.ಟಿ) ಯನ್ನ...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084167</th>\n      <td>ದೇವದುರ್ಗ: ತಾಲ್ಲೂಕಿನ ಕೆ. ಇರಬಗೇರಾ ಗ್ರಾಮ ಪಂಚಾಯಿತಿ...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084168</th>\n      <td>ಸಂತ ತೆರೇಸಮ್ಮನವರ ದೇವಾಲಯ: ಭಾನುವಾರ ‘ಜೋಸೆಫ್ ಮತ್ತು ...</td>\n      <td>Kannada</td>\n    </tr>\n  </tbody>\n</table>\n<p>1084169 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Create a dataset\n!mkdir -p /kaggle/working/indic_corpus_dataset\n!mv /kaggle/working/indic_corpus.csv /kaggle/working/indic_corpus_dataset/\n# For compressed files:\n# !mv /kaggle/working/indic_corpus.csv.gz /kaggle/working/indic_corpus_dataset/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:53:40.986347Z","iopub.execute_input":"2025-03-10T17:53:40.986756Z","iopub.status.idle":"2025-03-10T17:53:41.547065Z","shell.execute_reply.started":"2025-03-10T17:53:40.986724Z","shell.execute_reply":"2025-03-10T17:53:41.545725Z"}},"outputs":[{"name":"stdout","text":"mv: cannot stat '/kaggle/working/indic_corpus.csv': No such file or directory\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!pip install pdfminer pdf2image pytesseract langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:43:58.451472Z","iopub.execute_input":"2025-03-10T14:43:58.451854Z","iopub.status.idle":"2025-03-10T14:44:27.841829Z","shell.execute_reply.started":"2025-03-10T14:43:58.451823Z","shell.execute_reply":"2025-03-10T14:44:27.840635Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfminer\n  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\nRequirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\nCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from pdfminer) (3.21.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\nBuilding wheels for collected packages: pdfminer, langdetect\n  Building wheel for pdfminer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140078 sha256=bda0fdb6600a646b555e5ed93d1c9e7129120340f7c9448f7f63d3a61bffc857\n  Stored in directory: /root/.cache/pip/wheels/4e/c1/68/f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=4c195e25d477eb4f75cd664bb338af019bf9b65983870ca1ad9be7c9ca288d39\n  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\nSuccessfully built pdfminer langdetect\nInstalling collected packages: pdfminer, langdetect\nSuccessfully installed langdetect-1.0.9 pdfminer-20191125\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install pdfminer.six\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:45:11.684598Z","iopub.execute_input":"2025-03-10T14:45:11.684979Z","iopub.status.idle":"2025-03-10T14:45:15.754548Z","shell.execute_reply.started":"2025-03-10T14:45:11.684949Z","shell.execute_reply":"2025-03-10T14:45:15.753647Z"}},"outputs":[{"name":"stdout","text":"Collecting pdfminer.six\n  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (44.0.1)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\nDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pdfminer.six\nSuccessfully installed pdfminer.six-20240706\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install cleantext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T14:45:38.593031Z","iopub.execute_input":"2025-03-10T14:45:38.593384Z","iopub.status.idle":"2025-03-10T14:45:42.364627Z","shell.execute_reply.started":"2025-03-10T14:45:38.593355Z","shell.execute_reply":"2025-03-10T14:45:42.363338Z"}},"outputs":[{"name":"stdout","text":"Collecting cleantext\n  Downloading cleantext-1.1.4-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from cleantext) (3.2.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->cleantext) (1.17.0)\nDownloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\nInstalling collected packages: cleantext\nSuccessfully installed cleantext-1.1.4\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:55:31.664541Z","iopub.execute_input":"2025-03-10T17:55:31.665036Z","iopub.status.idle":"2025-03-10T17:55:31.681795Z","shell.execute_reply.started":"2025-03-10T17:55:31.664999Z","shell.execute_reply":"2025-03-10T17:55:31.680405Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                      text language\n0        'आइटम गर्ल' बनकर हिट हुई थीं राखी सावंत, आज कर...    Hindi\n1        नई दिल्ली: देश में अगस्त में थोक मूल्य सूचकांक...    Hindi\n2        वरिष्ठ समाजवादी नेता शरद यादव ने अगले साल लोकस...    Hindi\n3        बेंगलुरु के आसमान में इंडिगो के 2 विमान टकराने...    Hindi\n4        थोक कारखाने - उच्च वापस निर्माताओं और आपूर्तिक...    Hindi\n...                                                    ...      ...\n1084164  ಹಿಂದೂ ಧರ್ಮದಲ್ಲಿ ಸಂಪತ್ತಿನ ದೇವತೆಯೆಂದೇ ಪೂಜಿಸಲ್ಪಡು...  Kannada\n1084165  ಬೆಳಗಾವಿ, ಜೂ.24- ಅನಾರೋಗ್ಯದಿಂದ ಬಳಲಿ ದಯಾಮರಣಕ್ಕೆ ಅ...  Kannada\n1084166  ಕೇರಳ ಶಿಕ್ಷಕರ ಅರ್ಹತಾ ಪರೀಕ್ಷೆ (ಕೆ.ಟಿ.ಟಿ.ಟಿ) ಯನ್ನ...  Kannada\n1084167  ದೇವದುರ್ಗ: ತಾಲ್ಲೂಕಿನ ಕೆ. ಇರಬಗೇರಾ ಗ್ರಾಮ ಪಂಚಾಯಿತಿ...  Kannada\n1084168  ಸಂತ ತೆರೇಸಮ್ಮನವರ ದೇವಾಲಯ: ಭಾನುವಾರ ‘ಜೋಸೆಫ್ ಮತ್ತು ...  Kannada\n\n[1084169 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>'आइटम गर्ल' बनकर हिट हुई थीं राखी सावंत, आज कर...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>नई दिल्ली: देश में अगस्त में थोक मूल्य सूचकांक...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>वरिष्ठ समाजवादी नेता शरद यादव ने अगले साल लोकस...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>बेंगलुरु के आसमान में इंडिगो के 2 विमान टकराने...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>थोक कारखाने - उच्च वापस निर्माताओं और आपूर्तिक...</td>\n      <td>Hindi</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1084164</th>\n      <td>ಹಿಂದೂ ಧರ್ಮದಲ್ಲಿ ಸಂಪತ್ತಿನ ದೇವತೆಯೆಂದೇ ಪೂಜಿಸಲ್ಪಡು...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084165</th>\n      <td>ಬೆಳಗಾವಿ, ಜೂ.24- ಅನಾರೋಗ್ಯದಿಂದ ಬಳಲಿ ದಯಾಮರಣಕ್ಕೆ ಅ...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084166</th>\n      <td>ಕೇರಳ ಶಿಕ್ಷಕರ ಅರ್ಹತಾ ಪರೀಕ್ಷೆ (ಕೆ.ಟಿ.ಟಿ.ಟಿ) ಯನ್ನ...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084167</th>\n      <td>ದೇವದುರ್ಗ: ತಾಲ್ಲೂಕಿನ ಕೆ. ಇರಬಗೇರಾ ಗ್ರಾಮ ಪಂಚಾಯಿತಿ...</td>\n      <td>Kannada</td>\n    </tr>\n    <tr>\n      <th>1084168</th>\n      <td>ಸಂತ ತೆರೇಸಮ್ಮನವರ ದೇವಾಲಯ: ಭಾನುವಾರ ‘ಜೋಸೆಫ್ ಮತ್ತು ...</td>\n      <td>Kannada</td>\n    </tr>\n  </tbody>\n</table>\n<p>1084169 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:56:02.035237Z","iopub.execute_input":"2025-03-10T17:56:02.035640Z","iopub.status.idle":"2025-03-10T17:56:02.045371Z","shell.execute_reply.started":"2025-03-10T17:56:02.035581Z","shell.execute_reply":"2025-03-10T17:56:02.044290Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0          'आइटम गर्ल' बनकर हिट हुई थीं राखी सावंत, आज कर...\n1          नई दिल्ली: देश में अगस्त में थोक मूल्य सूचकांक...\n2          वरिष्ठ समाजवादी नेता शरद यादव ने अगले साल लोकस...\n3          बेंगलुरु के आसमान में इंडिगो के 2 विमान टकराने...\n4          थोक कारखाने - उच्च वापस निर्माताओं और आपूर्तिक...\n                                 ...                        \n1084164    ಹಿಂದೂ ಧರ್ಮದಲ್ಲಿ ಸಂಪತ್ತಿನ ದೇವತೆಯೆಂದೇ ಪೂಜಿಸಲ್ಪಡು...\n1084165    ಬೆಳಗಾವಿ, ಜೂ.24- ಅನಾರೋಗ್ಯದಿಂದ ಬಳಲಿ ದಯಾಮರಣಕ್ಕೆ ಅ...\n1084166    ಕೇರಳ ಶಿಕ್ಷಕರ ಅರ್ಹತಾ ಪರೀಕ್ಷೆ (ಕೆ.ಟಿ.ಟಿ.ಟಿ) ಯನ್ನ...\n1084167    ದೇವದುರ್ಗ: ತಾಲ್ಲೂಕಿನ ಕೆ. ಇರಬಗೇರಾ ಗ್ರಾಮ ಪಂಚಾಯಿತಿ...\n1084168    ಸಂತ ತೆರೇಸಮ್ಮನವರ ದೇವಾಲಯ: ಭಾನುವಾರ ‘ಜೋಸೆಫ್ ಮತ್ತು ...\nName: text, Length: 1084169, dtype: object"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:12:23.074577Z","iopub.execute_input":"2025-03-10T15:12:23.074905Z","iopub.status.idle":"2025-03-10T15:12:27.721704Z","shell.execute_reply.started":"2025-03-10T15:12:23.074883Z","shell.execute_reply":"2025-03-10T15:12:27.720648Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install modin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:14:30.438095Z","iopub.execute_input":"2025-03-10T15:14:30.438418Z","iopub.status.idle":"2025-03-10T15:14:37.986388Z","shell.execute_reply.started":"2025-03-10T15:14:30.438390Z","shell.execute_reply":"2025-03-10T15:14:37.985417Z"}},"outputs":[{"name":"stdout","text":"Collecting modin\n  Downloading modin-0.32.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.10/dist-packages (from modin) (2.2.3)\nRequirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from modin) (24.2)\nRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from modin) (1.26.4)\nRequirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.10/dist-packages (from modin) (2024.12.0)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from modin) (5.9.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->modin) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->modin) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->modin) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->modin) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->modin) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.4->modin) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->modin) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->modin) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.4->modin) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.4->modin) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.4->modin) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.4->modin) (2024.2.0)\nDownloading modin-0.32.0-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: modin\nSuccessfully installed modin-0.32.0\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import modin.pandas as pd\ndf = pd.read_csv(\"/kaggle/working/dataset_corpus.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:14:39.833700Z","iopub.execute_input":"2025-03-10T15:14:39.834012Z","execution_failed":"2025-03-10T15:21:40.774Z"}},"outputs":[{"name":"stderr","text":"UserWarning: The size of /dev/shm is too small (14495510528 bytes). The required size at least half of RAM (16831176704 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n2025-03-10 15:14:48,861\tINFO worker.py:1841 -- Started a local Ray instance.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:22:57.681765Z","iopub.execute_input":"2025-03-10T15:22:57.682082Z","iopub.status.idle":"2025-03-10T15:22:58.882389Z","shell.execute_reply.started":"2025-03-10T15:22:57.682057Z","shell.execute_reply":"2025-03-10T15:22:58.881675Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dd=pd.read_csv(\"/kaggle/working/indic_corpus.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:23:00.361012Z","iopub.execute_input":"2025-03-10T15:23:00.361609Z","iopub.status.idle":"2025-03-10T15:23:47.043320Z","shell.execute_reply.started":"2025-03-10T15:23:00.361572Z","shell.execute_reply":"2025-03-10T15:23:47.042136Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-31e4963bb3c5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/indic_corpus.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe0 in position 0: unexpected end of data"],"ename":"UnicodeDecodeError","evalue":"'utf-8' codec can't decode byte 0xe0 in position 0: unexpected end of data","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"dd","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}