{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10695978,"sourceType":"datasetVersion","datasetId":6627874},{"sourceId":11208776,"sourceType":"datasetVersion","datasetId":6998952}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:16.708605Z","iopub.execute_input":"2025-03-20T14:00:16.708897Z","iopub.status.idle":"2025-03-20T14:00:16.807475Z","shell.execute_reply.started":"2025-03-20T14:00:16.708867Z","shell.execute_reply":"2025-03-20T14:00:16.806643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T08:31:30.538537Z","iopub.execute_input":"2025-03-20T08:31:30.538900Z","iopub.status.idle":"2025-03-20T08:31:31.855574Z","shell.execute_reply.started":"2025-03-20T08:31:30.538868Z","shell.execute_reply":"2025-03-20T08:31:31.853998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get update && apt-get install -y poppler-utils tesseract-ocr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:28.518368Z","iopub.execute_input":"2025-03-20T14:00:28.518650Z","iopub.status.idle":"2025-03-20T14:00:48.221644Z","shell.execute_reply.started":"2025-03-20T14:00:28.518629Z","shell.execute_reply":"2025-03-20T14:00:48.220553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pandas requests beautifulsoup4 scrapy datasets pdfminer.six clean-text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:48.222872Z","iopub.execute_input":"2025-03-20T14:00:48.223121Z","iopub.status.idle":"2025-03-20T14:01:03.547210Z","shell.execute_reply.started":"2025-03-20T14:00:48.223099Z","shell.execute_reply":"2025-03-20T14:01:03.546155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:01:03.548831Z","iopub.execute_input":"2025-03-20T14:01:03.549173Z","iopub.status.idle":"2025-03-20T14:01:09.534313Z","shell.execute_reply.started":"2025-03-20T14:01:03.549142Z","shell.execute_reply":"2025-03-20T14:01:09.533216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\"]\nTARGET_PER_LANG = 250000  \nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    words = text.split()\n    return len(words) >= MIN_WORDS\n\ndef preprocess_image(image):\n    \"\"\"Preprocess scanned images to improve OCR accuracy.\"\"\"\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    processed_image = cv2.dilate(binary, kernel, iterations=1)\n    return Image.fromarray(processed_image)\n\ndef extract_text_from_pdf(pdf_path, batch_size=10):  \n    \"\"\"Extract text from scanned PDFs using OCR in batches.\"\"\"\n    try:\n        images = convert_from_path(pdf_path, dpi=200, first_page=1, last_page=20)  \n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ) + \"\\n\"\n            del batch_images\n            processed_image = None\n            gc.collect()\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\ndef detect_language_unicode(text):\n    \"\"\"Detects and labels language based on Unicode ranges.\"\"\"\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),  # Devanagari\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'), # Devanagari (overlap with Hindi)\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),  # Arabic script\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),    # Arabic script\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    \n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    \n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    \n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\ndef save_df_in_chunks(df, output_file, chunk_size=10000):\n    \"\"\"Save large dataframe in chunks to avoid memory issues.\"\"\"\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, \n                    mode=mode, \n                    header=header, \n                    index=False, \n                    escapechar='\\\\', \n                    encoding='utf-8-sig')\n        # Clear memory\n        del chunk\n        gc.collect()\n    print(f\"Saved {len(df)} samples to {output_file} in chunks\")\n\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    all_samples = 0\n    oscar_langs = {\n        \"hi\": \"Hindi\",\n        \"mr\": \"Marathi\",\n        \"gu\": \"Gujarati\",\n        \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\",\n        \"kn\": \"Kannada\"\n    }\n    \n    open(output_file, 'w', encoding='utf-8-sig').close()\n    \n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = min(10000, TARGET_PER_LANG)\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            \n            total_lang_samples = 0\n            for i in range(num_chunks):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, len(dataset))\n                \n                chunk_data = dataset[start_idx:end_idx]\n                chunk_df = pd.DataFrame(chunk_data)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                \n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    # Write chunk to file\n                    mode = 'a' if i > 0 or all_samples > 0 else 'w'\n                    header = i == 0 and all_samples == 0\n                    filtered_df.to_csv(output_file, \n                                      mode=mode, \n                                      header=header, \n                                      index=False, \n                                      escapechar='\\\\', \n                                      encoding='utf-8-sig')\n                    \n                    samples_added = len(filtered_df)\n                    total_lang_samples += samples_added\n                    all_samples += samples_added\n                \n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                \n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n                    \n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n            \n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    \n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"]\n    }\n    \n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    \n    for lang, urls in sites.items():\n        lang_texts = []\n        max_samples_per_url = TARGET_PER_LANG // len(urls)\n        \n        for url in urls:\n            try:\n                response = requests.get(url, headers=headers, timeout=10)\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                paragraphs = soup.find_all(\"p\")\n                \n                for p in paragraphs:\n                    if len(lang_texts) >= max_samples_per_url:\n                        break\n                        \n                    text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True)\n                    if filter_text_length(text):\n                        lang_texts.append({\"text\": text, \"language\": lang})\n                        \n                # Save immediately if we have enough data\n                if len(lang_texts) >= max_samples_per_url:\n                    break\n                    \n            except Exception as e:\n                print(f\"Error scraping {url}: {e}\")\n        \n        # Save language data\n        if lang_texts:\n            lang_df = pd.DataFrame(lang_texts)\n            mode = 'a' if all_samples > 0 else 'w'\n            header = all_samples == 0\n            lang_df.to_csv(output_file, \n                          mode=mode, \n                          header=header, \n                          index=False, \n                          escapechar='\\\\', \n                          encoding='utf-8-sig')\n            \n            all_samples += len(lang_df)\n            print(f\"Scraped {lang}: {len(lang_df)} samples\")\n            \n            del lang_df, lang_texts\n            gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    \n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    \n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    # Limit to 10 PDFs max to prevent memory issues\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:10]\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        \n        try:\n            # Try pdfminer for searchable PDFs first\n            text = extract_text(pdf_path)\n            if not text.strip():  # If empty, use OCR\n                text = extract_text_from_pdf(pdf_path, batch_size=5)  # Reduced batch size\n            \n            if not text:\n                continue\n            \n            # Split into paragraphs\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            \n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, lang_probs, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:  # Only include target languages\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            \n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, \n                             mode=mode, \n                             header=header, \n                             index=False, \n                             escapechar='\\\\', \n                             encoding='utf-8-sig')\n                \n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                \n                del pdf_df, pdf_corpus\n                gc.collect()\n            \n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n        \n        text = None\n        paragraphs = None\n        gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    \n    # Process each source individually and merge at the end\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        \n        open(output_file, 'w', encoding='utf-8-sig').close()\n        \n        # Process each language separately\n        for lang in LANGUAGES:\n            lang_samples = []\n            \n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    # Read in chunks\n                    chunk_size = 10000\n                    for chunk in pd.read_csv(source_file, \n                                            chunksize=chunk_size, \n                                            encoding='utf-8-sig', \n                                            escapechar='\\\\'):\n                        # Filter for current language\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            \n                            # If we have enough samples, stop reading\n                            total_samples = sum(len(df) for df in lang_samples)\n                            if total_samples >= TARGET_PER_LANG:\n                                break\n                    \n                    gc.collect()\n            \n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                available_samples = len(combined_lang)\n                \n                if available_samples > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                \n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, \n                                    mode=mode, \n                                    header=header, \n                                    index=False, \n                                    escapechar='\\\\', \n                                    encoding='utf-8-sig')\n                \n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                \n                # Clear memory\n                del combined_lang, lang_samples\n                gc.collect()\n            else:\n                print(f\"No samples found for {lang}\")\n        \n        total_lines = 0\n        with open(output_file, 'r', encoding='utf-8-sig') as f:\n            for _ in f:\n                total_lines += 1\n        \n        total_lines -= 1  # Subtract header line\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        \n        # Clean up temporary files\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:06:57.658914Z","iopub.execute_input":"2025-03-10T17:06:57.659297Z","iopub.status.idle":"2025-03-10T17:44:04.813424Z","shell.execute_reply.started":"2025-03-10T17:06:57.659268Z","shell.execute_reply":"2025-03-10T17:44:04.811934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"indic_corpus.csv\", encoding='utf-8-sig', escapechar='\\\\')","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:46:28.961732Z","iopub.execute_input":"2025-03-10T17:46:28.962301Z","iopub.status.idle":"2025-03-10T17:48:35.670113Z","shell.execute_reply.started":"2025-03-10T17:46:28.962263Z","shell.execute_reply":"2025-03-10T17:48:35.668976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install -y tesseract-ocr tesseract-ocr-hin tesseract-ocr-mar tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-kan tesseract-ocr-eng tesseract-ocr-urd tesseract-ocr-chi-sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:31.700099Z","iopub.execute_input":"2025-03-20T14:02:31.700473Z","iopub.status.idle":"2025-03-20T14:02:38.713557Z","shell.execute_reply.started":"2025-03-20T14:02:31.700442Z","shell.execute_reply":"2025-03-20T14:02:38.712489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdf2image pytesseract opencv-python pillow clean-text langdetect datasets requests beautifulsoup4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:38.715217Z","iopub.execute_input":"2025-03-20T14:02:38.715600Z","iopub.status.idle":"2025-03-20T14:02:42.161361Z","shell.execute_reply.started":"2025-03-20T14:02:38.715560Z","shell.execute_reply":"2025-03-20T14:02:42.160350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdfminer pdf2image pytesseract langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:42.163299Z","iopub.execute_input":"2025-03-20T14:02:42.163627Z","iopub.status.idle":"2025-03-20T14:03:07.039191Z","shell.execute_reply.started":"2025-03-20T14:02:42.163592Z","shell.execute_reply":"2025-03-20T14:03:07.038381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdfminer.six==20231228\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:07:00.074390Z","iopub.execute_input":"2025-03-20T14:07:00.074773Z","iopub.status.idle":"2025-03-20T14:07:04.442186Z","shell.execute_reply.started":"2025-03-20T14:07:00.074748Z","shell.execute_reply":"2025-03-20T14:07:04.440969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install cleantext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:03:10.534805Z","iopub.execute_input":"2025-03-20T14:03:10.535056Z","iopub.status.idle":"2025-03-20T14:03:14.196236Z","shell.execute_reply.started":"2025-03-20T14:03:10.535032Z","shell.execute_reply":"2025-03-20T14:03:14.195167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:12:23.074577Z","iopub.execute_input":"2025-03-10T15:12:23.074905Z","iopub.status.idle":"2025-03-10T15:12:27.721704Z","shell.execute_reply.started":"2025-03-10T15:12:23.074883Z","shell.execute_reply":"2025-03-10T15:12:27.720648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\ntry:\n    from pdfminer.high_level import extract_text\nexcept ImportError:\n    def extract_text(pdf_path):\n        print(f\"Using fallback extraction for {pdf_path} due to pdfminer import issue\")\n        return \"\"\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\nimport concurrent.futures\nimport time\n\nLANGUAGES = [\n    \"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\",\n    \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"\n]\nTARGET_PER_LANG = 250000  # Reduced for faster runtime\nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    return len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    return Image.fromarray(cv2.dilate(binary, kernel, iterations=1))\n\ndef extract_text_from_pdf(pdf_path, batch_size=5):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        start_time = time.time()\n        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=10)  # Reduced DPI and pages\n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ).replace('\\0', '')[:100000] + \"\\n\"  # Clean NULL, truncate\n            del batch_images\n            gc.collect()\n        print(f\"Finished {pdf_path} in {time.time() - start_time:.2f}s\")\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\ndef detect_language_unicode(text):\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Telugu': sum(1 for char in text if '\\u0C00' <= char <= '\\u0C7F'),\n        'Malayalam': sum(1 for char in text if '\\u0D00' <= char <= '\\u0D7F'),\n        'Punjabi': sum(1 for char in text if '\\u0A00' <= char <= '\\u0A7F'),\n        'Odia': sum(1 for char in text if '\\u0B00' <= char <= '\\u0B7F'),\n        'Assamese': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\ndef save_df_in_chunks(df, output_file, chunk_size=5000):\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n        del chunk\n        gc.collect()\n\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    oscar_langs = {\n        \"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\",\n        \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"\n    }\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    for code, lang in oscar_langs.items():\n        try:\n            print(f\"Loading OSCAR for {lang}...\")\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = 5000\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            total_lang_samples = 0\n            \n            for i in range(num_chunks):\n                chunk_data = dataset[i * chunk_size:(i + 1) * chunk_size]\n                chunk_df = pd.DataFrame(chunk_data)\n                chunk_df['text'] = chunk_df['text'].apply(lambda x: x.replace('\\0', '')[:100000] if isinstance(x, str) else x)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    mode = 'a' if all_samples > 0 else 'w'\n                    header = all_samples == 0\n                    filtered_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                    total_lang_samples += len(filtered_df)\n                    all_samples += len(filtered_df)\n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\ndef scrape_url(url, lang, max_samples):\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    lang_texts = []\n    try:\n        response = requests.get(url, headers=headers, timeout=5)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paragraphs = soup.find_all(\"p\")\n        for p in paragraphs:\n            if len(lang_texts) >= max_samples:\n                break\n            text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True).replace('\\0', '')[:100000]\n            if filter_text_length(text):\n                lang_texts.append({\"text\": text, \"language\": lang})\n        print(f\"Scraped {url}: {len(lang_texts)} samples\")\n    except Exception as e:\n        print(f\"Error scraping {url}: {e}\")\n    return lang_texts\n\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"],\n        \"Telugu\": [\"https://eenadu.net\", \"https://sakshi.com\"],\n        \"Malayalam\": [\"https://mathrubhumi.com\", \"https://www.manoramaonline.com\"],\n        \"Punjabi\": [\"https://punjabitribuneonline.com\", \"https://www.ajitjalandhar.com\"],\n        \"Odia\": [\"https://sambad.in\", \"https://dharitri.com\"],\n        \"Assamese\": [\"https://asomiyapratidin.in\", \"https://pratidintime.com\"]\n    }\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        for lang, urls in sites.items():\n            max_samples_per_url = TARGET_PER_LANG // len(urls)\n            future_to_url = {executor.submit(scrape_url, url, lang, max_samples_per_url): url for url in urls}\n            lang_texts = []\n            for future in concurrent.futures.as_completed(future_to_url):\n                lang_texts.extend(future.result())\n            if lang_texts:\n                lang_df = pd.DataFrame(lang_texts)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                lang_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                all_samples += len(lang_df)\n                print(f\"Scraped {lang}: {len(lang_df)} samples\")\n                del lang_df, lang_texts\n                gc.collect()\n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:5]  # Limit to 5 PDFs\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        try:\n            # Modified to handle pdfminer error more gracefully\n            try:\n                text = extract_text(pdf_path)\n            except Exception as e:\n                print(f\"Primary PDF extraction failed: {e}, using backup method\")\n                text = \"\"\n                \n            if not text.strip():\n                text = extract_text_from_pdf(pdf_path)\n            if not text:\n                continue\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, _, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                del pdf_df, pdf_corpus\n                gc.collect()\n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()\n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()\n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()\n    \n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        open(output_file, 'w', encoding='utf-8-sig').close()\n        for lang in LANGUAGES:\n            lang_samples = []\n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    for chunk in pd.read_csv(source_file, chunksize=5000, encoding='utf-8-sig', escapechar='\\\\'):\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            if sum(len(df) for df in lang_samples) >= TARGET_PER_LANG:\n                                break\n                    gc.collect()\n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                if len(combined_lang) > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                del combined_lang, lang_samples\n                gc.collect()\n        total_lines = sum(1 for _ in open(output_file, 'r', encoding='utf-8-sig')) - 1  # Subtract header\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:16:57.249315Z","iopub.execute_input":"2025-03-20T14:16:57.249660Z","iopub.status.idle":"2025-03-20T15:07:41.554729Z","shell.execute_reply.started":"2025-03-20T14:16:57.249638Z","shell.execute_reply":"2025-03-20T15:07:41.533771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gzip\nimport pandas as pd\n\n# Decompress and clean NULL bytes\nwith gzip.open('/kaggle/working/dataset_corpus.csv.gz', 'rb') as f_in:\n    with open('/kaggle/working/dataset_corpus_clean.csv', 'wb') as f_out:\n        # Read raw bytes, replace NULL bytes\n        data = f_in.read().replace(b'\\0', b'')\n        f_out.write(data)\n\n# Load cleaned CSV\ndf = pd.read_csv('/kaggle/working/dataset_corpus_clean.csv', engine='python', escapechar='\\\\')\nprint(f\"OSCAR samples: {len(df)}\")\nscraped_df = scrape_from_web()\npdf_df = extract_from_pdfs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T10:38:56.438622Z","iopub.execute_input":"2025-03-20T10:38:56.438913Z","execution_failed":"2025-03-20T10:44:10.381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nimport gc\nimport time\nfrom multiprocessing import Pool, cpu_count\nfrom datasets import load_dataset\nimport parquet\nimport gzip\n\n# Initialize Spark\nspark = SparkSession.builder \\\n    .appName(\"IndicCorpus\") \\\n    .config(\"spark.executor.memory\", \"8g\") \\\n    .config(\"spark.driver.memory\", \"8g\") \\\n    .getOrCreate()\n\nLANGUAGES = [\n    \"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\",\n    \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"\n]\nTARGET_PER_LANG = 416667  # ~5M total / 12\nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    return len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    return Image.fromarray(cv2.dilate(binary, kernel, iterations=1))\n\ndef extract_text_from_pdf(pdf_path):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        start_time = time.time()\n        text = extract_text(pdf_path)\n        if not text.strip():\n            images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=50)\n            text = \"\"\n            for image in images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ).replace('\\0', '')[:100000] + \"\\n\"\n                del processed_image\n                gc.collect()\n        print(f\"Finished {pdf_path} in {time.time() - start_time:.2f}s\")\n        return text.strip(), os.path.basename(pdf_path).split('_')[0]  # Assume filename like \"Odia_1.pdf\"\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\", \"\"\n\ndef detect_language_unicode(text):\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Telugu': sum(1 for char in text if '\\u0C00' <= char <= '\\u0C7F'),\n        'Malayalam': sum(1 for char in text if '\\u0D00' <= char <= '\\u0D7F'),\n        'Punjabi': sum(1 for char in text if '\\u0A00' <= char <= '\\u0A7F'),\n        'Odia': sum(1 for char in text if '\\u0B00' <= char <= '\\u0B7F'),\n        'Assamese': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None\n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    return top_lang if top_lang in LANGUAGES else None\n\ndef process_pdf(pdf_path):\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n    if not text:\n        return []\n    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    corpus = []\n    for para in paragraphs:\n        if filter_text_length(para):\n            detected_lang = detect_language_unicode(para)\n            lang = hinted_lang if hinted_lang in LANGUAGES else detected_lang\n            if lang in LANGUAGES:\n                corpus.append({\"text\": para, \"language\": lang})\n    return corpus\n\ndef load_oscar():\n    print(\"Loading OSCAR data...\")\n    oscar_langs = {\n        \"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\",\n        \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"\n    }\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            for i in range(min(TARGET_PER_LANG * 2, len(dataset))):  # Oversample, filter later\n                text = dataset[i]['text'].replace('\\0', '')[:100000]\n                if filter_text_length(text):\n                    oscar_data.append({\"text\": text, \"language\": lang})\n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    df = pd.DataFrame(oscar_data)\n    df.to_parquet(\"/kaggle/working/temp_oscar.parquet\", compression='gzip')\n    print(f\"Saved {len(df)} OSCAR samples\")\n    return len(df)\n\ndef load_samanantar():\n    print(\"Loading Samanantar data...\")\n    samanantar_langs = {\n        \"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\",\n        \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"\n    }\n    samanantar_data = []\n    try:\n        dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n        for row in dataset:\n            for lang_code, lang_name in samanantar_langs.items():\n                text = row.get(lang_code, \"\").replace('\\0', '')[:100000]\n                if text and filter_text_length(text):\n                    samanantar_data.append({\"text\": text, \"language\": lang_name})\n    except Exception as e:\n        print(f\"Error loading Samanantar: {e}\")\n    df = pd.DataFrame(samanantar_data)\n    df.to_parquet(\"/kaggle/working/temp_samanantar.parquet\", compression='gzip')\n    print(f\"Saved {len(df)} Samanantar samples\")\n    return len(df)\n\ndef process_pdfs_parallel(pdf_dir=\"/kaggle/input/meta-folder\"):\n    print(\"Processing PDFs in parallel...\")\n    pdf_files = []\n    for lang_folder in os.listdir(pdf_dir):\n        lang_path = os.path.join(pdf_dir, lang_folder)\n        if os.path.isdir(lang_path):\n            for pdf_file in os.listdir(lang_path):\n                if pdf_file.endswith(\".pdf\"):\n                    pdf_files.append(os.path.join(lang_path, pdf_file))\n    \n    with Pool(cpu_count()) as pool:\n        results = pool.map(process_pdf, pdf_files)\n    \n    pdf_corpus = [item for sublist in results for item in sublist]\n    df = pd.DataFrame(pdf_corpus)\n    df.to_parquet(\"/kaggle/working/temp_pdf.parquet\", compression='gzip')\n    print(f\"Saved {len(df)} PDF samples\")\n    return len(df)\n\ndef build_corpus(pdf_dir=\"/kaggle/input/meta-folder\"):\n    print(\"Building corpus...\")\n    for f in os.listdir(\"/kaggle/working\"):\n        if f.endswith((\".parquet\", \".gz\")):\n            os.remove(os.path.join(\"/kaggle/working\", f))\n    \n    pdf_count = process_pdfs_parallel(pdf_dir)\n    oscar_count = load_oscar()\n    samanantar_count = load_samanantar()\n    \n    spark_df = None\n    for source_file in [\"/kaggle/working/temp_pdf.parquet\", \"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]:\n        if os.path.exists(source_file):\n            temp_df = spark.read.parquet(source_file)\n            spark_df = temp_df if spark_df is None else spark_df.union(temp_df)\n    \n    if spark_df:\n        spark_df = spark_df.dropDuplicates([\"text\"])\n        balanced_df = spark_df.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")) \\\n            .filter(F.col(\"language\").isin(LANGUAGES)) \\\n            .join(spark_df, \"language\", \"inner\") \\\n            .orderBy(F.rand()) \\\n            .limit(TARGET_PER_LANG) \\\n            .select(\"text\", \"language\")\n        \n        balanced_df.write.parquet(\"/kaggle/working/indic_corpus.parquet\", mode=\"overwrite\", compression=\"gzip\")\n        total_samples = balanced_df.count()\n        print(f\"Saved {total_samples} samples to /kaggle/working/indic_corpus.parquet\")\n        \n        dist = balanced_df.groupBy(\"language\").count().collect()\n        for row in dist:\n            print(f\"{row['language']}: {row['count']} samples\")\n        if total_samples < TARGET_PER_LANG * len(LANGUAGES):\n            print(f\"Warning: Only {total_samples} samples collected, below 5M target!\")\n    \n    for f in [\"temp_pdf.parquet\", \"temp_oscar.parquet\", \"temp_samanantar.parquet\"]:\n        if os.path.exists(f\"/kaggle/working/{f}\"):\n            os.remove(f\"/kaggle/working/{f}\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdf2image pytesseract opencv-python pillow pdfminer.six datasets pyarrow pyspark fasttext\n!apt-get install -y tesseract-ocr tesseract-ocr-hin tesseract-ocr-mar tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-kan tesseract-ocr-tel tesseract-ocr-mal tesseract-ocr-pan tesseract-ocr-ori tesseract-ocr-asm tesseract-ocr-eng tesseract-ocr-urd tesseract-ocr-chi-sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T17:58:40.889549Z","iopub.execute_input":"2025-03-29T17:58:40.889925Z","iopub.status.idle":"2025-03-29T17:58:53.601883Z","shell.execute_reply.started":"2025-03-29T17:58:40.889892Z","shell.execute_reply":"2025-03-29T17:58:53.601047Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\nRequirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nCollecting pdfminer.six\n  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.1)\nRequirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\nRequirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (44.0.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.6)\nRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pdfminer.six\nSuccessfully installed pdfminer.six-20250327\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr is already the newest version (4.1.1-2.1build1).\ntesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\ntesseract-ocr-eng set to manually installed.\nThe following NEW packages will be installed:\n  tesseract-ocr-asm tesseract-ocr-ben tesseract-ocr-chi-sim tesseract-ocr-hin tesseract-ocr-kan\n  tesseract-ocr-mal tesseract-ocr-mar tesseract-ocr-ori tesseract-ocr-pan tesseract-ocr-tam\n  tesseract-ocr-tel tesseract-ocr-urd\n0 upgraded, 12 newly installed, 0 to remove and 129 not upgraded.\nNeed to get 13.1 MB of archives.\nAfter this operation, 27.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-asm all 1:4.00~git30-7274cfa-1.1 [1,421 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ben all 1:4.00~git30-7274cfa-1.1 [516 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-sim all 1:4.00~git30-7274cfa-1.1 [1,634 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hin all 1:4.00~git30-7274cfa-1.1 [913 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kan all 1:4.00~git30-7274cfa-1.1 [1,659 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mal all 1:4.00~git30-7274cfa-1.1 [1,678 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mar all 1:4.00~git30-7274cfa-1.1 [862 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ori all 1:4.00~git30-7274cfa-1.1 [1,024 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pan all 1:4.00~git30-7274cfa-1.1 [322 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tam all 1:4.00~git30-7274cfa-1.1 [1,071 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tel all 1:4.00~git30-7274cfa-1.1 [1,012 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-urd all 1:4.00~git30-7274cfa-1.1 [1,000 kB]\nFetched 13.1 MB in 0s (34.5 MB/s)             \nSelecting previously unselected package tesseract-ocr-asm.\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../00-tesseract-ocr-asm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-ben.\nPreparing to unpack .../01-tesseract-ocr-ben_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-chi-sim.\nPreparing to unpack .../02-tesseract-ocr-chi-sim_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-hin.\nPreparing to unpack .../03-tesseract-ocr-hin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-kan.\nPreparing to unpack .../04-tesseract-ocr-kan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-mal.\nPreparing to unpack .../05-tesseract-ocr-mal_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-mar.\nPreparing to unpack .../06-tesseract-ocr-mar_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-ori.\nPreparing to unpack .../07-tesseract-ocr-ori_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-pan.\nPreparing to unpack .../08-tesseract-ocr-pan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-tam.\nPreparing to unpack .../09-tesseract-ocr-tam_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-tel.\nPreparing to unpack .../10-tesseract-ocr-tel_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-urd.\nPreparing to unpack .../11-tesseract-ocr-urd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# corpus_construction.py\nimport pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport torch\nimport os\nimport gc\nfrom multiprocessing import Pool, cpu_count\nfrom datasets import load_dataset\nimport fasttext\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"IndicCorpus\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nTARGET_PER_LANG = 500000  # ~6M total\nMIN_WORDS = 50\nBATCH_SIZE = 10  # Process 10 images at a time on GPU\n\n# FastText path\nFASTTEXT_PATH = \"/kaggle/input/fasttextefficienttextclassification/lid.176.bin\"\nft_model = fasttext.load_model(FASTTEXT_PATH)\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef preprocess_image_cpu(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    return Image.fromarray(cv2.dilate(binary, np.ones((1, 1), np.uint8), iterations=1))\n\ndef preprocess_image_gpu(images):\n    try:\n        # Resize images to fixed size (1024x1024) and convert to numpy\n        resized_images = [image.resize((1024, 1024), Image.Resampling.LANCZOS) for image in images]\n        image_arrays = [np.array(img) for img in resized_images]\n        \n        # Process in batches to avoid memory overload\n        processed_images = []\n        for i in range(0, len(image_arrays), BATCH_SIZE):\n            batch = image_arrays[i:i + BATCH_SIZE]\n            tensor_images = torch.tensor(np.stack(batch), dtype=torch.uint8).to(device)\n            \n            # Grayscale on GPU\n            gray_images = 0.299 * tensor_images[:, :, :, 0] + 0.587 * tensor_images[:, :, :, 1] + 0.114 * tensor_images[:, :, :, 2]\n            \n            # Thresholding on GPU\n            binary_images = (gray_images > 150).type(torch.uint8) * 255\n            \n            # Dilation on GPU\n            kernel = torch.ones((1, 1, 1, 1), dtype=torch.uint8).to(device)\n            dilated_images = torch.nn.functional.conv2d(binary_images.unsqueeze(1), kernel, padding=0).squeeze(1)\n            \n            # Back to CPU\n            processed_images.extend([Image.fromarray(img.cpu().numpy()) for img in dilated_images])\n        \n        return processed_images\n    except Exception as e:\n        print(f\"GPU preprocessing failed: {e}. Falling back to CPU.\")\n        return [preprocess_image_cpu(img) for img in images]\n\ndef extract_text_from_pdf(pdf_path):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        text = extract_text(pdf_path)\n        if not text.strip():\n            images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=50)\n            processed_images = preprocess_image_gpu(images) if torch.cuda.is_available() else [preprocess_image_cpu(img) for img in images]\n            text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n            del processed_images, images\n            torch.cuda.empty_cache()  # Clear GPU memory\n            gc.collect()\n        hinted_lang = os.path.basename(os.path.dirname(pdf_path))\n        return text.strip(), hinted_lang if hinted_lang in LANGUAGES else None\n    except Exception as e:\n        print(f\"Error: {pdf_path}: {e}\")\n        return \"\", None\n\ndef detect_language(text, hinted_lang=None):\n    if hinted_lang in LANGUAGES:\n        return hinted_lang\n    sindhi_chars = sum(1 for char in text if '\\u0600' <= char <= '\\u06FF')\n    total_chars = len(text)\n    if total_chars > 0 and sindhi_chars / total_chars > 0.5:\n        return \"Sindhi\"\n    text_no_newlines = text.replace('\\n', ' ')\n    lang = ft_model.predict(text_no_newlines)[0][0].replace('__label__', '')\n    return {'hi': 'Hindi', 'mr': 'Marathi', 'gu': 'Gujarati', 'bn': 'Bengali', 'ta': 'Tamil', 'kn': 'Kannada', 'te': 'Telugu', 'ml': 'Malayalam', 'pa': 'Punjabi', 'or': 'Odia', 'as': 'Assamese'}.get(lang, None)\n\ndef process_pdf(pdf_path):\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n    if not text:\n        return []\n    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    return [{\"text\": para, \"language\": detect_language(para, hinted_lang)} for para in paragraphs if filter_text_length(para)]\n\ndef process_pdfs_parallel(pdf_dir=\"/kaggle/input/indic-data-corpuss3/pdfs data corpus\"):\n    pdf_files = [os.path.join(lang_path, f) for lang_folder in os.listdir(pdf_dir) if os.path.isdir(lang_path := os.path.join(pdf_dir, lang_folder)) for f in os.listdir(lang_path) if f.endswith(\".pdf\")]\n    with Pool(cpu_count()) as pool:\n        results = pool.map(process_pdf, pdf_files)\n    df = pd.DataFrame([item for sublist in results for item in sublist])\n    df.to_parquet(\"/kaggle/working/temp_pdf.parquet\", compression='gzip')\n    return len(df)\n\ndef load_oscar():\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            oscar_data.extend({\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} for item in dataset if filter_text_length(item['text']))\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    df = pd.DataFrame(oscar_data).sample(min(3000000, len(oscar_data)))\n    df.to_parquet(\"/kaggle/working/temp_oscar.parquet\", compression='gzip')\n    return len(df)\n\ndef load_samanantar():\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n    samanantar_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} for row in dataset for lang_code, lang_name in samanantar_langs.items() if row.get(lang_code) and filter_text_length(row[lang_code])]\n    df = pd.DataFrame(samanantar_data).sample(min(2000000, len(samanantar_data)))\n    df.to_parquet(\"/kaggle/working/temp_samanantar.parquet\", compression='gzip')\n    return len(df)\n\ndef build_corpus(pdf_dir=\"/kaggle/input/indic-data-corpuss3/pdfs data corpus\", output_file=\"/kaggle/working/indic_corpus.parquet\"):\n    temp_files = [\"/kaggle/working/temp_pdf.parquet\", \"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    pdf_count = process_pdfs_parallel(pdf_dir)\n    oscar_count = load_oscar()\n    samanantar_count = load_samanantar()\n    \n    if os.path.exists(output_file):\n        existing_df = spark.read.parquet(output_file)\n        new_df = spark.read.parquet(*temp_files)\n        combined_df = existing_df.union(new_df).dropDuplicates([\"text\"])\n    else:\n        combined_df = spark.read.parquet(*temp_files).dropDuplicates([\"text\"])\n    \n    balanced_df = combined_df.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).filter(F.col(\"language\").isin(LANGUAGES)).join(combined_df, \"language\").orderBy(F.rand()).limit(TARGET_PER_LANG).select(\"text\", \"language\")\n    balanced_df.write.parquet(output_file, mode=\"overwrite\", compression=\"gzip\")\n    total_samples = balanced_df.count()\n    print(f\"Saved {total_samples} samples to {output_file}\")\n    dist = balanced_df.groupBy(\"language\").count().collect()\n    for row in dist:\n        print(f\"{row['language']}: {row['count']}\")\n    \n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n    print(f\"Output size: {size_mb:.2f} MB\")\n    if size_mb > 19000:\n        print(\"Warning: File exceeds 19GB, may not fit Kaggle limit!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T19:06:36.045803Z","iopub.execute_input":"2025-03-29T19:06:36.046181Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/Sindhi POetry.pdfProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/unset0000unse_i8n3.pdfProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/choondsindhikaha0000unse.pdfProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/pgsl.31053.bestsindhisabham0000hola.pdf\n\n\n\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/choondsindhikaha0000bhag.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/janhamamu_oriya_1980_october_chandamama.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/67dddf0db8712.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/Chha-Mana-Atha-Guntha_FakirMohanSenapati_www.OdiaBooks.com.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/Raskel.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/Purnachandra.Odia.Bhashakosha-Volume.1-The.Vowels.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/janhamamu_oriya_1980_november_chandamama.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/2015.451834.Anuradha-Sarma.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/Assamese - The Apocrypha.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/oldtestamentina00auxigoog.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/2015.451589.Asam-Sahitya-Sabhar-Ruplekha.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\n","output_type":"stream"},{"name":"stderr","text":"Process ForkPoolWorker-4:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-1-41a949784af2>\", line 101, in process_pdf\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\nProcess ForkPoolWorker-2:\n  File \"<ipython-input-1-41a949784af2>\", line 79, in extract_text_from_pdf\n    text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n  File \"<ipython-input-1-41a949784af2>\", line 79, in <listcomp>\n    text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 486, in image_to_string\n    return {\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 489, in <lambda>\n    Output.STRING: lambda: run_and_get_output(*args),\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 352, in run_and_get_output\n    run_tesseract(**kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 282, in run_tesseract\n    with timeout_manager(proc, timeout) as error_string:\nProcess ForkPoolWorker-3:\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 144, in timeout_manager\n    yield proc.communicate()[1]\n  File \"<ipython-input-1-41a949784af2>\", line 101, in process_pdf\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n  File \"/usr/lib/python3.10/subprocess.py\", line 1154, in communicate\n    stdout, stderr = self._communicate(input, endtime, timeout)\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"<ipython-input-1-41a949784af2>\", line 79, in extract_text_from_pdf\n    text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n  File \"/usr/lib/python3.10/subprocess.py\", line 2021, in _communicate\n    ready = selector.select(timeout)\n  File \"<ipython-input-1-41a949784af2>\", line 79, in <listcomp>\n    text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n    fd_event_list = self._selector.poll(timeout)\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 486, in image_to_string\n    return {\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 382, in signal_handler\n    raise KeyboardInterrupt()\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 489, in <lambda>\n    Output.STRING: lambda: run_and_get_output(*args),\nKeyboardInterrupt\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 352, in run_and_get_output\n    run_tesseract(**kwargs)\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-1-41a949784af2>\", line 101, in process_pdf\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 282, in run_tesseract\n    with timeout_manager(proc, timeout) as error_string:\n  File \"<ipython-input-1-41a949784af2>\", line 79, in extract_text_from_pdf\n    text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n  File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"<ipython-input-1-41a949784af2>\", line 79, in <listcomp>\n    text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 144, in timeout_manager\n    yield proc.communicate()[1]\n  File \"/usr/lib/python3.10/subprocess.py\", line 1154, in communicate\n    stdout, stderr = self._communicate(input, endtime, timeout)\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 486, in image_to_string\n    return {\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 489, in <lambda>\n    Output.STRING: lambda: run_and_get_output(*args),\n  File \"/usr/lib/python3.10/subprocess.py\", line 2021, in _communicate\n    ready = selector.select(timeout)\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 352, in run_and_get_output\n    run_tesseract(**kwargs)\n  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n    fd_event_list = self._selector.poll(timeout)\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 282, in run_tesseract\n    with timeout_manager(proc, timeout) as error_string:\n  File \"/usr/lib/python3.10/contextlib.py\", line 135, in __enter__\n    return next(self.gen)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 382, in signal_handler\n    raise KeyboardInterrupt()\n  File \"/usr/local/lib/python3.10/dist-packages/pytesseract/pytesseract.py\", line 144, in timeout_manager\n    yield proc.communicate()[1]\nKeyboardInterrupt\n  File \"/usr/lib/python3.10/subprocess.py\", line 1154, in communicate\n    stdout, stderr = self._communicate(input, endtime, timeout)\n  File \"/usr/lib/python3.10/subprocess.py\", line 2021, in _communicate\n    ready = selector.select(timeout)\n  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n    fd_event_list = self._selector.poll(timeout)\n  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 382, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!apt-get update && apt-get install -y poppler-utils tesseract-ocr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:03:23.900487Z","iopub.execute_input":"2025-03-29T18:03:23.900973Z","iopub.status.idle":"2025-03-29T18:03:35.258960Z","shell.execute_reply.started":"2025-03-29T18:03:23.900935Z","shell.execute_reply":"2025-03-29T18:03:35.257897Z"}},"outputs":[{"name":"stdout","text":"Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\nHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                                              \nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]                             \nGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]                \nGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]                           \nGet:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]                                \nGet:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]\nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\nGet:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\nGet:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:12 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]    \nGet:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]              \nGet:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]        \nHit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease                        \nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]           \nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\nGet:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]          \nGet:20 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]    \nGet:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\nGet:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nGet:23 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,790 kB]\nGet:24 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]\nGet:25 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,686 kB]                    \nGet:26 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\nFetched 30.2 MB in 2s (12.7 MB/s)                           \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr is already the newest version (4.1.1-2.1build1).\nThe following additional packages will be installed:\n  libpoppler-dev libpoppler-private-dev libpoppler118\nThe following NEW packages will be installed:\n  poppler-utils\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118\n3 upgraded, 1 newly installed, 0 to remove and 166 not upgraded.\nNeed to get 1,462 kB of archives.\nAfter this operation, 696 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.6 [199 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.6 [5,184 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.6 [1,071 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\nFetched 1,462 kB in 0s (5,973 kB/s)    \n(Reading database ... 127448 files and directories currently installed.)\nPreparing to unpack .../libpoppler-private-dev_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nPreparing to unpack .../libpoppler-dev_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nPreparing to unpack .../libpoppler118_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nSelecting previously unselected package poppler-utils.\nPreparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking poppler-utils (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.6) ...\nSetting up poppler-utils (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.6) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.4) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}