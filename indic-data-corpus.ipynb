{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11208776,"sourceType":"datasetVersion","datasetId":6998952}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:16.708605Z","iopub.execute_input":"2025-03-20T14:00:16.708897Z","iopub.status.idle":"2025-03-20T14:00:16.807475Z","shell.execute_reply.started":"2025-03-20T14:00:16.708867Z","shell.execute_reply":"2025-03-20T14:00:16.806643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T08:31:30.538537Z","iopub.execute_input":"2025-03-20T08:31:30.538900Z","iopub.status.idle":"2025-03-20T08:31:31.855574Z","shell.execute_reply.started":"2025-03-20T08:31:30.538868Z","shell.execute_reply":"2025-03-20T08:31:31.853998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get update && apt-get install -y poppler-utils tesseract-ocr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:28.518368Z","iopub.execute_input":"2025-03-20T14:00:28.518650Z","iopub.status.idle":"2025-03-20T14:00:48.221644Z","shell.execute_reply.started":"2025-03-20T14:00:28.518629Z","shell.execute_reply":"2025-03-20T14:00:48.220553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pandas requests beautifulsoup4 scrapy datasets pdfminer.six clean-text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:00:48.222872Z","iopub.execute_input":"2025-03-20T14:00:48.223121Z","iopub.status.idle":"2025-03-20T14:01:03.547210Z","shell.execute_reply.started":"2025-03-20T14:00:48.223099Z","shell.execute_reply":"2025-03-20T14:01:03.546155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:01:03.548831Z","iopub.execute_input":"2025-03-20T14:01:03.549173Z","iopub.status.idle":"2025-03-20T14:01:09.534313Z","shell.execute_reply.started":"2025-03-20T14:01:03.549142Z","shell.execute_reply":"2025-03-20T14:01:09.533216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\"]\nTARGET_PER_LANG = 250000  \nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    words = text.split()\n    return len(words) >= MIN_WORDS\n\ndef preprocess_image(image):\n    \"\"\"Preprocess scanned images to improve OCR accuracy.\"\"\"\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    processed_image = cv2.dilate(binary, kernel, iterations=1)\n    return Image.fromarray(processed_image)\n\ndef extract_text_from_pdf(pdf_path, batch_size=10):  \n    \"\"\"Extract text from scanned PDFs using OCR in batches.\"\"\"\n    try:\n        images = convert_from_path(pdf_path, dpi=200, first_page=1, last_page=20)  \n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ) + \"\\n\"\n            del batch_images\n            processed_image = None\n            gc.collect()\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\ndef detect_language_unicode(text):\n    \"\"\"Detects and labels language based on Unicode ranges.\"\"\"\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),  # Devanagari\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'), # Devanagari (overlap with Hindi)\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),  # Arabic script\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),    # Arabic script\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    \n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    \n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    \n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\ndef save_df_in_chunks(df, output_file, chunk_size=10000):\n    \"\"\"Save large dataframe in chunks to avoid memory issues.\"\"\"\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, \n                    mode=mode, \n                    header=header, \n                    index=False, \n                    escapechar='\\\\', \n                    encoding='utf-8-sig')\n        # Clear memory\n        del chunk\n        gc.collect()\n    print(f\"Saved {len(df)} samples to {output_file} in chunks\")\n\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    all_samples = 0\n    oscar_langs = {\n        \"hi\": \"Hindi\",\n        \"mr\": \"Marathi\",\n        \"gu\": \"Gujarati\",\n        \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\",\n        \"kn\": \"Kannada\"\n    }\n    \n    open(output_file, 'w', encoding='utf-8-sig').close()\n    \n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = min(10000, TARGET_PER_LANG)\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            \n            total_lang_samples = 0\n            for i in range(num_chunks):\n                start_idx = i * chunk_size\n                end_idx = min((i + 1) * chunk_size, len(dataset))\n                \n                chunk_data = dataset[start_idx:end_idx]\n                chunk_df = pd.DataFrame(chunk_data)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                \n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    # Write chunk to file\n                    mode = 'a' if i > 0 or all_samples > 0 else 'w'\n                    header = i == 0 and all_samples == 0\n                    filtered_df.to_csv(output_file, \n                                      mode=mode, \n                                      header=header, \n                                      index=False, \n                                      escapechar='\\\\', \n                                      encoding='utf-8-sig')\n                    \n                    samples_added = len(filtered_df)\n                    total_lang_samples += samples_added\n                    all_samples += samples_added\n                \n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                \n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n                    \n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n            \n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    \n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"]\n    }\n    \n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    \n    for lang, urls in sites.items():\n        lang_texts = []\n        max_samples_per_url = TARGET_PER_LANG // len(urls)\n        \n        for url in urls:\n            try:\n                response = requests.get(url, headers=headers, timeout=10)\n                soup = BeautifulSoup(response.content, \"html.parser\")\n                paragraphs = soup.find_all(\"p\")\n                \n                for p in paragraphs:\n                    if len(lang_texts) >= max_samples_per_url:\n                        break\n                        \n                    text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True)\n                    if filter_text_length(text):\n                        lang_texts.append({\"text\": text, \"language\": lang})\n                        \n                # Save immediately if we have enough data\n                if len(lang_texts) >= max_samples_per_url:\n                    break\n                    \n            except Exception as e:\n                print(f\"Error scraping {url}: {e}\")\n        \n        # Save language data\n        if lang_texts:\n            lang_df = pd.DataFrame(lang_texts)\n            mode = 'a' if all_samples > 0 else 'w'\n            header = all_samples == 0\n            lang_df.to_csv(output_file, \n                          mode=mode, \n                          header=header, \n                          index=False, \n                          escapechar='\\\\', \n                          encoding='utf-8-sig')\n            \n            all_samples += len(lang_df)\n            print(f\"Scraped {lang}: {len(lang_df)} samples\")\n            \n            del lang_df, lang_texts\n            gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    \n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    \n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    # Limit to 10 PDFs max to prevent memory issues\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:10]\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        \n        try:\n            # Try pdfminer for searchable PDFs first\n            text = extract_text(pdf_path)\n            if not text.strip():  # If empty, use OCR\n                text = extract_text_from_pdf(pdf_path, batch_size=5)  # Reduced batch size\n            \n            if not text:\n                continue\n            \n            # Split into paragraphs\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            \n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, lang_probs, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:  # Only include target languages\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            \n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, \n                             mode=mode, \n                             header=header, \n                             index=False, \n                             escapechar='\\\\', \n                             encoding='utf-8-sig')\n                \n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                \n                del pdf_df, pdf_corpus\n                gc.collect()\n            \n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n        \n        text = None\n        paragraphs = None\n        gc.collect()\n    \n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    \n    # Process each source individually and merge at the end\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()  # Force garbage collection\n    \n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        \n        open(output_file, 'w', encoding='utf-8-sig').close()\n        \n        # Process each language separately\n        for lang in LANGUAGES:\n            lang_samples = []\n            \n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    # Read in chunks\n                    chunk_size = 10000\n                    for chunk in pd.read_csv(source_file, \n                                            chunksize=chunk_size, \n                                            encoding='utf-8-sig', \n                                            escapechar='\\\\'):\n                        # Filter for current language\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            \n                            # If we have enough samples, stop reading\n                            total_samples = sum(len(df) for df in lang_samples)\n                            if total_samples >= TARGET_PER_LANG:\n                                break\n                    \n                    gc.collect()\n            \n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                available_samples = len(combined_lang)\n                \n                if available_samples > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                \n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, \n                                    mode=mode, \n                                    header=header, \n                                    index=False, \n                                    escapechar='\\\\', \n                                    encoding='utf-8-sig')\n                \n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                \n                # Clear memory\n                del combined_lang, lang_samples\n                gc.collect()\n            else:\n                print(f\"No samples found for {lang}\")\n        \n        total_lines = 0\n        with open(output_file, 'r', encoding='utf-8-sig') as f:\n            for _ in f:\n                total_lines += 1\n        \n        total_lines -= 1  # Subtract header line\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        \n        # Clean up temporary files\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:06:57.658914Z","iopub.execute_input":"2025-03-10T17:06:57.659297Z","iopub.status.idle":"2025-03-10T17:44:04.813424Z","shell.execute_reply.started":"2025-03-10T17:06:57.659268Z","shell.execute_reply":"2025-03-10T17:44:04.811934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"indic_corpus.csv\", encoding='utf-8-sig', escapechar='\\\\')","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:46:28.961732Z","iopub.execute_input":"2025-03-10T17:46:28.962301Z","iopub.status.idle":"2025-03-10T17:48:35.670113Z","shell.execute_reply.started":"2025-03-10T17:46:28.962263Z","shell.execute_reply":"2025-03-10T17:48:35.668976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install -y tesseract-ocr tesseract-ocr-hin tesseract-ocr-mar tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-kan tesseract-ocr-eng tesseract-ocr-urd tesseract-ocr-chi-sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:31.700099Z","iopub.execute_input":"2025-03-20T14:02:31.700473Z","iopub.status.idle":"2025-03-20T14:02:38.713557Z","shell.execute_reply.started":"2025-03-20T14:02:31.700442Z","shell.execute_reply":"2025-03-20T14:02:38.712489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdf2image pytesseract opencv-python pillow clean-text langdetect datasets requests beautifulsoup4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:38.715217Z","iopub.execute_input":"2025-03-20T14:02:38.715600Z","iopub.status.idle":"2025-03-20T14:02:42.161361Z","shell.execute_reply.started":"2025-03-20T14:02:38.715560Z","shell.execute_reply":"2025-03-20T14:02:42.160350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdfminer pdf2image pytesseract langdetect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:02:42.163299Z","iopub.execute_input":"2025-03-20T14:02:42.163627Z","iopub.status.idle":"2025-03-20T14:03:07.039191Z","shell.execute_reply.started":"2025-03-20T14:02:42.163592Z","shell.execute_reply":"2025-03-20T14:03:07.038381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pdfminer.six==20231228\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:07:00.074390Z","iopub.execute_input":"2025-03-20T14:07:00.074773Z","iopub.status.idle":"2025-03-20T14:07:04.442186Z","shell.execute_reply.started":"2025-03-20T14:07:00.074748Z","shell.execute_reply":"2025-03-20T14:07:04.440969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install cleantext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:03:10.534805Z","iopub.execute_input":"2025-03-20T14:03:10.535056Z","iopub.status.idle":"2025-03-20T14:03:14.196236Z","shell.execute_reply.started":"2025-03-20T14:03:10.535032Z","shell.execute_reply":"2025-03-20T14:03:14.195167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:12:23.074577Z","iopub.execute_input":"2025-03-10T15:12:23.074905Z","iopub.status.idle":"2025-03-10T15:12:27.721704Z","shell.execute_reply.started":"2025-03-10T15:12:23.074883Z","shell.execute_reply":"2025-03-10T15:12:27.720648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom datasets import load_dataset\ntry:\n    from pdfminer.high_level import extract_text\nexcept ImportError:\n    def extract_text(pdf_path):\n        print(f\"Using fallback extraction for {pdf_path} due to pdfminer import issue\")\n        return \"\"\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nfrom cleantext import clean\nimport gc\nfrom langdetect import detect_langs\nimport concurrent.futures\nimport time\n\nLANGUAGES = [\n    \"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\",\n    \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"\n]\nTARGET_PER_LANG = 250000  # Reduced for faster runtime\nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    if not isinstance(text, str):\n        return False\n    return len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    kernel = np.ones((1, 1), np.uint8)\n    return Image.fromarray(cv2.dilate(binary, kernel, iterations=1))\n\ndef extract_text_from_pdf(pdf_path, batch_size=5):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        start_time = time.time()\n        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=10)  # Reduced DPI and pages\n        text = \"\"\n        for i in range(0, len(images), batch_size):\n            batch_images = images[i:i + batch_size]\n            for image in batch_images:\n                processed_image = preprocess_image(image)\n                text += pytesseract.image_to_string(\n                    processed_image,\n                    lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi',\n                    config=\"--psm 6 --oem 1\"\n                ).replace('\\0', '')[:100000] + \"\\n\"  # Clean NULL, truncate\n            del batch_images\n            gc.collect()\n        print(f\"Finished {pdf_path} in {time.time() - start_time:.2f}s\")\n        return text.strip()\n    except Exception as e:\n        print(f\"Error extracting text from {pdf_path}: {e}\")\n        return \"\"\n\ndef detect_language_unicode(text):\n    lang_counts = {\n        'Hindi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Marathi': sum(1 for char in text if '\\u0900' <= char <= '\\u097F'),\n        'Sindhi': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Gujarati': sum(1 for char in text if '\\u0A80' <= char <= '\\u0AFF'),\n        'Bengali': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Tamil': sum(1 for char in text if '\\u0B80' <= char <= '\\u0BFF'),\n        'Kannada': sum(1 for char in text if '\\u0C80' <= char <= '\\u0CFF'),\n        'Telugu': sum(1 for char in text if '\\u0C00' <= char <= '\\u0C7F'),\n        'Malayalam': sum(1 for char in text if '\\u0D00' <= char <= '\\u0D7F'),\n        'Punjabi': sum(1 for char in text if '\\u0A00' <= char <= '\\u0A7F'),\n        'Odia': sum(1 for char in text if '\\u0B00' <= char <= '\\u0B7F'),\n        'Assamese': sum(1 for char in text if '\\u0980' <= char <= '\\u09FF'),\n        'Urdu': sum(1 for char in text if '\\u0600' <= char <= '\\u06FF'),\n        'Chinese': sum(1 for char in text if '\\u4E00' <= char <= '\\u9FFF'),\n        'English': sum(1 for char in text if '\\u0041' <= char <= '\\u007A' or '\\u0061' <= char <= '\\u007A')\n    }\n    total_chars = sum(lang_counts.values())\n    if total_chars == 0:\n        return None, {}, {}\n    lang_probs = {lang: count / total_chars for lang, count in lang_counts.items()}\n    top_lang = max(lang_probs, key=lang_probs.get)\n    if top_lang in LANGUAGES:\n        return top_lang, lang_probs, {top_lang: text}\n    return None, lang_probs, {}\n\ndef save_df_in_chunks(df, output_file, chunk_size=5000):\n    for i in range(0, len(df), chunk_size):\n        chunk = df.iloc[i:i + chunk_size]\n        mode = 'w' if i == 0 else 'a'\n        header = i == 0\n        chunk.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n        del chunk\n        gc.collect()\n\ndef load_from_datasets(output_file=\"dataset_corpus.csv\"):\n    print(\"Loading from public datasets...\")\n    oscar_langs = {\n        \"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\",\n        \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\",\n        \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"\n    }\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    for code, lang in oscar_langs.items():\n        try:\n            print(f\"Loading OSCAR for {lang}...\")\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            chunk_size = 5000\n            num_chunks = min(TARGET_PER_LANG // chunk_size, len(dataset) // chunk_size)\n            total_lang_samples = 0\n            \n            for i in range(num_chunks):\n                chunk_data = dataset[i * chunk_size:(i + 1) * chunk_size]\n                chunk_df = pd.DataFrame(chunk_data)\n                chunk_df['text'] = chunk_df['text'].apply(lambda x: x.replace('\\0', '')[:100000] if isinstance(x, str) else x)\n                filtered_df = chunk_df[chunk_df['text'].apply(filter_text_length)]\n                if not filtered_df.empty:\n                    filtered_df = filtered_df[['text']].assign(language=lang)\n                    mode = 'a' if all_samples > 0 else 'w'\n                    header = all_samples == 0\n                    filtered_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                    total_lang_samples += len(filtered_df)\n                    all_samples += len(filtered_df)\n                del chunk_data, chunk_df, filtered_df\n                gc.collect()\n                if total_lang_samples >= TARGET_PER_LANG:\n                    break\n            print(f\"Loaded {total_lang_samples} {lang} samples from OSCAR\")\n        except Exception as e:\n            print(f\"Error loading OSCAR for {lang}: {e}\")\n    print(f\"Saved total of {all_samples} samples from datasets to {output_file}\")\n    return all_samples\n\ndef scrape_url(url, lang, max_samples):\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    lang_texts = []\n    try:\n        response = requests.get(url, headers=headers, timeout=5)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        paragraphs = soup.find_all(\"p\")\n        for p in paragraphs:\n            if len(lang_texts) >= max_samples:\n                break\n            text = clean(p.get_text(), no_line_breaks=True, no_urls=True, no_emails=True).replace('\\0', '')[:100000]\n            if filter_text_length(text):\n                lang_texts.append({\"text\": text, \"language\": lang})\n        print(f\"Scraped {url}: {len(lang_texts)} samples\")\n    except Exception as e:\n        print(f\"Error scraping {url}: {e}\")\n    return lang_texts\n\ndef scrape_from_web(output_file=\"scraped_corpus.csv\"):\n    print(\"Scraping from web...\")\n    sites = {\n        \"Hindi\": [\"https://hindi.bbc.com\", \"https://www.bhaskar.com\"],\n        \"Marathi\": [\"https://lokmat.com\", \"https://maharashtratimes.com\"],\n        \"Sindhi\": [\"https://awamiawaz.pk\"],\n        \"Gujarati\": [\"https://divyabhaskar.co.in\", \"https://sandesh.com\"],\n        \"Bengali\": [\"https://anandabazar.com\", \"https://eisamay.com\"],\n        \"Tamil\": [\"https://dinamalar.com\", \"https://dailythanthi.com\"],\n        \"Kannada\": [\"https://prajavani.net\", \"https://vijaykarnataka.com\"],\n        \"Telugu\": [\"https://eenadu.net\", \"https://sakshi.com\"],\n        \"Malayalam\": [\"https://mathrubhumi.com\", \"https://www.manoramaonline.com\"],\n        \"Punjabi\": [\"https://punjabitribuneonline.com\", \"https://www.ajitjalandhar.com\"],\n        \"Odia\": [\"https://sambad.in\", \"https://dharitri.com\"],\n        \"Assamese\": [\"https://asomiyapratidin.in\", \"https://pratidintime.com\"]\n    }\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        for lang, urls in sites.items():\n            max_samples_per_url = TARGET_PER_LANG // len(urls)\n            future_to_url = {executor.submit(scrape_url, url, lang, max_samples_per_url): url for url in urls}\n            lang_texts = []\n            for future in concurrent.futures.as_completed(future_to_url):\n                lang_texts.extend(future.result())\n            if lang_texts:\n                lang_df = pd.DataFrame(lang_texts)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                lang_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                all_samples += len(lang_df)\n                print(f\"Scraped {lang}: {len(lang_df)} samples\")\n                del lang_df, lang_texts\n                gc.collect()\n    print(f\"Saved total of {all_samples} samples from web to {output_file}\")\n    return all_samples\n\ndef extract_from_pdfs(pdf_dir=\"/kaggle/input/pdddffs/allpdfs\", output_file=\"pdf_corpus.csv\"):\n    print(\"Extracting from PDFs...\")\n    if not os.path.exists(pdf_dir):\n        print(f\"PDF directory {pdf_dir} not found, skipping...\")\n        return 0\n    open(output_file, 'w', encoding='utf-8-sig').close()\n    all_samples = 0\n    pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")][:5]  # Limit to 5 PDFs\n    \n    for pdf_file in pdf_files:\n        pdf_path = os.path.join(pdf_dir, pdf_file)\n        pdf_corpus = []\n        try:\n            # Modified to handle pdfminer error more gracefully\n            try:\n                text = extract_text(pdf_path)\n            except Exception as e:\n                print(f\"Primary PDF extraction failed: {e}, using backup method\")\n                text = \"\"\n                \n            if not text.strip():\n                text = extract_text_from_pdf(pdf_path)\n            if not text:\n                continue\n            paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n            for para in paragraphs:\n                if filter_text_length(para):\n                    top_lang, _, _ = detect_language_unicode(para)\n                    if top_lang in LANGUAGES:\n                        pdf_corpus.append({\"text\": para, \"language\": top_lang})\n            if pdf_corpus:\n                pdf_df = pd.DataFrame(pdf_corpus)\n                mode = 'a' if all_samples > 0 else 'w'\n                header = all_samples == 0\n                pdf_df.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                all_samples += len(pdf_df)\n                print(f\"Extracted {len(pdf_df)} samples from {pdf_file}\")\n                del pdf_df, pdf_corpus\n                gc.collect()\n        except Exception as e:\n            print(f\"Error processing {pdf_path}: {e}\")\n    print(f\"Saved total of {all_samples} samples from PDFs to {output_file}\")\n    return all_samples\n\ndef build_corpus(output_file=\"indic_corpus.csv\"):\n    print(\"Building the corpus...\")\n    dataset_count = load_from_datasets(\"temp_dataset_corpus.csv\")\n    gc.collect()\n    scraped_count = scrape_from_web(\"temp_scraped_corpus.csv\")\n    gc.collect()\n    pdf_count = extract_from_pdfs(output_file=\"temp_pdf_corpus.csv\")\n    gc.collect()\n    \n    if dataset_count + scraped_count + pdf_count > 0:\n        print(\"Merging and balancing corpus...\")\n        open(output_file, 'w', encoding='utf-8-sig').close()\n        for lang in LANGUAGES:\n            lang_samples = []\n            for source_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n                if os.path.exists(source_file) and os.path.getsize(source_file) > 0:\n                    for chunk in pd.read_csv(source_file, chunksize=5000, encoding='utf-8-sig', escapechar='\\\\'):\n                        lang_chunk = chunk[chunk['language'] == lang]\n                        if not lang_chunk.empty:\n                            lang_samples.append(lang_chunk)\n                            if sum(len(df) for df in lang_samples) >= TARGET_PER_LANG:\n                                break\n                    gc.collect()\n            if lang_samples:\n                combined_lang = pd.concat(lang_samples)\n                if len(combined_lang) > TARGET_PER_LANG:\n                    combined_lang = combined_lang.sample(n=TARGET_PER_LANG, random_state=42)\n                mode = 'a' if os.path.getsize(output_file) > 0 else 'w'\n                header = mode == 'w'\n                combined_lang.to_csv(output_file, mode=mode, header=header, index=False, escapechar='\\\\', encoding='utf-8-sig')\n                print(f\"Added {len(combined_lang)} {lang} samples to final corpus\")\n                del combined_lang, lang_samples\n                gc.collect()\n        total_lines = sum(1 for _ in open(output_file, 'r', encoding='utf-8-sig')) - 1  # Subtract header\n        print(f\"Final corpus saved to {output_file} with {total_lines} samples\")\n        for temp_file in [\"temp_dataset_corpus.csv\", \"temp_scraped_corpus.csv\", \"temp_pdf_corpus.csv\"]:\n            if os.path.exists(temp_file):\n                os.remove(temp_file)\n    else:\n        print(\"No data collected!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T14:16:57.249315Z","iopub.execute_input":"2025-03-20T14:16:57.249660Z","iopub.status.idle":"2025-03-20T15:07:41.554729Z","shell.execute_reply.started":"2025-03-20T14:16:57.249638Z","shell.execute_reply":"2025-03-20T15:07:41.533771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gzip\nimport pandas as pd\n\n# Decompress and clean NULL bytes\nwith gzip.open('/kaggle/working/dataset_corpus.csv.gz', 'rb') as f_in:\n    with open('/kaggle/working/dataset_corpus_clean.csv', 'wb') as f_out:\n        # Read raw bytes, replace NULL bytes\n        data = f_in.read().replace(b'\\0', b'')\n        f_out.write(data)\n\n# Load cleaned CSV\ndf = pd.read_csv('/kaggle/working/dataset_corpus_clean.csv', engine='python', escapechar='\\\\')\nprint(f\"OSCAR samples: {len(df)}\")\nscraped_df = scrape_from_web()\npdf_df = extract_from_pdfs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T10:38:56.438622Z","iopub.execute_input":"2025-03-20T10:38:56.438913Z","execution_failed":"2025-03-20T10:44:10.381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# corpus_construction.py\nimport pandas as pd\nfrom datasets import load_dataset\nimport os\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nTARGET_PER_LANG = 500000  # ~5.5M total for 11 langs (no Sindhi)\nMIN_WORDS = 50\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef load_oscar():\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            oscar_data.extend({\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} for item in dataset if filter_text_length(item['text']))\n            print(f\"Loaded OSCAR {lang}: {len([d for d in oscar_data if d['language'] == lang])} samples\")\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    df = pd.DataFrame(oscar_data).sample(min(3000000, len(oscar_data)))  # Cap at 3M\n    df.to_parquet(\"/kaggle/working/temp_oscar.parquet\", compression='gzip')\n    return len(df)\n\ndef load_samanantar():\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n    samanantar_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} \n                       for row in dataset for lang_code, lang_name in samanantar_langs.items() \n                       if row.get(lang_code) and filter_text_length(row[lang_code])]\n    df = pd.DataFrame(samanantar_data).sample(min(2000000, len(samanantar_data)))  # Cap at 2M\n    df.to_parquet(\"/kaggle/working/temp_samanantar.parquet\", compression='gzip')\n    return len(df)\n\ndef build_corpus(output_file=\"/kaggle/working/indic_corpus.parquet\"):\n    temp_files = [\"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    oscar_count = load_oscar()\n    samanantar_count = load_samanantar()\n    print(f\"OSCAR samples: {oscar_count}, Samanantar samples: {samanantar_count}\")\n    \n    # Merge datasets with Pandas\n    oscar_df = pd.read_parquet(\"/kaggle/working/temp_oscar.parquet\")\n    samanantar_df = pd.read_parquet(\"/kaggle/working/temp_samanantar.parquet\")\n    combined_df = pd.concat([oscar_df, samanantar_df]).drop_duplicates(subset=[\"text\"]).sample(frac=1)\n    \n    # Balance across languages\n    balanced_data = []\n    for lang in LANGUAGES:\n        lang_data = combined_df[combined_df[\"language\"] == lang]\n        if len(lang_data) > 0:\n            balanced_data.append(lang_data.sample(min(TARGET_PER_LANG, len(lang_data))))\n    balanced_df = pd.concat(balanced_data)\n    \n    # Save to Parquet\n    balanced_df.to_parquet(output_file, compression='gzip')\n    total_samples = len(balanced_df)\n    print(f\"Saved {total_samples} samples to {output_file}\")\n    for lang in LANGUAGES:\n        count = len(balanced_df[balanced_df[\"language\"] == lang])\n        print(f\"{lang}: {count}\")\n    \n    # Clean up\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    # Check size\n    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n    print(f\"Output size: {size_mb:.2f} MB\")\n    if size_mb > 19000:\n        print(\"Warning: File exceeds 19GB, may not fit Kaggle limit! Reduce TARGET_PER_LANG.\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:00:02.298977Z","iopub.execute_input":"2025-03-31T09:00:02.299349Z","execution_failed":"2025-03-31T09:18:10.955Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdabe25e1f8845b8a0a76ff7b4c53071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e2e38f70d4044ae85c4acd0d13d5e60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c112efae736548388e36207eb83e814b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8261cfeb3e2b4287be9b8438c1f308d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/409M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cafa3f72d744de79e15da841f3f244e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/406M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e1192ef9c1141f89a5fabe0ffcc7f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4236b0381cfa48799fe97ad114046f1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71bdc7e2c194c509049f1b8afbc51e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1909387 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74d7a61e201b46a88b72afb6e0c582a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea1a406535d4e528b692862f6048e58"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Hindi: 1586182 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e935a9e167d4a7ea8651b75f7794d9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"539621e41c6443e38dafc01f6611a9b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/212556 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eb7606559344665af9281f06792d487"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Marathi: 179667 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f869cb2be8fd4c6ea12b42f82c09da52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/163M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10bc625fc94403cb83f233ac6bf059e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/169834 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855d06b8c0aa41d9ac949601deb7aa53"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Gujarati: 124228 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/328 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b1789d290c24b3a94c055c5d05d8c77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c3b043e95c494b8c0101bec81b5701"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d1fa44856a741cb992cdf7978ecb931"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/391M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b145a04078c4148b4ed244a4d4645c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/86.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3893f0e7a394ad6a8c482951a9b1691"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1114481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"794998c6bb704a119f9b57ff1b1adc22"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Bengali: 940160 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/246 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e378378e4c644fec99d7b8105df890fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/342M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e5e7def15e4c959e2d0dc115db7e63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/343M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b484b8e3c68445f0a6a481639a595b1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/285M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7e519e2a8074a0dabf27a682bdfb1df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/833101 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d93519f50f40409abc6bb77be8eb53"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Tamil: 618835 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"570920a4181b40c896ed2244fd224d11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/216M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fada7544e239405192b23f3b3a6ab01c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/251064 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4836bc77ca459faa1b02def0bf590b"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Kannada: 178468 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c5bb656ae24d3c8fcb77d671fa6c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/342M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd9a3c42a1a4ae3b98762eeffdd92cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/312644 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ca111339a7443c9b939109e08041eb"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Telugu: 235171 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/164 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18775a447bad479aabea3d8fe2255b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/358M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15326649a6a443f587f92551f019d530"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/138M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c78451201047e0928f2e7b1181cf49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/453904 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39a9116384e340de8e660df81216d1b3"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Malayalam: 338428 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa45320f4eaa4e15a75452389b25b144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/102M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34d32bde35ad4726b54823eb2838ab69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87235 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a944298fc924ae7a16c0c16672302bd"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Punjabi: 73478 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"596eebac18374abe9a7472fe7e094746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/38.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e425723f7c499a859b41607604141a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/44230 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9391a07e3a49fd84818ddffebe040f"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Odia: 38360 samples\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aafb4b1f6604899932387b08c874859"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/15.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351c759b784649a89e9390fd26d6ce2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9212 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"720a3fbe358d44f2a13463d4377e26d8"}},"metadata":{}},{"name":"stdout","text":"Loaded OSCAR Assamese: 7957 samples\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pip install parquet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:55:17.613908Z","iopub.execute_input":"2025-03-31T08:55:17.614262Z","iopub.status.idle":"2025-03-31T08:55:47.856786Z","shell.execute_reply.started":"2025-03-31T08:55:17.614231Z","shell.execute_reply":"2025-03-31T08:55:47.855744Z"}},"outputs":[{"name":"stdout","text":"Collecting parquet\n  Downloading parquet-1.3.1-py3-none-any.whl.metadata (4.1 kB)\nCollecting thriftpy2 (from parquet)\n  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.10/dist-packages (from thriftpy2->parquet) (3.0.11)\nRequirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.10/dist-packages (from thriftpy2->parquet) (3.11)\nRequirement already satisfied: six~=1.15 in /usr/local/lib/python3.10/dist-packages (from thriftpy2->parquet) (1.17.0)\nDownloading parquet-1.3.1-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: thriftpy2\n  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for thriftpy2: filename=thriftpy2-0.5.2-cp310-cp310-linux_x86_64.whl size=1749932 sha256=d89599ab97f2ea2c19f3cbd9fdd2f4dbca66835892ef266369e0991d955a1b3a\n  Stored in directory: /root/.cache/pip/wheels/90/28/5f/279788e86e2eaccb3edc73bde9c815a9527602739a56344ff7\nSuccessfully built thriftpy2\nInstalling collected packages: thriftpy2, parquet\nSuccessfully installed parquet-1.3.1 thriftpy2-0.5.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install pdf2image pytesseract opencv-python pillow pdfminer.six datasets pyarrow pyspark fasttext\n!apt-get install -y tesseract-ocr tesseract-ocr-hin tesseract-ocr-mar tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-kan tesseract-ocr-tel tesseract-ocr-mal tesseract-ocr-pan tesseract-ocr-ori tesseract-ocr-asm tesseract-ocr-eng tesseract-ocr-urd tesseract-ocr-chi-sim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:01:39.222431Z","iopub.execute_input":"2025-03-31T08:01:39.222628Z","iopub.status.idle":"2025-03-31T08:01:52.664148Z","shell.execute_reply.started":"2025-03-31T08:01:39.222608Z","shell.execute_reply":"2025-03-31T08:01:52.663394Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\nRequirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nCollecting pdfminer.six\n  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.1)\nRequirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\nRequirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.3)\nRequirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\nRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.1)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (44.0.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.13.6)\nRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (75.1.0)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.21.2->opencv-python) (2.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.21.2->opencv-python) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.2->opencv-python) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.21.2->opencv-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.21.2->opencv-python) (2024.2.0)\nDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pdfminer.six\nSuccessfully installed pdfminer.six-20250327\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr is already the newest version (4.1.1-2.1build1).\ntesseract-ocr-eng is already the newest version (1:4.00~git30-7274cfa-1.1).\ntesseract-ocr-eng set to manually installed.\nThe following NEW packages will be installed:\n  tesseract-ocr-asm tesseract-ocr-ben tesseract-ocr-chi-sim tesseract-ocr-hin tesseract-ocr-kan\n  tesseract-ocr-mal tesseract-ocr-mar tesseract-ocr-ori tesseract-ocr-pan tesseract-ocr-tam\n  tesseract-ocr-tel tesseract-ocr-urd\n0 upgraded, 12 newly installed, 0 to remove and 129 not upgraded.\nNeed to get 13.1 MB of archives.\nAfter this operation, 27.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-asm all 1:4.00~git30-7274cfa-1.1 [1,421 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ben all 1:4.00~git30-7274cfa-1.1 [516 kB]\nGet:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-chi-sim all 1:4.00~git30-7274cfa-1.1 [1,634 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-hin all 1:4.00~git30-7274cfa-1.1 [913 kB]\nGet:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-kan all 1:4.00~git30-7274cfa-1.1 [1,659 kB]\nGet:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mal all 1:4.00~git30-7274cfa-1.1 [1,678 kB]\nGet:7 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-mar all 1:4.00~git30-7274cfa-1.1 [862 kB]\nGet:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-ori all 1:4.00~git30-7274cfa-1.1 [1,024 kB]\nGet:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-pan all 1:4.00~git30-7274cfa-1.1 [322 kB]\nGet:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tam all 1:4.00~git30-7274cfa-1.1 [1,071 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-tel all 1:4.00~git30-7274cfa-1.1 [1,012 kB]\nGet:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-urd all 1:4.00~git30-7274cfa-1.1 [1,000 kB]\nFetched 13.1 MB in 1s (10.2 MB/s)             \nSelecting previously unselected package tesseract-ocr-asm.\n(Reading database ... 127400 files and directories currently installed.)\nPreparing to unpack .../00-tesseract-ocr-asm_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-ben.\nPreparing to unpack .../01-tesseract-ocr-ben_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-chi-sim.\nPreparing to unpack .../02-tesseract-ocr-chi-sim_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-hin.\nPreparing to unpack .../03-tesseract-ocr-hin_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-kan.\nPreparing to unpack .../04-tesseract-ocr-kan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-mal.\nPreparing to unpack .../05-tesseract-ocr-mal_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-mar.\nPreparing to unpack .../06-tesseract-ocr-mar_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-ori.\nPreparing to unpack .../07-tesseract-ocr-ori_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-pan.\nPreparing to unpack .../08-tesseract-ocr-pan_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-tam.\nPreparing to unpack .../09-tesseract-ocr-tam_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-tel.\nPreparing to unpack .../10-tesseract-ocr-tel_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\nSelecting previously unselected package tesseract-ocr-urd.\nPreparing to unpack .../11-tesseract-ocr-urd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\nUnpacking tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-ben (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-tam (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-pan (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-ori (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-urd (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-mar (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-asm (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-mal (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-chi-sim (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-kan (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-hin (1:4.00~git30-7274cfa-1.1) ...\nSetting up tesseract-ocr-tel (1:4.00~git30-7274cfa-1.1) ...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# corpus_construction.py\nimport pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport torch\nimport os\nimport gc\nfrom multiprocessing import Pool, cpu_count\nfrom datasets import load_dataset\nimport fasttext\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"IndicCorpus\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nTARGET_PER_LANG = 500000  # ~6M total\nMIN_WORDS = 50\nBATCH_SIZE = 10  # Process 10 images at a time on GPU\n\n# FastText path\nFASTTEXT_PATH = \"/kaggle/input/fasttextefficienttextclassification/lid.176.bin\"\nft_model = fasttext.load_model(FASTTEXT_PATH)\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef preprocess_image_cpu(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    return Image.fromarray(cv2.dilate(binary, np.ones((1, 1), np.uint8), iterations=1))\n\ndef preprocess_image_gpu(images):\n    try:\n        # Resize images to fixed size (1024x1024) and convert to numpy\n        resized_images = [image.resize((1024, 1024), Image.Resampling.LANCZOS) for image in images]\n        image_arrays = [np.array(img) for img in resized_images]\n        \n        # Process in batches to avoid memory overload\n        processed_images = []\n        for i in range(0, len(image_arrays), BATCH_SIZE):\n            batch = image_arrays[i:i + BATCH_SIZE]\n            tensor_images = torch.tensor(np.stack(batch), dtype=torch.uint8).to(device)\n            \n            # Grayscale on GPU\n            gray_images = 0.299 * tensor_images[:, :, :, 0] + 0.587 * tensor_images[:, :, :, 1] + 0.114 * tensor_images[:, :, :, 2]\n            \n            # Thresholding on GPU\n            binary_images = (gray_images > 150).type(torch.uint8) * 255\n            \n            # Dilation on GPU\n            kernel = torch.ones((1, 1, 1, 1), dtype=torch.uint8).to(device)\n            dilated_images = torch.nn.functional.conv2d(binary_images.unsqueeze(1), kernel, padding=0).squeeze(1)\n            \n            # Back to CPU\n            processed_images.extend([Image.fromarray(img.cpu().numpy()) for img in dilated_images])\n        \n        return processed_images\n    except Exception as e:\n        print(f\"GPU preprocessing failed: {e}. Falling back to CPU.\")\n        return [preprocess_image_cpu(img) for img in images]\n\ndef extract_text_from_pdf(pdf_path):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        text = extract_text(pdf_path)\n        if not text.strip():\n            images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=50)\n            processed_images = preprocess_image_gpu(images) if torch.cuda.is_available() else [preprocess_image_cpu(img) for img in images]\n            text = \"\".join([pytesseract.image_to_string(img, lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in processed_images])\n            del processed_images, images\n            torch.cuda.empty_cache()  # Clear GPU memory\n            gc.collect()\n        hinted_lang = os.path.basename(os.path.dirname(pdf_path))\n        return text.strip(), hinted_lang if hinted_lang in LANGUAGES else None\n    except Exception as e:\n        print(f\"Error: {pdf_path}: {e}\")\n        return \"\", None\n\ndef detect_language(text, hinted_lang=None):\n    if hinted_lang in LANGUAGES:\n        return hinted_lang\n    sindhi_chars = sum(1 for char in text if '\\u0600' <= char <= '\\u06FF')\n    total_chars = len(text)\n    if total_chars > 0 and sindhi_chars / total_chars > 0.5:\n        return \"Sindhi\"\n    text_no_newlines = text.replace('\\n', ' ')\n    lang = ft_model.predict(text_no_newlines)[0][0].replace('__label__', '')\n    return {'hi': 'Hindi', 'mr': 'Marathi', 'gu': 'Gujarati', 'bn': 'Bengali', 'ta': 'Tamil', 'kn': 'Kannada', 'te': 'Telugu', 'ml': 'Malayalam', 'pa': 'Punjabi', 'or': 'Odia', 'as': 'Assamese'}.get(lang, None)\n\ndef process_pdf(pdf_path):\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n    if not text:\n        return []\n    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    return [{\"text\": para, \"language\": detect_language(para, hinted_lang)} for para in paragraphs if filter_text_length(para)]\n\ndef process_pdfs_parallel(pdf_dir=\"/kaggle/input/indic-data-corpuss3/pdfs data corpus\"):\n    pdf_files = [os.path.join(lang_path, f) for lang_folder in os.listdir(pdf_dir) if os.path.isdir(lang_path := os.path.join(pdf_dir, lang_folder)) for f in os.listdir(lang_path) if f.endswith(\".pdf\")]\n    with Pool(cpu_count()) as pool:\n        results = pool.map(process_pdf, pdf_files)\n    df = pd.DataFrame([item for sublist in results for item in sublist])\n    df.to_parquet(\"/kaggle/working/temp_pdf.parquet\", compression='gzip')\n    return len(df)\n\ndef load_oscar():\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            oscar_data.extend({\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} for item in dataset if filter_text_length(item['text']))\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    df = pd.DataFrame(oscar_data).sample(min(3000000, len(oscar_data)))\n    df.to_parquet(\"/kaggle/working/temp_oscar.parquet\", compression='gzip')\n    return len(df)\n\ndef load_samanantar():\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n    samanantar_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} for row in dataset for lang_code, lang_name in samanantar_langs.items() if row.get(lang_code) and filter_text_length(row[lang_code])]\n    df = pd.DataFrame(samanantar_data).sample(min(2000000, len(samanantar_data)))\n    df.to_parquet(\"/kaggle/working/temp_samanantar.parquet\", compression='gzip')\n    return len(df)\n\ndef build_corpus(pdf_dir=\"/kaggle/input/indic-data-corpuss3/pdfs data corpus\", output_file=\"/kaggle/working/indic_corpus.parquet\"):\n    temp_files = [\"/kaggle/working/temp_pdf.parquet\", \"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    pdf_count = process_pdfs_parallel(pdf_dir)\n    oscar_count = load_oscar()\n    samanantar_count = load_samanantar()\n    \n    if os.path.exists(output_file):\n        existing_df = spark.read.parquet(output_file)\n        new_df = spark.read.parquet(*temp_files)\n        combined_df = existing_df.union(new_df).dropDuplicates([\"text\"])\n    else:\n        combined_df = spark.read.parquet(*temp_files).dropDuplicates([\"text\"])\n    \n    balanced_df = combined_df.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).filter(F.col(\"language\").isin(LANGUAGES)).join(combined_df, \"language\").orderBy(F.rand()).limit(TARGET_PER_LANG).select(\"text\", \"language\")\n    balanced_df.write.parquet(output_file, mode=\"overwrite\", compression=\"gzip\")\n    total_samples = balanced_df.count()\n    print(f\"Saved {total_samples} samples to {output_file}\")\n    dist = balanced_df.groupBy(\"language\").count().collect()\n    for row in dist:\n        print(f\"{row['language']}: {row['count']}\")\n    \n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n    print(f\"Output size: {size_mb:.2f} MB\")\n    if size_mb > 19000:\n        print(\"Warning: File exceeds 19GB, may not fit Kaggle limit!\")\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:03:11.799604Z","iopub.execute_input":"2025-03-31T08:03:11.799931Z","iopub.status.idle":"2025-03-31T08:52:21.951620Z","shell.execute_reply.started":"2025-03-31T08:03:11.799904Z","shell.execute_reply":"2025-03-31T08:52:21.950143Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/choondsindhikaha0000unse.pdfProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/Sindhi POetry.pdfProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/pgsl.31053.bestsindhisabham0000hola.pdfProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/unset0000unse_i8n3.pdf\n\n\n\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Sindhi/choondsindhikaha0000bhag.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/janhamamu_oriya_1980_october_chandamama.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/67dddf0db8712.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/Chha-Mana-Atha-Guntha_FakirMohanSenapati_www.OdiaBooks.com.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/Raskel.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/Purnachandra.Odia.Bhashakosha-Volume.1-The.Vowels.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/odia/janhamamu_oriya_1980_november_chandamama.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/2015.451834.Anuradha-Sarma.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/Assamese - The Apocrypha.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/oldtestamentina00auxigoog.pdf\nProcessing PDF: /kaggle/input/indic-data-corpuss3/pdfs data corpus/Assamese/2015.451589.Asam-Sahitya-Sabhar-Ruplekha.pdf\nGPU preprocessing failed: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method. Falling back to CPU.\n","output_type":"stream"},{"name":"stderr","text":"Process ForkPoolWorker-1:\nProcess ForkPoolWorker-2:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-41a949784af2>\u001b[0m in \u001b[0;36m<cell line: 170>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mbuild_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-41a949784af2>\u001b[0m in \u001b[0;36mbuild_corpus\u001b[0;34m(pdf_dir, output_file)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0mpdf_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_pdfs_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0moscar_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_oscar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msamanantar_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_samanantar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-41a949784af2>\u001b[0m in \u001b[0;36mprocess_pdfs_parallel\u001b[0;34m(pdf_dir)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mpdf_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlang_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_path\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_pdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/temp_pdf.parquet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"!apt-get update && apt-get install -y poppler-utils tesseract-ocr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T08:02:50.392976Z","iopub.execute_input":"2025-03-31T08:02:50.393305Z","iopub.status.idle":"2025-03-31T08:03:01.843329Z","shell.execute_reply.started":"2025-03-31T08:02:50.393275Z","shell.execute_reply":"2025-03-31T08:03:01.842479Z"}},"outputs":[{"name":"stdout","text":"Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\nGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]                \nGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]                           \nHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                                              \nGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\nGet:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]                                \nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]                             \nGet:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [69.9 kB]                 \nGet:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]              \nGet:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,788 kB]  \nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]      \nGet:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\nGet:13 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]         \nHit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease                 \nGet:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]   \nGet:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]                    \nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]            \nGet:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [33.6 kB]   \nGet:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.8 kB]\nGet:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,737 kB]              \nGet:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,045 kB]                \nGet:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,892 kB]     \nGet:23 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,049 kB]         \nGet:24 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\nGet:25 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\nGet:26 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\nFetched 30.2 MB in 3s (12.0 MB/s)                            \nReading package lists... Done\nW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ntesseract-ocr is already the newest version (4.1.1-2.1build1).\nThe following additional packages will be installed:\n  libpoppler-dev libpoppler-private-dev libpoppler118\nThe following NEW packages will be installed:\n  poppler-utils\nThe following packages will be upgraded:\n  libpoppler-dev libpoppler-private-dev libpoppler118\n3 upgraded, 1 newly installed, 0 to remove and 166 not upgraded.\nNeed to get 1,462 kB of archives.\nAfter this operation, 696 kB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-private-dev amd64 22.02.0-2ubuntu0.6 [199 kB]\nGet:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-dev amd64 22.02.0-2ubuntu0.6 [5,184 B]\nGet:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler118 amd64 22.02.0-2ubuntu0.6 [1,071 kB]\nGet:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.6 [186 kB]\nFetched 1,462 kB in 0s (5,960 kB/s)     \n(Reading database ... 127448 files and directories currently installed.)\nPreparing to unpack .../libpoppler-private-dev_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nPreparing to unpack .../libpoppler-dev_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler-dev:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nPreparing to unpack .../libpoppler118_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking libpoppler118:amd64 (22.02.0-2ubuntu0.6) over (22.02.0-2ubuntu0.5) ...\nSelecting previously unselected package poppler-utils.\nPreparing to unpack .../poppler-utils_22.02.0-2ubuntu0.6_amd64.deb ...\nUnpacking poppler-utils (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler118:amd64 (22.02.0-2ubuntu0.6) ...\nSetting up poppler-utils (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler-dev:amd64 (22.02.0-2ubuntu0.6) ...\nSetting up libpoppler-private-dev:amd64 (22.02.0-2ubuntu0.6) ...\nProcessing triggers for man-db (2.10.2-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.4) ...\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install datasets pyarrow pyspark transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:33:29.721995Z","iopub.execute_input":"2025-03-31T09:33:29.722337Z","iopub.status.idle":"2025-03-31T09:33:35.416925Z","shell.execute_reply.started":"2025-03-31T09:33:29.722288Z","shell.execute_reply":"2025-03-31T09:33:35.415370Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (19.0.1)\nRequirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# corpus_construction.py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, rand\nfrom datasets import load_dataset\nimport os\n\n# Initialize Spark\nspark = SparkSession.builder \\\n    .appName(\"IndicCorpus\") \\\n    .config(\"spark.executor.memory\", \"8g\") \\\n    .config(\"spark.driver.memory\", \"8g\") \\\n    .config(\"spark.memory.offHeap.enabled\", \"true\") \\\n    .config(\"spark.memory.offHeap.size\", \"4g\") \\\n    .getOrCreate()\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nTARGET_PER_LANG = 400000  # ~4.4M total for 11 langs\nMIN_WORDS = 50\nCHUNK_SIZE = 50000  # Smaller chunks to manage memory\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef load_oscar_to_parquet(temp_file=\"/kaggle/working/temp_oscar.parquet\"):\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    total_samples = 0\n    first_write = True\n    \n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            print(f\"OSCAR {lang} dataset loaded with {len(dataset)} samples\")\n            lang_samples = 0\n            \n            for i in range(0, len(dataset), CHUNK_SIZE):\n                chunk = dataset[i:i + CHUNK_SIZE]\n                chunk_data = [{\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} \n                              for item in chunk if filter_text_length(item['text'])]\n                if chunk_data:\n                    chunk_df = spark.createDataFrame(chunk_data)\n                    mode = \"overwrite\" if first_write else \"append\"\n                    chunk_df.write.mode(mode).parquet(temp_file, compression=\"gzip\")\n                    first_write = False\n                    lang_samples += len(chunk_data)\n                    total_samples += len(chunk_data)\n                if total_samples >= 3000000:  # Cap at 3M\n                    break\n            print(f\"Loaded OSCAR {lang}: {lang_samples} samples\")\n            if total_samples >= 3000000:\n                break\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    \n    if total_samples == 0:\n        print(\"No OSCAR data loaded. Skipping OSCAR.\")\n        return 0\n    return total_samples\n\ndef load_samanantar_to_parquet(temp_file=\"/kaggle/working/temp_samanantar.parquet\"):\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    try:\n        dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n        print(f\"Samanantar dataset loaded with {len(dataset)} samples\")\n        total_samples = 0\n        first_write = True\n        \n        for i in range(0, len(dataset), CHUNK_SIZE):\n            chunk = dataset[i:i + CHUNK_SIZE]\n            chunk_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} \n                          for row in chunk for lang_code, lang_name in samanantar_langs.items() \n                          if row.get(lang_code) and filter_text_length(row[lang_code])]\n            if chunk_data:\n                chunk_df = spark.createDataFrame(chunk_data)\n                mode = \"overwrite\" if first_write else \"append\"\n                chunk_df.write.mode(mode).parquet(temp_file, compression=\"gzip\")\n                first_write = False\n                total_samples += len(chunk_data)\n            if total_samples >= 2000000:  # Cap at 2M\n                break\n        print(f\"Loaded Samanantar: {total_samples} samples\")\n        return total_samples\n    except Exception as e:\n        print(f\"Error Samanantar: {e}\")\n        return 0\n\ndef build_corpus(output_file=\"/kaggle/working/indic_corpus.parquet\"):\n    temp_files = [\"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    oscar_count = load_oscar_to_parquet()\n    samanantar_count = load_samanantar_to_parquet()\n    print(f\"OSCAR samples: {oscar_count}, Samanantar samples: {samanantar_count}\")\n    \n    if oscar_count == 0 and samanantar_count == 0:\n        print(\"No data loaded from OSCAR or Samanantar. Exiting.\")\n        spark.stop()\n        return\n    \n    # Merge datasets with Spark\n    combined_df = None\n    if oscar_count > 0:\n        oscar_df = spark.read.parquet(\"/kaggle/working/temp_oscar.parquet\")\n        combined_df = oscar_df\n    if samanantar_count > 0:\n        samanantar_df = spark.read.parquet(\"/kaggle/working/temp_samanantar.parquet\")\n        combined_df = samanantar_df if combined_df is None else combined_df.union(samanantar_df)\n    \n    if combined_df is None:\n        print(\"No combined DataFrame created. Exiting.\")\n        spark.stop()\n        return\n    \n    combined_df = combined_df.dropDuplicates([\"text\"]).orderBy(rand())\n    \n    # Balance across languages\n    balanced_df = combined_df.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")) \\\n                             .filter(col(\"language\").isin(LANGUAGES)) \\\n                             .join(combined_df, \"language\") \\\n                             .orderBy(rand()) \\\n                             .limit(TARGET_PER_LANG) \\\n                             .select(\"text\", \"language\")\n    \n    # Save to Parquet\n    balanced_df.write.mode(\"overwrite\").parquet(output_file, compression=\"gzip\")\n    total_samples = balanced_df.count()\n    print(f\"Saved {total_samples} samples to {output_file}\")\n    dist = balanced_df.groupBy(\"language\").count().collect()\n    for row in dist:\n        print(f\"{row['language']}: {row['count']}\")\n    \n    # Clean up\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    # Check size\n    size_mb = os.path.getsize(output_file) / (1024 * 1024)\n    print(f\"Output size: {size_mb:.2f} MB\")\n    if size_mb > 19000:\n        print(\"Warning: File exceeds 19GB, may not fit Kaggle limit!\")\n    \n    spark.stop()\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T09:33:46.400878Z","iopub.execute_input":"2025-03-31T09:33:46.401232Z","iopub.status.idle":"2025-03-31T09:40:14.583402Z","shell.execute_reply.started":"2025-03-31T09:33:46.401203Z","shell.execute_reply":"2025-03-31T09:40:14.581603Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/303k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f3f84263a11445b8c0e1ee6c8b7f91d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"oscar.py:   0%|          | 0.00/14.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b2931dfceec4a15a31aff2eabedfae0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/410 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"026b75a25f754eea8e0ac5faf0c9aca9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93e56a9de8dc4cffa4948d8e6df300c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/409M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b846b6e89ca42308abadf3701e83846"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/406M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fadcfc75d23647f0a1ac2e8b4aef63e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/407M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b6034f348ec437dbb3249ae3b8b8681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/380M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff95adaadabc4707b8d59f4a79ce7fdd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1909387 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ea723d775f4bdc87b81c0f2f855fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading dataset shards:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6742c3681e304e24a2143885a62e464d"}},"metadata":{}},{"name":"stdout","text":"OSCAR Hindi dataset loaded with 1909387 samples\nError OSCAR Hindi: string indices must be integers\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8026e7ad9f8944dbb48217113b3d3a13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/300M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"600cbd14b4d94bda8c153ab056cfcdb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/212556 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ad665c2a0f4549ac5660acc5f3efc0"}},"metadata":{}},{"name":"stdout","text":"OSCAR Marathi dataset loaded with 212556 samples\nError OSCAR Marathi: string indices must be integers\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e242bc2beb04c5db54157ab94f3132c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/163M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492c5fdc0cfe45de88515ffe091314bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/169834 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f00b6f4ca52480fad8e37bc07f16963"}},"metadata":{}},{"name":"stdout","text":"OSCAR Gujarati dataset loaded with 169834 samples\nError OSCAR Gujarati: string indices must be integers\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/328 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2cf0cd4c874a46aec9ff8a96e1c487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"effd47de7c5c46528d447224ae9968b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/390M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c68df6a180f46cca3e9d22766972d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/391M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54445bd844bd4f619d901ea5fb460900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/86.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be7447bf0f6144af8dc2d5ebcdf3c8a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1114481 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abefe11add144806b944db20c0e538c6"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-72ee2113f863>\u001b[0m in \u001b[0;36m<cell line: 146>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mbuild_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-72ee2113f863>\u001b[0m in \u001b[0;36mbuild_corpus\u001b[0;34m(output_file)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0moscar_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_oscar_to_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0msamanantar_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_samanantar_to_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OSCAR samples: {oscar_count}, Samanantar samples: {samanantar_count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-72ee2113f863>\u001b[0m in \u001b[0;36mload_oscar_to_parquet\u001b[0;34m(temp_file)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moscar_langs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oscar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"unshuffled_deduplicated_{code}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"OSCAR {lang} dataset loaded with {len(dataset)} samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlang_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2151\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m   2152\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2153\u001b[0m         \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                         \u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_proc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_proc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                     self._download_and_prepare(\n\u001b[0m\u001b[1;32m    925\u001b[0m                         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                         \u001b[0mverification_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_download_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_splits_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1648\u001b[0;31m         super()._download_and_prepare(\n\u001b[0m\u001b[1;32m   1649\u001b[0m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             \u001b[0mverification_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m                 \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 raise OSError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                 for job_id, done, content in self._prepare_split_single(\n\u001b[0m\u001b[1;32m   1487\u001b[0m                     \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_prepare_split_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m                 ):\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1623\u001b[0m                         )\n\u001b[1;32m   1624\u001b[0m                     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m                     \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhkey_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_examples_on_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_duplicate_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mbatch_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 batch_examples[col] = [\n\u001b[0m\u001b[1;32m    492\u001b[0m                     \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChunkedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 batch_examples[col] = [\n\u001b[0;32m--> 492\u001b[0;31m                     \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pylist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChunkedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                 ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}