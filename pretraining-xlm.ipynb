{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nimport gc\nfrom multiprocessing import Pool, cpu_count\nfrom datasets import load_dataset\nimport fasttext\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"IndicCorpus\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nTARGET_PER_LANG = 500000  # ~6M total, adjustable\nMIN_WORDS = 50\n\n# FastText for filtering\nft_model = fasttext.load_model('lid.176.bin')\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    return Image.fromarray(cv2.dilate(binary, np.ones((1, 1), np.uint8), iterations=1))\n\ndef extract_text_from_pdf(pdf_path):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        text = extract_text(pdf_path)\n        if not text.strip():\n            images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=50)\n            text = \"\".join([pytesseract.image_to_string(preprocess_image(img), lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in images])\n            gc.collect()\n        return text.strip(), os.path.basename(pdf_path).split('_')[0]\n    except Exception as e:\n        print(f\"Error: {pdf_path}: {e}\")\n        return \"\", \"\"\n\ndef detect_language(text):\n    lang = ft_model.predict(text)[0][0].replace('__label__', '')\n    return {'hi': 'Hindi', 'mr': 'Marathi', 'gu': 'Gujarati', 'bn': 'Bengali', 'ta': 'Tamil', 'kn': 'Kannada', 'te': 'Telugu', 'ml': 'Malayalam', 'pa': 'Punjabi', 'or': 'Odia', 'as': 'Assamese'}.get(lang, None)\n\ndef process_pdf(pdf_path):\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n    if not text:\n        return []\n    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    return [{\"text\": para, \"language\": hinted_lang if hinted_lang in LANGUAGES else detect_language(para)} for para in paragraphs if filter_text_length(para)]\n\ndef process_pdfs_parallel(pdf_dir=\"/kaggle/input/meta-folder\"):\n    pdf_files = [os.path.join(lang_path, f) for lang_folder in os.listdir(pdf_dir) if os.path.isdir(lang_path := os.path.join(pdf_dir, lang_folder)) for f in os.listdir(lang_path) if f.endswith(\".pdf\")]\n    with Pool(cpu_count()) as pool:\n        results = pool.map(process_pdf, pdf_files)\n    df = pd.DataFrame([item for sublist in results for item in sublist])\n    df.to_parquet(\"/kaggle/working/temp_pdf.parquet\", compression='gzip')\n    return len(df)\n\ndef load_oscar():\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            oscar_data.extend({\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} for item in dataset if filter_text_length(item['text']))\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    df = pd.DataFrame(oscar_data).sample(min(3000000, len(oscar_data)))\n    df.to_parquet(\"/kaggle/working/temp_oscar.parquet\", compression='gzip')\n    return len(df)\n\ndef load_samanantar():\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n    samanantar_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} for row in dataset for lang_code, lang_name in samanantar_langs.items() if row.get(lang_code) and filter_text_length(row[lang_code])]\n    df = pd.DataFrame(samanantar_data).sample(min(2000000, len(samanantar_data)))\n    df.to_parquet(\"/kaggle/working/temp_samanantar.parquet\", compression='gzip')\n    return len(df)\n\ndef build_corpus(pdf_dir=\"/kaggle/input/meta-folder\", output_file=\"/kaggle/working/indic_corpus.parquet\"):\n    temp_files = [\"/kaggle/working/temp_pdf.parquet\", \"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    pdf_count = process_pdfs_parallel(pdf_dir)\n    oscar_count = load_oscar()\n    samanantar_count = load_samanantar()\n    \n    # Merge existing corpus if it exists\n    if os.path.exists(output_file):\n        existing_df = spark.read.parquet(output_file)\n        new_df = spark.read.parquet(*temp_files)\n        combined_df = existing_df.union(new_df).dropDuplicates([\"text\"])\n    else:\n        combined_df = spark.read.parquet(*temp_files).dropDuplicates([\"text\"])\n    \n    # Balance and save\n    balanced_df = combined_df.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).filter(F.col(\"language\").isin(LANGUAGES)).join(combined_df, \"language\").orderBy(F.rand()).limit(TARGET_PER_LANG).select(\"text\", \"language\")\n    balanced_df.write.parquet(output_file, mode=\"overwrite\", compression=\"gzip\")\n    total_samples = balanced_df.count()\n    print(f\"Saved {total_samples} samples to {output_file}\")\n    dist = balanced_df.groupBy(\"language\").count().collect()\n    for row in dist:\n        print(f\"{row['language']}: {row['count']}\")\n    \n    # Clean up\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}