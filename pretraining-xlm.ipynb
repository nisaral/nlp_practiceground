{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets transformers adapters sklearn torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport os\nimport gc\nfrom multiprocessing import Pool, cpu_count\nfrom datasets import load_dataset\nimport fasttext\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"IndicCorpus\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nTARGET_PER_LANG = 500000  # ~6M total, adjustable\nMIN_WORDS = 50\n\n# FastText for filtering\nft_model = fasttext.load_model('lid.176.bin')\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef preprocess_image(image):\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)\n    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    return Image.fromarray(cv2.dilate(binary, np.ones((1, 1), np.uint8), iterations=1))\n\ndef extract_text_from_pdf(pdf_path):\n    try:\n        print(f\"Processing PDF: {pdf_path}\")\n        text = extract_text(pdf_path)\n        if not text.strip():\n            images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=50)\n            text = \"\".join([pytesseract.image_to_string(preprocess_image(img), lang='hin+mar+ben+tam+kan+tel+mal+pan+ori+asm+eng+urd+chi', config=\"--psm 6 --oem 1\").replace('\\0', '')[:100000] + \"\\n\" for img in images])\n            gc.collect()\n        return text.strip(), os.path.basename(pdf_path).split('_')[0]\n    except Exception as e:\n        print(f\"Error: {pdf_path}: {e}\")\n        return \"\", \"\"\n\ndef detect_language(text):\n    lang = ft_model.predict(text)[0][0].replace('__label__', '')\n    return {'hi': 'Hindi', 'mr': 'Marathi', 'gu': 'Gujarati', 'bn': 'Bengali', 'ta': 'Tamil', 'kn': 'Kannada', 'te': 'Telugu', 'ml': 'Malayalam', 'pa': 'Punjabi', 'or': 'Odia', 'as': 'Assamese'}.get(lang, None)\n\ndef process_pdf(pdf_path):\n    text, hinted_lang = extract_text_from_pdf(pdf_path)\n    if not text:\n        return []\n    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n    return [{\"text\": para, \"language\": hinted_lang if hinted_lang in LANGUAGES else detect_language(para)} for para in paragraphs if filter_text_length(para)]\n\ndef process_pdfs_parallel(pdf_dir=\"/kaggle/input/meta-folder\"):\n    pdf_files = [os.path.join(lang_path, f) for lang_folder in os.listdir(pdf_dir) if os.path.isdir(lang_path := os.path.join(pdf_dir, lang_folder)) for f in os.listdir(lang_path) if f.endswith(\".pdf\")]\n    with Pool(cpu_count()) as pool:\n        results = pool.map(process_pdf, pdf_files)\n    df = pd.DataFrame([item for sublist in results for item in sublist])\n    df.to_parquet(\"/kaggle/working/temp_pdf.parquet\", compression='gzip')\n    return len(df)\n\ndef load_oscar():\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            oscar_data.extend({\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} for item in dataset if filter_text_length(item['text']))\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    df = pd.DataFrame(oscar_data).sample(min(3000000, len(oscar_data)))\n    df.to_parquet(\"/kaggle/working/temp_oscar.parquet\", compression='gzip')\n    return len(df)\n\ndef load_samanantar():\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n    samanantar_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} for row in dataset for lang_code, lang_name in samanantar_langs.items() if row.get(lang_code) and filter_text_length(row[lang_code])]\n    df = pd.DataFrame(samanantar_data).sample(min(2000000, len(samanantar_data)))\n    df.to_parquet(\"/kaggle/working/temp_samanantar.parquet\", compression='gzip')\n    return len(df)\n\ndef build_corpus(pdf_dir=\"/kaggle/input/meta-folder\", output_file=\"/kaggle/working/indic_corpus.parquet\"):\n    temp_files = [\"/kaggle/working/temp_pdf.parquet\", \"/kaggle/working/temp_oscar.parquet\", \"/kaggle/working/temp_samanantar.parquet\"]\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n    \n    pdf_count = process_pdfs_parallel(pdf_dir)\n    oscar_count = load_oscar()\n    samanantar_count = load_samanantar()\n    \n    # Merge existing corpus if it exists\n    if os.path.exists(output_file):\n        existing_df = spark.read.parquet(output_file)\n        new_df = spark.read.parquet(*temp_files)\n        combined_df = existing_df.union(new_df).dropDuplicates([\"text\"])\n    else:\n        combined_df = spark.read.parquet(*temp_files).dropDuplicates([\"text\"])\n    \n    # Balance and save\n    balanced_df = combined_df.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).filter(F.col(\"language\").isin(LANGUAGES)).join(combined_df, \"language\").orderBy(F.rand()).limit(TARGET_PER_LANG).select(\"text\", \"language\")\n    balanced_df.write.parquet(output_file, mode=\"overwrite\", compression=\"gzip\")\n    total_samples = balanced_df.count()\n    print(f\"Saved {total_samples} samples to {output_file}\")\n    dist = balanced_df.groupBy(\"language\").count().collect()\n    for row in dist:\n        print(f\"{row['language']}: {row['count']}\")\n    \n    # Clean up\n    for f in temp_files:\n        if os.path.exists(f):\n            os.remove(f)\n\nif __name__ == \"__main__\":\n    build_corpus()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# corpus_chunking.py\nfrom datasets import load_dataset\nimport pandas as pd\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\nMIN_WORDS = 50\nCHUNK_SIZE = 100000  # 100K samples per chunk\n\ndef filter_text_length(text):\n    return isinstance(text, str) and len(text.split()) >= MIN_WORDS\n\ndef load_oscar_chunk(start_idx, chunk_size=CHUNK_SIZE):\n    oscar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    oscar_data = []\n    for code, lang in oscar_langs.items():\n        try:\n            dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{code}\", split=\"train\", trust_remote_code=True)\n            end_idx = min(start_idx + chunk_size, len(dataset))\n            chunk = dataset[start_idx:end_idx]\n            oscar_data.extend({\"text\": item['text'].replace('\\0', '')[:100000], \"language\": lang} \n                             for item in chunk if filter_text_length(item['text']))\n            print(f\"OSCAR {lang}: {len([d for d in oscar_data if d['language'] == lang])} samples\")\n            if len(oscar_data) >= chunk_size:\n                break\n        except Exception as e:\n            print(f\"Error OSCAR {lang}: {e}\")\n    return pd.DataFrame(oscar_data).sample(min(chunk_size, len(oscar_data)))\n\ndef load_samanantar_chunk(start_idx, chunk_size=CHUNK_SIZE):\n    samanantar_langs = {\"hi\": \"Hindi\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"bn\": \"Bengali\", \"ta\": \"Tamil\", \"kn\": \"Kannada\", \"te\": \"Telugu\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"or\": \"Odia\", \"as\": \"Assamese\"}\n    try:\n        dataset = load_dataset(\"togethercomputer/samanantar\", split=\"train\", trust_remote_code=True)\n        end_idx = min(start_idx + chunk_size, len(dataset))\n        chunk = dataset[start_idx:end_idx]\n        samanantar_data = [{\"text\": row[lang_code].replace('\\0', '')[:100000], \"language\": lang_name} \n                          for row in chunk for lang_code, lang_name in samanantar_langs.items() \n                          if row.get(lang_code) and filter_text_length(row[lang_code])]\n        print(f\"Samanantar chunk: {len(samanantar_data)} samples\")\n        return pd.DataFrame(samanantar_data).sample(min(chunk_size, len(samanantar_data)))\n    except Exception as e:\n        print(f\"Error Samanantar: {e}\")\n        return pd.DataFrame()\n\ndef get_chunk(chunk_idx, output_file=\"/kaggle/working/chunk.parquet\"):\n    start_idx = chunk_idx * CHUNK_SIZE\n    oscar_df = load_oscar_chunk(start_idx)\n    samanantar_df = load_samanantar_chunk(start_idx)\n    combined_df = pd.concat([oscar_df, samanantar_df]).drop_duplicates(subset=[\"text\"]).sample(frac=1)\n    combined_df.to_parquet(output_file, compression=\"gzip\")\n    print(f\"Chunk {chunk_idx} saved with {len(combined_df)} samples\")\n    return combined_df\n\nif __name__ == \"__main__\":\n    for i in range(3):  # Test 3 chunks\n        get_chunk(i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pretraining.py\nfrom transformers import XLMRobertaForMaskedLM, XLMRobertaTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import Dataset\nimport pandas as pd\nimport os\n\ndef pretrain_xlmr(chunk_file=\"/kaggle/working/chunk.parquet\", model_dir=\"/kaggle/working/xlmr_pretrained\"):\n    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\" if not os.path.exists(model_dir) else model_dir)\n    model = XLMRobertaForMaskedLM.from_pretrained(\"xlm-roberta-base\" if not os.path.exists(model_dir) else model_dir)\n    \n    # Load chunk\n    df = pd.read_parquet(chunk_file)\n    dataset = Dataset.from_pandas(df)\n    encodings = tokenizer(df[\"text\"].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n    dataset = Dataset.from_dict({\"input_ids\": encodings[\"input_ids\"], \"attention_mask\": encodings[\"attention_mask\"]})\n    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n    \n    # Training args\n    training_args = TrainingArguments(\n        output_dir=model_dir,\n        overwrite_output_dir=not os.path.exists(model_dir),\n        per_device_train_batch_size=16,\n        num_train_epochs=1,  # 1 epoch per chunk\n        save_steps=10000,\n        logging_steps=100,\n        learning_rate=1e-5,\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=dataset,\n        data_collator=data_collator,\n    )\n    \n    trainer.train()\n    model.save_pretrained(model_dir)\n    tokenizer.save_pretrained(model_dir)\n    print(f\"Pre-trained on chunk, saved to {model_dir}\")\n\nif __name__ == \"__main__\":\n    pretrain_xlmr()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# finetuning.py\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer, Trainer, TrainingArguments\nfrom transformers.adapters import AdapterConfig\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score\nimport os\n\nLANGUAGES = [\"Hindi\", \"Marathi\", \"Sindhi\", \"Gujarati\", \"Bengali\", \"Tamil\", \"Kannada\", \"Telugu\", \"Malayalam\", \"Punjabi\", \"Odia\", \"Assamese\"]\n\ndef finetune_xlmr(pretrained_dir=\"/kaggle/working/xlmr_pretrained\", output_base_dir=\"/kaggle/working/finetuned\"):\n    tokenizer = XLMRobertaTokenizer.from_pretrained(pretrained_dir)\n    model = XLMRobertaForSequenceClassification.from_pretrained(pretrained_dir, num_labels=3)\n    \n    # Add adapters\n    adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=16)\n    for lang in LANGUAGES:\n        if not model.has_adapter(lang):\n            model.add_adapter(lang, config=adapter_config)\n    if not model.has_adapter(\"lora\"):\n        model.add_adapter(\"lora\", config=AdapterConfig.load(\"lora\", r=8, alpha=16))\n    model.train_adapter([\"lora\"] + LANGUAGES)\n    model.freeze_model()\n    \n    # IndicGLUE tasks\n    indicglue_tasks = {\n        \"innews\": {\"num_labels\": 3, \"desc\": \"News Genre Classification\"},\n        \"sentiment\": {\"num_labels\": 2, \"desc\": \"Sentiment Analysis\"}\n    }\n    \n    for task, config in indicglue_tasks.items():\n        if model.config.num_labels != config[\"num_labels\"]:\n            model = XLMRobertaForSequenceClassification.from_pretrained(pretrained_dir, num_labels=config[\"num_labels\"])\n            for lang in LANGUAGES:\n                if not model.has_adapter(lang):\n                    model.add_adapter(lang, config=adapter_config)\n            if not model.has_adapter(\"lora\"):\n                model.add_adapter(\"lora\", config=AdapterConfig.load(\"lora\", r=8, alpha=16))\n            model.train_adapter([\"lora\"] + LANGUAGES)\n            model.freeze_model()\n        \n        dataset = load_dataset(\"ai4bharat/indicglue\", task)\n        train_data = dataset[\"train\"].map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True, max_length=512), batched=True)\n        eval_data = dataset[\"validation\"].map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=True, max_length=512), batched=True)\n        \n        train_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n        eval_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n        \n        training_args = TrainingArguments(\n            output_dir=f\"{output_base_dir}_{task}\",\n            overwrite_output_dir=not os.path.exists(f\"{output_base_dir}_{task}\"),\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            num_train_epochs=1,  # 1 epoch per chunk\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1\",\n            learning_rate=2e-5,\n        )\n        \n        def compute_metrics(pred):\n            labels = pred.label_ids\n            preds = pred.predictions.argmax(-1)\n            return {\"accuracy\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds, average=\"weighted\")}\n        \n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=train_data,\n            eval_dataset=eval_data,\n            compute_metrics=compute_metrics,\n        )\n        \n        trainer.train()\n        eval_results = trainer.evaluate()\n        print(f\"Task: {config['desc']} ({task})\")\n        print(f\"Accuracy: {eval_results['eval_accuracy']:.4f}\")\n        print(f\"F1 Score: {eval_results['eval_f1']:.4f}\")\n        \n        model.save_pretrained(f\"{output_base_dir}_{task}\")\n        tokenizer.save_pretrained(f\"{output_base_dir}_{task}\")\n\nif __name__ == \"__main__\":\n    finetune_xlmr()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}