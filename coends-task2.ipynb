{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10445490,"sourceType":"datasetVersion","datasetId":6465641},{"sourceId":10505239,"sourceType":"datasetVersion","datasetId":6503459}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nfrom tqdm import tqdm\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:28:59.859216Z","iopub.execute_input":"2025-01-20T10:28:59.859665Z","iopub.status.idle":"2025-01-20T10:29:00.223457Z","shell.execute_reply.started":"2025-01-20T10:28:59.859624Z","shell.execute_reply":"2025-01-20T10:29:00.222734Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vjtitask/sample_submission.csv\n/kaggle/input/vjtitask/train.csv\n/kaggle/input/vjtitask/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q pyicu\n!pip install -q pycld2\n!pip install -q polyglot\n!pip install -q textstat\n!pip install -q googletrans\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T11:24:24.631712Z","iopub.execute_input":"2025-01-20T11:24:24.632033Z","execution_failed":"2025-01-20T11:24:58.906Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.9/263.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport re\n\nimport textstat\nfrom scipy import stats\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport transformers\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.optimizers import Adam\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,\\\n                                            CountVectorizer,\\\n                                            HashingVectorizer\n\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom googletrans import Translator\nfrom nltk import WordNetLemmatizer\nfrom polyglot.detect import Detector\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nstopword=set(STOPWORDS)\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-20T11:24:58.907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langdetect \n!pip install indic_transliteration","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-20T11:24:58.907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langdetect import detect\nfrom indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T15:04:24.397723Z","iopub.execute_input":"2025-01-17T15:04:24.398024Z","iopub.status.idle":"2025-01-17T15:04:24.407831Z","shell.execute_reply.started":"2025-01-17T15:04:24.397999Z","shell.execute_reply":"2025-01-17T15:04:24.406815Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA! \n**visualizing and analysing the comments**","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/vjtitask/train.csv\")\ndf2=pd.read_csv(\"/kaggle/input/vjtitask/test.csv\")\nsubmission=pd.read_csv(\"/kaggle/input/vjtitask/sample_submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:05.124773Z","iopub.execute_input":"2025-01-20T10:29:05.125080Z","iopub.status.idle":"2025-01-20T10:29:05.283718Z","shell.execute_reply.started":"2025-01-20T10:29:05.125054Z","shell.execute_reply":"2025-01-20T10:29:05.283049Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:31:24.010551Z","iopub.execute_input":"2025-01-20T09:31:24.010816Z","iopub.status.idle":"2025-01-20T09:31:24.026519Z","shell.execute_reply.started":"2025-01-20T09:31:24.010774Z","shell.execute_reply":"2025-01-20T09:31:24.025755Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 id                                               text\n0     1041016773991  मोदी साहेब वरती चांगले तो पण त्यांच्या साईटला ...\n1      109362481297  In #Jawan you will get: 1. 1st class action 2....\n2      985019053532  किती दहशत आहे दोन्ही पवारांची बारामती कर मोकळे...\n3      436629695381  आजवर का नाही विकास केलात मित्रा घरात किती प्रत...\n4      585196067684  घराणेशाही थांबवायची असेल तर मोठ्या कॉलेज विद्य...\n...             ...                                                ...\n1995   148770317688  , अरे चूतीया, रोजगार उपलब्ध करून देणे सरकारच न...\n1996   288582831883  समाजातील गरीब लोकांसाठी मदतीला धावून येणारा एक...\n1997   285086247879  Yuvraj is one of the most underrated players i...\n1998   114758927004  If muslim,sikh can wear their relegious attire...\n1999   417819392797  अजित गव्हाणे हे जनतेत मिळूनमिसळून वागणारे नेते...\n\n[2000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041016773991</td>\n      <td>मोदी साहेब वरती चांगले तो पण त्यांच्या साईटला ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>109362481297</td>\n      <td>In #Jawan you will get: 1. 1st class action 2....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>985019053532</td>\n      <td>किती दहशत आहे दोन्ही पवारांची बारामती कर मोकळे...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>436629695381</td>\n      <td>आजवर का नाही विकास केलात मित्रा घरात किती प्रत...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>585196067684</td>\n      <td>घराणेशाही थांबवायची असेल तर मोठ्या कॉलेज विद्य...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>148770317688</td>\n      <td>, अरे चूतीया, रोजगार उपलब्ध करून देणे सरकारच न...</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>288582831883</td>\n      <td>समाजातील गरीब लोकांसाठी मदतीला धावून येणारा एक...</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>285086247879</td>\n      <td>Yuvraj is one of the most underrated players i...</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>114758927004</td>\n      <td>If muslim,sikh can wear their relegious attire...</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>417819392797</td>\n      <td>अजित गव्हाणे हे जनतेत मिळूनमिसळून वागणारे नेते...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T19:48:03.220630Z","iopub.execute_input":"2025-01-19T19:48:03.220937Z","iopub.status.idle":"2025-01-19T19:48:03.241916Z","shell.execute_reply.started":"2025-01-19T19:48:03.220911Z","shell.execute_reply":"2025-01-19T19:48:03.241248Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                 id                                               text  \\\n0      500796286320  Wow! From what I've observed from this documen...   \n1      838906157157  काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...   \n2     1011026626743  अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...   \n3     1068853499446  She's saying that \"doing her own research\" led...   \n4      502772748919  That is not Karen, that is perfectly reasonabl...   \n...             ...                                                ...   \n3995   815979127763  Why not just buy some oil platforms and start ...   \n3996   426508330840  अरे भाई हे तर सगळे हिंदीत बोलतात आणि राहता महा...   \n3997  1016442533101  सरकार ने जी योजना आणली आहे ती खरोखरच खूप चांगल...   \n3998  1096741594952  मुख्यमंत्री एकनाथ शिंदे यांच्या नेतृत्वाखाली म...   \n3999  1027635577738  सद्या फडणवीस जो बोलतोय तो केंद्राच्या भरवश्याव...   \n\n      complaint  demands  praise  questions  \n0           0.0      0.0     1.0        0.0  \n1           1.0      0.0     0.0        0.0  \n2           0.0      0.0     1.0        0.0  \n3           1.0      0.0     0.0        0.0  \n4           1.0      0.0     0.0        0.0  \n...         ...      ...     ...        ...  \n3995        0.0      0.0     0.0        1.0  \n3996        1.0      0.0     0.0        0.0  \n3997        1.0      0.0     0.0        0.0  \n3998        0.0      0.0     1.0        0.0  \n3999        1.0      0.0     0.0        0.0  \n\n[4000 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500796286320</td>\n      <td>Wow! From what I've observed from this documen...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>838906157157</td>\n      <td>काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1011026626743</td>\n      <td>अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1068853499446</td>\n      <td>She's saying that \"doing her own research\" led...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>502772748919</td>\n      <td>That is not Karen, that is perfectly reasonabl...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>815979127763</td>\n      <td>Why not just buy some oil platforms and start ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>426508330840</td>\n      <td>अरे भाई हे तर सगळे हिंदीत बोलतात आणि राहता महा...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>1016442533101</td>\n      <td>सरकार ने जी योजना आणली आहे ती खरोखरच खूप चांगल...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>1096741594952</td>\n      <td>मुख्यमंत्री एकनाथ शिंदे यांच्या नेतृत्वाखाली म...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>1027635577738</td>\n      <td>सद्या फडणवीस जो बोलतोय तो केंद्राच्या भरवश्याव...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"#df2['language'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:47:15.607628Z","iopub.execute_input":"2025-01-19T15:47:15.607907Z","iopub.status.idle":"2025-01-19T15:47:15.611373Z","shell.execute_reply.started":"2025-01-19T15:47:15.607886Z","shell.execute_reply":"2025-01-19T15:47:15.610643Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#df['language'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:47:15.862100Z","iopub.execute_input":"2025-01-19T15:47:15.862440Z","iopub.status.idle":"2025-01-19T15:47:15.865694Z","shell.execute_reply.started":"2025-01-19T15:47:15.862405Z","shell.execute_reply":"2025-01-19T15:47:15.864957Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#def is_hinglish(text):\n    #hinglish_keywords = ['badiya','kya','kaise', 'bahut','raha']\n    #for word in hinglish_keywords:\n     #   if word in text.lower():\n      #      return True\n    #return False\n\n#df1['language'] = df1.apply(lambda row: 'Hinglish' if is_hinglish(row['text']) else row['language'], axis=1)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:47:16.102529Z","iopub.execute_input":"2025-01-19T15:47:16.102824Z","iopub.status.idle":"2025-01-19T15:47:16.106043Z","shell.execute_reply.started":"2025-01-19T15:47:16.102801Z","shell.execute_reply":"2025-01-19T15:47:16.105302Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#df1[df1['language']==\"Hinglish\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:47:16.349973Z","iopub.execute_input":"2025-01-19T15:47:16.350249Z","iopub.status.idle":"2025-01-19T15:47:16.353761Z","shell.execute_reply.started":"2025-01-19T15:47:16.350228Z","shell.execute_reply":"2025-01-19T15:47:16.353011Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport random\n\n\ntext_examples = {\n    \"demands\": {\n        \"marathi\": [\n            \"मी दोन दिवसांपूर्वी जो फ्रीज ऑर्डर केला होता, तो अजून आला नाही. कृपया याची माहिती द्या आणि त्वरित डिलिव्हरी करा.\",\n            \"आमच्या बिलाचे पेंडिंग पेमेंट आहे का ते मला कळवा आणि शक्य तितक्या लवकर बिलाचे निवारण करा.\",\n            \"तुमच्या बँकेचे नवीन होम लोनचे दर काय आहेत? मला घर खरेदीसाठी वित्तपुरवठा हवा आहे.\",\n            \"घराणेशाही थांबवायची असेल तर मोठ्या कॉलेज विद्यापीठ मध्ये विध्यार्थीसंघटना वाढवायला पाहिजेत आणि त्यांच्या निवडणुका चालू करायला पाहिजेत!\",\n            \"माझ्या घरातील वीज पुरवठा दोन दिवसांपासून विस्कळीत आहे. कृपया याची तत्काळ दुरुस्ती करावी आणि मला पुढील २४ तासांमध्ये अपडेट पाठवावा.\",\n            \"ऑर्डर केलेल्या उत्पादनाचे वितरण अजूनपर्यंत झाले नाही. मला ते उत्पादन खूप महत्त्वाचे आहे आणि कृपया आजच डिलीव्हरी पूर्ण करा.\",\n            \"कृपया मला माझ्या ताज्या ऑर्डरच्या स्थितीविषयी माहिती द्या. मी ऑर्डर केलेल्या वस्तूची मला खूप गरज आहे, आणि तो उत्पादन मला लवकरात लवकर मिळावा, हे माझ्यासाठी खूप महत्त्वाचं आहे. जर कुठल्या कारणामुळे त्याची डिलीव्हरी लांबणीवर पडत असेल, तर कृपया मला कारण सांगावे आणि मी कसे पुढे जाऊ शकतो, हे सांगावे. मी अशा परिस्थितीत पूर्णपणे थांबू इच्छित नाही, आणि मला सर्वात जलद डिलीव्हरी पर्याय विचारात घेतल्यास हे कसे होईल, ते कृपया स्पष्ट करा.\",\n             \"माझ्या मागील ऑर्डरचा पेमेंट अजूनपर्यंत प्रोसेस झाला नाही. माझा पेमेंट गेटवे योग्यपणे काम करत नाहीय, आणि मी त्या कारणामुळे आधीच किमान तीन वेळा पुनःप्रयास केला आहे. तुमच्या वेबसाइटवरील सर्व पेमेंट गेटवे समजून घेतल्यास, ते खूपच हळू आहेत आणि अनेक वेळा ते क्रॅश होतात. कृपया हे त्वरित दुरुस्त करा आणि मी तुमच्याकडून अपेक्षीत सेवा प्राप्त करू इच्छितो. त्याचबरोबर, मला इतर सुरक्षित आणि जलद पेमेंट गेटवे उपलब्ध करून द्या.\",\n             \"तुमच्या दिलेल्या सेवेच्या दर्जा सुधारावा. मला खूप वेळा तुमच्याशी संपर्क साधावा लागला आणि जेव्हा ते सॉल्व होतं नाही, तेव्हा मी जास्त तणावात होतो. ग्राहक सेवेचे प्रतिनिधी अधिक ट्रेनिंग घेत असावेत, कारण मला खात्री नाही की ते समस्येचे योग्य समाधान देत आहेत. कृपया या संदर्भात कार्यवाही करा आणि मला खात्री देणे आवश्यक आहे की भविष्यात अशी समस्या उद्भवणार नाही.\",\n            \"तुम्ही म्हणाला होता की शाळेतील क्रीडा मैदानासाठी नवीन साधनं आणणार. ती कधी येणार आहेत? मुलांना सराव करायचा आहे.\",\"आमच्या भागात सार्वजनिक शौचालय नाही. कृपया काहीतरी उपाय करा, कारण स्वच्छतेची खूप गरज आहे.\"\n        ],\n        \"hindi\": [\n            \"सरकारी योजनाओं की जानकारी गांव के हर व्यक्ति तक पहुँचाने के लिए मोबाइल ऐप लॉन्च करें, जिससे लोगों को समय पर जानकारी मिल सके।\",\n            \"स्थानीय बाजारों में छोटे दुकानदारों के लिए किराए में छूट दी जाए। उनकी आय महामारी के कारण बुरी तरह प्रभावित हुई है।\",\n            \"मुझे मेरी सेवाओं के बारे में पूरी जानकारी चाहिए, जिसमें मेरे सभी खर्चे और उनके विवरण हो ताकि मैं अपने बजट को बेहतर तरीके से प्रबंधित कर सकूं।\",\n            \"मेरी पिछले हफ्ते की ऑर्डर अभी तक डिलीवर नहीं हुई है। कृपया इसे तुरंत डिलीवर करें क्योंकि यह मेरे लिए अत्यधिक महत्वपूर्ण है।\",\n            \"कृपया मेरी दरखास्त को प्राथमिकता के आधार पर प्रोसेस करें क्योंकि यह अत्यंत महत्वपूर्ण है और मुझे जल्द परिणाम चाहिए।\",\n            \"आपकी डिलीवरी सर्विस बहुत धीमी हो गई है, और मुझे बार-बार डिलीवरी की स्थिति को लेकर पूछताछ करनी पड़ती है। मैंने जिस उत्पाद को ऑर्डर किया था, वह मुझे अब तक नहीं मिला है, और यह अब काफी ज़रूरी हो गया है। कृपया मेरे आदेश की स्थिति तुरंत स्पष्ट करें और मुझे यह सुनिश्चित करें कि डिलीवरी जल्दी होगी। मैं इस मुद्दे पर और अधिक विलंब नहीं सह सकता, क्योंकि इस उत्पाद का मेरे व्यवसाय के लिए अत्यधिक महत्व है। कृपया इसकी प्राथमिकता बढ़ाएं और मुझे जल्द से जल्द डिलीवरी का आश्वासन दें।\",\n            \"मैंने आपके पास सेवा के लिए आवेदन किया था, लेकिन मैंने अब तक किसी भी तरह का उत्तर या प्रतिक्रिया नहीं प्राप्त की है। मैंने कई बार आपके कस्टमर सपोर्ट से संपर्क किया है, लेकिन हर बार मुझे बस यह कहा जाता है कि 'हम जल्द ही संपर्क करेंगे,' और फिर कोई कार्रवाई नहीं होती। क्या आप कृपया मेरी इस आवेदन पर कार्यवाही करेंगे और मुझे त्वरित प्रतिक्रिया देंगे? मुझे लगता है कि आपकी सेवा में सुधार की जरूरत है, ताकि ग्राहक आसानी से संपर्क कर सकें और समस्याओं का समाधान हो सके।\",\n            \"मेरे पास एक मोबाइल फोन का मुद्दा है जो बिल्कुल ठीक से काम नहीं कर रहा है। मैं ने पहले ही तकनीकी सहायता से संपर्क किया है, लेकिन अभी तक कोई हल नहीं मिला। फोन की बैटरी बहुत जल्दी खत्म हो जाती है, और नेटवर्क भी बार-बार कट जाता है। कृपया जल्दी से जल्दी इसे ठीक करवाने की व्यवस्था करें, क्योंकि यह अब मेरे लिए एक महत्वपूर्ण कार्य में रुकावट पैदा कर रहा है। मुझे उम्मीद है कि आप जल्द ही इस मुद्दे पर ध्यान देंगे और इसे समाधान करेंगे।\",\n            \"Can you please forward me the notes from yesterday’s lecture? I couldn’t attend because of a family emergency, and I need to catch up before the next class.\",\n            \"यार, मुझे कल की मीटिंग के minutes urgently चाहिए. Manager ने पूछे हैं, और मेरे पास कोई details नहीं हैं. प्लीज जल्दी से भेज दो.\",\n            \"Hey, can you help me book tickets for the weekend trip? I’m stuck at work and don’t have time to check the schedules.\",\n            \"तुम्हारे पास maths की वो reference book है ना? मुझे एक equation समझने में problem हो रही है. प्लीज आज ही दे दो.\",\n            \"Can you remind the landlord to fix the water leakage? It’s been almost a week, and it’s only getting worse. Please follow up with him today.\",\"Yaar, humare building mein jo lift hai, wo har doosre din band ho jaati hai. Aise mein kya humein chadhne ke liye stairs ka use karna padega? Jaldi se repair karwa do.\"\n        ],\n        \"english\": [\n            \"I have been waiting for the furniture delivery for over two weeks now. Please provide an update and ensure it reaches this week.\",\n            \"My internet connection has been down since yesterday evening. Kindly send a technician to fix it urgently.\",\n            \"The air conditioning in the conference room is not working properly. Can you have it serviced before the next meeting?\",\n             \"Maine do din pehle courier service ke liye request ki thi, abhi tak koi update nahi aayi. Jaldi se mujhe status batao.\",\n             \"Yeh AC pura properly kaam nahi kar raha hai. Please technician bhejo aur repair karwa do.\",\n             \"Kya aap mujhe mere electricity bill ke detailed breakdown de sakte ho? Kuch amounts galat lag rahe hain.\",\n             \"Mujhe pata chala ki aapke paas ek naya cashback offer chal raha hai. Mujhe iska benefit kaise milega, please explain karo.\",\n             \"Tujhe office supplies urgent chahiye kya , kal tak deliver kar sakti kya?\",\n            \"Mujhe mera latest bill ka pura breakdown chahiye, jisme saare charges aur reasons detail mein diye hoon. Please jaldi bhejiye.\",\n             \"Meri order ab tak deliver nahi hui hai. Mujhe aaj urgent kaam ke liye zarurat hai, toh please fast delivery karwa dijiye.\",\n            \"I have been waiting for my furniture delivery for over two weeks now. I was initially promised a delivery date, but the product has not yet arrived. I am not sure if it’s an issue on your end or with the delivery company, but I would like to know the specific reason for this delay. This has been very frustrating as I’ve been trying to plan around the delivery, but each time I contact customer support, I get vague responses. Could you please provide a detailed explanation and an updated delivery date? I need this matter resolved as soon as possible.\",\n            \"I ordered a new mobile phone two weeks ago, and I have not received it yet. I am not sure where the delivery is being delayed, but it has been long enough now for me to get concerned. The tracking information is not updated frequently, and I feel like I’m being left in the dark. Can you please provide me with an exact timeline on when I can expect my phone? Additionally, I would like to request an expedited shipping option, as this delay has already caused me inconvenience.\",\n            \"My internet connection has been down for over 24 hours, and I have not been able to use my services at home for work purposes. This is extremely frustrating, as I rely heavily on my internet connection for both work and personal matters. I’ve tried troubleshooting on my end, but it seems like the problem is with your service. Could you please send a technician as soon as possible to inspect and repair the connection? I would also appreciate it if you could offer some compensation for the prolonged outage.\",\n            \"Arey yaar, kal jo match ke tickets lene bole the, woh kab arrange karoge? Abhi tak kuch update nahi diya.\"\n            \"Bro, mujhe aaj ka assignment bhej de na, kal submit karna hai aur maine banaya nahi.\",\n            \"Didi, woh tuition ke liye jo naye books aaye hain, kya mujhe unka ek set arrange kar ke de sakti ho?\",\n            \"Arey yaar, yeh Ganpati pandal ke decor ke ideas de de, Tujhe ideas kaafi ache aate hain aur pls ganpati ka booking bhi tu hi karde .\",\n            \"Tu kal office se jaldi aa sakta hai kya? Plan karte hain Marine Drive walk ka bhot dino se hum mile nhi lets spend some qualtiy time.\",\n            \"Oye, tu woh group discussion ke liye points bana ke ready kar na. Kal woh tough sir ke saamne presentation hai. Agar usne kuch ulta question poocha toh kaise handle karenge? Main bhi apna part kar raha hoon, but teamwork zaroori hai yaar.\",\n            \"Bro, can you please help me with the notes from last week’s lecture? I missed it because of the train delay, and I’m totally clueless about tomorrow’s test. Just click pictures and send them on WhatsApp; it’ll be a huge favor, yaar. Don’t forget, okay?\"\n        ],\n        \"Malyalam\":[\n            \"എനിക്ക് കഴിഞ്ഞ ആറുമാസത്തിനുള്ളിൽ നടന്ന ഇടപാടുകളുടെ വിശദമായ വിവരങ്ങൾ ആവശ്യമുണ്ട്. ഓരോ ചെലവും എന്തിനാണ് വന്നതെന്ന് വ്യക്തമാക്കുക.\",\n            \"വീട് നവീകരണത്തിനായി അനുവദനീയമായ വായ്പയുടെ എല്ലാ വിശദാംശങ്ങളും അയക്കുക. അതിൽ നിന്നും ഏറ്റവും നല്ല ഓഫർ തിരഞ്ഞെടുക്കാൻ ഞാൻ സഹായം ആവശ്യമുണ്ട്.\",\n            \"माझ्या बरोबर स्टेशनवर येशील का? मला तिथून काही सामान आणायचं आहे आणि ते खूप जड आहे. एकट्याने न्यायचं तर अवघड आहे. कृपया थोडं मदत करशील का?\",\n            \"यार, मुझे urgent notes चाहिए अगले हफ्ते के लिए. तुमने जो summary बनाई थी, वो भेज दो. मैं वादा करता हूं कि तुम्हारा नाम presentation में जरूर mention करूंगा.\",\n            \"Can you help me complete the project draft by tonight? I’m stuck on the analysis part, and you’re way better at it. Let’s split the sections and finish this together.\",\n            \"तुझ्याकडे जर चार्जर असेल तर मला देशील का? माझा फोन पूर्णपणे डिस्चार्ज आहे, आणि मला ऑफिसच्या कामासाठी चार्ज करायचं आहे.\",\n             \"क्या तुम प्लीज मुझे train schedule check करके बता सकते हो? मुझे urgent काम से बाहर जाना है, और time सही से पता नहीं चल रहा है.\",\n             \"Can you please arrange for a bigger classroom for our weekly club meetings? The current one is too small for everyone to fit in comfortably. We’ve been requesting this for a while now, and it’s important for better participation and interaction during discussions.\",\n              \"क्या आप हमारे स्कूल के playground को repair करवाने की व्यवस्था कर सकते हैं? बच्चे खेलते समय चोटिल हो रहे हैं क्योंकि मैदान uneven है और कई जगह गड्ढे भी हैं. यह urgent matter है, कृपया इस पर ध्यान दें.\",\n             \"तुम्हाला जमल्यास लायब्ररीच्या वेळा वाढवू शकता का? परीक्षा जवळ येत आहेत, आणि अभ्यासासाठी थोडा जास्त वेळ लागतो. जर हे शक्य असेल, तर खूप विद्यार्थ्यांना फायदा होईल.\",\n             \"The cafeteria menu needs an upgrade. Can we have more healthy and budget-friendly options? Many of us have been finding the current menu repetitive and a bit unhealthy for everyday consumption.\",\n             \"आप लोग community park की सफाई regular basis पर क्यों नहीं करते? यह पार्क सबके use के लिए है, और हर जगह गंदगी जमा हो रही है. कृपया सफाई को schedule में डालें.\",\n             \"आम्ही गणपती मंडळासाठी यंदा नवीन मंडप उभारायचा विचार करतो आहोत. त्यासाठी कचरा उचलणे, लाईट्स लावणे, आणि जागा स्वच्छ ठेवण्याच्या तुमच्या सेवांची आवश्यकता आहे. कृपया लवकर उत्तर द्या.\"\n        ],\n        \"gujrati\":[\n            \"વિજળી કાપના કારણે મારી ઓફિસનું કામ પ્રભાવિત થયું છે. કૃપા કરીને પાણી અને વિજળીની સમસ્યાનું તરત નિરાકરણ કરો.\"\n        ]\n    },\n    \"questions\": {\n        \"marathi\": [\n            \"सरकारने नुकत्याच मंजूर केलेल्या कृषी विधेयकांचा शेतकऱ्यांच्या उत्पन्नावर कसा परिणाम होईल?\",\n            \"सार्वजनिक वाहतूक व्यवस्था सुधारण्यासाठी स्थानिक प्रशासन कोणती धोरणे आखत आहे आणि त्याची अंमलबजावणी कधी होईल?\",\n            \"ग्रामीण भागातील विद्यार्थ्यांना डिजिटल शिक्षण तंत्रज्ञान उपलब्ध करून देण्यासाठी कोणती पावले उचलली जातील?\",\n           \"क्याआप बता सकते हैं कि मैंने पिछली बार जो कंप्लेंट दर्ज की थी, उस पर क्या एक्शन लिया गया?\",\n            \"तुमच्या नवीन योजनांचा कसा फायदा होईल? मला कोणते पर्याय उपलब्ध आहेत याची अधिक माहिती हवी आहे.\",\n            \"जर मी माझ्या आरोग्य विम्याचा दावा करायचा असेल तर प्रक्रिया काय आहे आणि मला कोणते कागदपत्र लागतील?\",\n            \"तुम्ही दिलेल्या अलीकडील योजनांबद्दल मला थोडं अधिक माहिती हवी आहे. कशाप्रकारे आणि कोणत्या पद्धतीने ती माझ्या व्यवसायासाठी फायदेशीर ठरेल? मला त्या योजनांच्या अटी, नियम आणि अनुप्रयोग प्रक्रियांची संपूर्ण माहिती पाहिजे. हे समजून घेणे माझ्यासाठी महत्त्वाचे आहे, कारण मी त्याचा सर्वोत्तम वापर करून, माझ्या कंपन्याला वाढवू इच्छितो. कृपया यावर लवकर कार्यवाही करा.\",\n            \"तुमच्या सेवा उत्कृष्ट आहेत, आणि मी तुमच्या टीमला समर्पणाच्या कामाबद्दल सल्ला देतो. तथापि, माझ्या काही गोष्टींसाठी अतिरिक्त माहिती मिळवण्याची आवश्यकता आहे, जसे की काही वेळांमध्ये विशेष बदलांची मदत कशी होईल, तसेच व्यवसायाच्या डिझाइनसाठी पुढील कदम.\",\"सर, पुढच्या आठवड्यात स्पर्धेचा अंतिम फेरीचा कार्यक्रम आहे का? वेळ आणि जागेबद्दल कळवा.\"\n        ],\n        \"hindi\": [\n            \"नई शिक्षा नीति से ग्रामीण बच्चों की शिक्षा में किस प्रकार के बदलाव देखने को मिलेंगे?\",\n            \"सरकारी अस्पतालों में संसाधनों की कमी को पूरा करने के लिए कौन सी नई योजनाएँ लागू की जा रही हैं?\",\n            \"क्या शहरी क्षेत्रों में पानी के संकट को हल करने के लिए दीर्घकालिक योजना बनाई जा रही है?\",\n            \"क्या आपकी टीम मुझे बता सकती है कि अगर मेरा ऑर्डर डिलीवर नहीं हुआ है तो मुझे क्या करना चाहिए और उसकी प्रक्रिया कितनी लंबी होती है?\",\n            \"आपके द्वारा पेश की गई योजनाओं में कौन-कौन से फायदे हैं और मुझे किसके लिए आवेदन करना चाहिए?\",\n            \"क्या कोई तरीका है जिससे मैं अपने बैंक अकाउंट की डिटेल्स जल्दी अपडेट कर सकूं और कोई समस्या न हो?\",\n            \"क्या मुझे इस साल के अंत तक आप द्वारा प्रस्तावित योजनाओं के परिणामों के बारे में अधिक जानकारी मिल सकती है? मेरे पास कुछ ऐसे प्रश्न हैं जो मेरी योजनाओं में प्रभावी रूप से योगदान कर सकते हैं, और मैं चाहता हूँ कि यह योजना जल्द से जल्द कार्यान्वित हो। क्या आप मुझे योजनाओं के बारे में विशिष्ट जानकारी दे सकते हैं, और क्या कोई विशेष परिस्थिति है जिसे मुझे ध्यान में रखना चाहिए?\",\n            \"क्या आप मेरी पिछली सेवा समीक्षा के बारे में कुछ और जानकारी दे सकते हैं? मैंने अपनी पिछली सेवा से जुड़े कुछ सवाल पूछे थे, लेकिन मुझे अभी तक कोई स्पष्ट उत्तर नहीं मिला। कृपया मुझे इसका अधिक विस्तृत विवरण दें ताकि मैं इस बारे में अपने निर्णय पर विचार कर सकूं।\",\n            \"क्या तुम्हें पता है, इस बार गणपति के लिए Lalbaug cha Raja कब open होगा? मैं हर साल जाता हूं, लेकिन अभी तक dates confirm नहीं हैं. क्या process है pass लेने का, और कितनी भीड़ होने वाली है?\"\n\n        ],\n        \"english\": [\n            \"Why is the warranty on the product void when I clearly followed the usage instructions\",\n            \"Can you explain how the rewards points system works and how I can redeem them\",\n            \"was the vibrator supposed to do that or i went overboard with it....did i get a little too freaky this time..ughhh\",\n            \"What are the steps involved in applying for a loan extension? Are there any additional charges?\",\n            \"Can you provide some insight into why the delivery was delayed and what measures are being taken to prevent it in the future?\",\n            \"Woh jo offer kal advertise kiya tha, woh abhi bhi available hai kya ya expire ho gaya?\",\n            \"Mere account mein late fee ka charge kyun add kiya gaya hai? Maine payment time pe kiya tha na.\",\n            \"Kya mujhe pata sakte ho ki customer care support ka fastest response time kitna hai?\",\n            \"The reason geese are still in central saskatchewan is because there's no snow and they still have food with the grain on the ground. The Canada Goose is probably best known for its fall migration, so in September and October, these guys will be flying south from their native land of Canada to overwinter in the United States. The reason they do this migration is primarily for food availability.\",\n            \"Mujhe samajh nahi aa raha ki yeh EMI calculation kaise hua hai. Kya aap mujhe explain karenge?\",\n            \"do you see any media houses on reddit, or celebrities or reddit being mentioned in any commercial mode of communication? do your parents/grandparents know about reddit? but (mostly) they ll know about facebook, instagram. thatâ€™s what is mainstream. reddit has about 50 million daily users while instagram has over 500 million daily users, so it should be self explanatory but nvm ðŸ’ðŸ»â€â™€ï\",\n            \"You sure that helps? Because I can't even walk that's how bad it made me feel . Low back pain is unbelievable. Muscles, headaches and I can't sleep without sleeping med. Clanapam wich is probably not good for me to take. Originally they proscribed this med. For heart palpation. Now I know whycI had heart palpation- elevated blood pressure. Don't we learn a lot just from interacting here .!!!\",\n            \"Could you provide a breakdown of how your new product feature will benefit me in the long run? I am really interested in understanding its potential advantages and whether it’s suitable for my needs. I have seen the initial product description, but I would like more detailed information on how it works, the technical specifications, and its compatibility with existing products. Can you explain how these new features will directly address my needs and make my current processes more efficient?\",\n            \"Could you let me know what steps are involved in applying for an extension on my subscription plan? I want to make sure that I understand the entire process, including any additional charges or requirements. Additionally, I would appreciate it if you could let me know if there are any promotions or discounts that I could use when extending my plan, as I am trying to make this a cost-effective decision. Could you provide a timeline for when this extension can be processed?\",\n            \"Yaar, kya tujhe pata hai ki Kalaghoda Arts Festival kab start hota hai? Main abhi tak schedule nahi dekh paaya. Kya iss baar bhi woh street performances aur workshops honge? Last time kaafi interesting laga tha, toh soch raha hoon iss baar pura explore karoon.\",\n            \"Do you know the process for applying for the advanced workshop on AI and Robotics? Where should I register?\",\n            \"Acha sun, kal ka group discussion main kaun-kaun lead karega? Mujhe time pe jaana hai toh plan kar raha hoon.\",\n            \"Can you clarify the grading criteria for the final project? Are there specific weightages for creativity, content, and presentation? Understanding this will help us prepare better and ensure we meet expectations.\",\n            \"क्या आप मुझे बता सकते हैं कि इस सप्ताह के अंत तक स्कूल की छुट्टियां कब से शुरू होंगी? हमें अपनी योजना बनाने में मदद मिलेगी।\",\n            \"Could you explain how the new feedback system works? Are we supposed to submit it online or in person? I just want to make sure I follow the correct process\",\n            \"माझ्या मुलासाठी स्कूलच्या प्रवेशासाठी अर्ज कसा करावा हे मी समजून घेऊ इच्छितो. सरकारने शाळांमध्ये विविध योजनांचा कार्यान्वयन सुरू केले आहे का? कोणती योजना माझ्या मुलासाठी उपयुक्त असेल? शिक्षणाचा स्तर वाढवण्यासाठी प्रशासनाच्या पुढील पावलांविषयी मला माहिती पाहिजे. शाळेच्या दाखल्यांमध्ये कोणत्या विशिष्ट गोष्टींची आवश्यकता आहे? आम्हाला सहकार्य करण्यासाठी प्रशासनाकडून योग्य माहिती मिळवली पाहिजे. कृपया मार्गदर्शन करा, आम्ही योग्य निर्णय घेऊ शकू.\"\n\n        ],\n        \"gujrati\":[\n            \"મારા હોમ લોન માટેની નવી યોજનાનો લાભ કેવી રીતે લેવો અને મને કેટલું વ્યાજ ભરવું પડશે?\"        ]\n    },\n    \"complaint\": {\n        \"english\": [\n            \n            \"My internet connection has been down since yesterday evening. Kindly send a technician to fix it urgently.\",\n            \"Well, that driver needs to penalized heavily for this. There are plenty of warnings. He can see the yellow flags flashing, the car on the side of the track, the car in the middle of the track and this is not like a blind corner where you are surprised by something appearing in front of you, it's a straight, plently of time to slowdown. He has no excuses for going that fast in such a situation.\",\n             \n            \"Bhai, mera bill kuch jyada ho gaya hai, kuch extra charges the. Kya karna padega?\",\n            \"Kuch bhi!! The guy is talking about battling misinformation and here you are making another bs claim about how youâ€™ve travelled 300 kms and cast your vote..unless there has been a secret ballot held for your constituency or just for you perhaps pray tell me where exactly did you go vote..or perhaps you misunderstood and you thought he meant voting on reality showsðŸ¤·ðŸ»â€â™‚ï¸I mean come on man seriously\" ,\n            \"Yaar, tumhari delivery service bahut slow hai. Ek toh koi updates nahi mil rahe. band hi kardo tum  apna yeh bussiness\",\n            \"Bhai, app baar baar crash ho rahi hai, kuch kaam karo yaar mera bhot important text ane wala hai ispe.\",\n            \"Mam is so not fair with us on the selection for the club mentors, she's only judging people based on their CGPA and not their skills so fucking unreasonable\",\n            \"Tu bata, Mumbai local ka crowd din ba din itna zyada kyun ho raha hai? Saans lena mushkil hota hai.\",\n            \"If muslim,sikh can wear their relegious attire freely then why it becomes an issue if hindu do same , Is this equality On this comment there are mainly muslim ,sikhs saying about the future of these kids care about your future they will surely become nice educated people Not like you guys partilly favour to your religion . Kattar hindu hu ye sab nahi shunga mere dharam pe alank lagane wallo ka\",\n            \"It seems the video is made to satisfy Modi Bhakts who are outside Maharashtra. Situation is very different in Maharashtra. People don`t attend Shinde and BJP`s Rally. BJP pays people to come to event and people run away while Bhashaan is going on .... BJP didn`t contest Andheri seat. They are scared to do so .... Also Modi is scared to contest BMC election, which is vacant from last 1 year ....\",\n            \"The lecture today was so confusing! The professor didn’t even explain the concepts properly and kept skipping slides. How are we supposed to understand the syllabus at this pace? We should talk to him and request more detailed classes.\",\n            \"Shazam bro I am your big fan if you see this msg pls reply me I have been using 14 pro for 5 months I had a school trip 3 weeks ago on that trip we had a dj program after that my camera had fully gone camera is not working I donâ€™t no what to do ðŸ¥ºðŸ’” I am from a middle class family 5 months pattiye pole Pani eduth vaangiya phone aayirunnu ippo moonji irikaan is there any way for solving this problem\",\n            \"Classroom की condition देखी है तुमने? Fans तो चलते ही नहीं हैं, और गर्मी में बैठना मुश्किल हो जाता है. ये AC कब repair होगा, कुछ पता है क्या? हमने कई बार complaint दी है, लेकिन कोई action नहीं लिया गया.\"\n        ],\n        \"hindi\": [\n            \"आपकी डिलीवरी सर्विस बहुत धीमी है। मुझे बहुत इंतजार करना पड़ा।\",\n            \"ऑनलाइन सेवेच्या वेळापत्रकात नेहमी गडबड होत असते. हे कधी सुधारणार?\",\n            \"Maine 3 din pehle order kiya tha, abhi tak mujhe delivery ka koi update nahi mila.\",\n            \"The payment portal on your website is too slow and keeps crashing.\",\n            \"तुमच्या कर्मचारी कर्मचार्‍यांचा वागणूक अत्यंत रुखा आहे. कृपया त्या बाबीवर लक्ष द्या.\",\n            \"तुमच्या वितरण सेवा खूपच धीमा आहे. माझ्या ऑर्डरची स्थिती तपासा.\",\n            \"तुमच्या वेबसाइटवर बार-बार क्रॅश होतो आहे. यावर उपाय शोधा.\",\n            \"गेल्या महिन्यात मी वीज बिल भरलं होतं, पण तरीही मला परत बिल आलं आहे. कृपया दुरुस्त करा.\",\n            \"तुमचा कॉल सतत डिसकनेक्ट होतो. नेटवर्कची समस्या काही केल्या संपत नाहीये.\",\n            \"काल मी गाडीतून प्रवास करताना खड्ड्यांमुळे त्रास झालो. रस्त्यांची अवस्था सुधारावी.\",\n            \"पाणीपुरवठा खंडित झालाय, सकाळपासून पाणी नाही. लवकरात लवकर पुरवठा चालू करा.\",\n            \"तुमच्या स्टॉपवर रिक्षा नेहमी उपलब्ध नसते. काही पर्याय आहे का?\",\n            \"मोदी साहेब वरती चांगले तो पण त्यांच्या साईटला आजूबाजूला सगळे काम करणार आहेत ना ती सगळी स्वतःच्या किसा भरणारी आहेत आणि त्यामुळे आम्हाला फक्त मोदी साहेबांमुळे फक्त बघून ह्या लोकांना मला मत कमळायला टाकू वाटते मोदी साहेबांना यांना पण त्यांची सगळी दोन नंबरची प\",\n             \"The road near our colony is in such bad condition that it takes an extra 30 minutes to reach the station. When will the authorities fix this mess?\",\n             \"Classroom में हर दिन पानी की दिक्कत हो रही है. Water cooler खराब पड़ा है, और कोई action नहीं लिया जा रहा. इतनी basic चीज़ भी manage नहीं कर सकते क्या?\",\n            \"किती दहशत आहे दोन्ही पवारांची बारामती कर मोकळे बोलू पण शकत नाहीत.त्यांना कोणा एकट्याचे उघड समर्थन देखील करू शकत नाहीत. खरंच बारामती मध्ये खूप दहशत दोन्ही पवारांची हे लक्षात येते\",\n             \"माझ्या कॉलनीमधल्या कचऱ्याच्या टाक्या खूप भरल्या आहेत, आणि अजून कोणी उचलायला आलेलं नाही. यामुळे परिसरात दुर्गंधी पसरली आहे. तात्काळ काहीतरी करायला हवं.\",\n             \"Why is the gym AC not working for the past week? It’s so uncomfortable to work out in this heat. I’ve complained twice, but no action has been taken.\",\n            \"Library में books का arrangement इतना खराब है कि कोई भी book time पर नहीं मिलती. Staff को system improve करना चाहिए ताकि students को convenience हो.\",\n           \"तुम्हें पता है, hostel का खाना अब बिल्कुल खराब हो गया है. हर दिन वही oily और ठंडा खाना मिलता है. कुछ बोलना पड़ेगा warden को.\",\n           \"The streetlights in our area haven’t been working for a week now. It’s so unsafe at night. Someone should report this to the municipal office.\",\n           \"Classroom में projector फिर से खराब हो गया है. Presentation देना impossible हो जाता है. IT department कभी timely repair क्यों नहीं करता?\",\n           \"Why does the bus always run late in the mornings? It’s making everyone late for work. This needs to be addressed immediately.\",\n         \"Library में WiFi इतना slow है कि कोई भी resource download नहीं हो पाता. ये तो students के लिए basic necessity है, और इसे जल्दी ठीक होना चाहिए.\",\n            \"The gym equipment in our society is in poor condition, with several machines not functioning properly. Despite repeated complaints, nothing has been fixed. It’s high time this issue is addressed.\",\n            \"हमारी बिल्डिंग में पानी की सप्लाई पिछले कुछ दिनों से अनियमित हो गई है. कभी पानी आता है, कभी नहीं. प्लीज इसकी जांच करके इसे तुरंत ठीक करें.\",\n           \"Yaar, jo tiffin aaj college ke canteen se liya tha, usme khana ekdum stale tha. Complain karni padegi.\",\"Classroom ke fans repair karne ke liye ab tak koi technician nahi aaya. Bahut zyada garmi ho rahi hai.\",\n            \"शाळेतील पिण्याच्या पाण्याच्या टाक्या अजूनही साफ झाल्या नाहीत. त्यामुळे विद्यार्थ्यांच्या आरोग्याला धोका आहे. कृपया पाण्याच्या स्वच्छतेसाठी काहीतरी ठोस उपाय करा.\",\n            \"रात के समय बिजली बार-बार कट रही है, जिससे हम काम नहीं कर पा रहे हैं। कृपया इस समस्या का समाधान जल्द से जल्द करवाइए। यह बहुत परेशानी का कारण बन रहा है।\",\n \"The public transport service has been consistently delayed over the last few weeks. I missed important appointments because of this. Can we expect any improvements or alternatives to prevent this from continuing?\",\n \"Bhai, mere apartment ka parking area jo hai, wo bohot messy ho gaya hai. Log apni car idhar-udhar park karte hain. Humein kuch manage karke parking ka system fix karna padega.\",\n            \"मैंने आपकी कंपनी से एक लैपटॉप खरीदा था, लेकिन जब उसे खोला तो उसमें कुछ खामियां थीं। स्क्रीन पर धब्बे थे और बैटरी की बैकअप भी बहुत कम थी। मैंने ग्राहक सेवा से संपर्क किया, लेकिन मुझे कोई उचित समाधान नहीं मिला। मुझे उम्मीद थी कि आपकी कंपनी उच्च गुणवत्ता वाले उत्पाद प्रदान करेगी, लेकिन अब मुझे बहुत निराशा हो रही है। कृपया इस समस्या को जल्द हल करें और मुझे रिप्लेसमेंट या रिफंड की प्रक्रिया के बारे में जानकारी दें।\",\n\"मुझे आपके द्वारा दी गई इंटरनेट सेवा से बहुत परेशानी हो रही है। पिछले कुछ दिनों से इंटरनेट की स्पीड बहुत धीमी हो गई है, और कई बार तो सर्विस बिल्कुल भी काम नहीं कर रही। मैंने कई बार शिकायत की है, लेकिन कोई ठोस समाधान नहीं मिला। यह मेरे लिए बहुत ही असुविधाजनक है क्योंकि मैं वर्क फ्रॉम होम करता हूं और इस सेवा की अत्यधिक आवश्यकता है। कृपया इसे जल्द ठीक करें और मेरी समस्या का समाधान करें।\",\n\n\"मैंने कुछ दिन पहले आपके स्टोर से एक जोड़ी जूते खरीदी थी, लेकिन जब मैंने उसे पहना तो उसे बहुत असुविधा महसूस हुई। जूते की क्वालिटी बिल्कुल खराब थी और उसमें से बहुत तेज़ गंध आ रही थी। मैंने उसे वापस करने की कोशिश की, लेकिन स्टोर में किसी ने मेरी मदद नहीं की। अब मुझे यह महसूस हो रहा है कि आपने मुझे दोषी करार दिया है। कृपया इसे शीघ्र सुलझाएं और मुझे अपनी समस्या का समाधान दें।\",\n            \"Maine tumhari gym membership li thi aur jo services mujhe mile, wo bilkul bhi satisfactory nahi thi. Gym ki cleanliness pe bahut dikkat thi, aur equipment bhi out of order the. Jab maine complaint ki, toh staff ne sirf fake assurances diye, lekin kuch bhi improve nahi kiya gaya. Aise kaise chalega? Agar tumhare gym ka standard itna low hai, toh mujhe apni membership refund karni padegi. Mujhe better services ki ummid thi. Please jaldi solution do, warna main koi aur gym join karunga.\",\n\n\"Tumhari food delivery service ne mujhe kaafi disappoint kiya hai. Mainne ek pizza order kiya tha, lekin jab wo aaya toh wo bilkul cold tha. Uska taste bhi kuch aur hi tha, bilkul fresh nahi laga. Maine customer service se contact kiya, lekin unhone koi proper explanation nahi diya. Aise kaise chalega? Mujhe toh lagta hai ki tumhare delivery process ko improve karne ki zarurat hai. Agar mujhe baar baar aise issues face karne padenge, toh main tumhe recommend nahi karunga. Jaldi solution chahiye.\",\"I’m facing a serious issue with my mobile phone, as it keeps freezing frequently, even after multiple resets. I need to use important apps for work, but this issue is making it almost impossible. I reached out to customer service, but they haven’t been helpful at all. It seems like they don’t understand the urgency. Please provide a proper solution or replacement, as this has been very disruptive for my daily tasks. I would appreciate it if this issue is resolved at the earliest.\",\n            \"सरकारच्या सार्वजनिक वाहतूक सेवांमध्ये प्रचंड अडचणी आहेत. बसेस वेळेवर धावत नाहीत आणि प्रवाशांना अतिरिक्त शुल्क आकारले जाते. सरकारने सार्वजनिक वाहतूक प्रणालीमध्ये सुधारणा केली पाहिजे, ज्यामुळे प्रवास सुलभ आणि किफायतशीर होईल.\",\n            \"The train services at the railway station are frequently delayed. Passengers do not receive proper information, and delays have become a common occurrence. The government should improve the information systems at stations and make the train services more reliable.\",\n            \"सरकारच्या शाळांमध्ये शिक्षकांची कमतरता आहे. विद्यार्थ्यांची संख्या खूप जास्त आहे आणि शिक्षकांचा अभाव आहे. काही विषयांमध्ये विशेष तज्ञ शिक्षकांनाही भरण्यात आलेले नाही. यामुळे शिक्षणावर नकारात्मक परिणाम होतो आहे. सरकारने अधिक शिक्षकांची नियुक्ती करण्याची आवश्यकता आहे.\",\n            \n\n\n        ],\n        \n    },\n    \"praise\":{\n        \"english\":[\n            \"Aapke kaam ka impact profound hai. Aap extraordinary ability rakhte ho ideas ko life mein lane aur boundaries push karne mein. Aapka enthusiasm aur passion ke saath kaam karna ek pleasure hai.\",\n            \"Aapka leadership aur vision ne humare projects ko naye heights tak le gaya hai. Team ko motivate aur guide karne ki aapki ability remarkable hai. Aapka dedication aur unwavering commitment to excellence ke liye hum deeply appreciate karte hai.\",\n            \"तुमची वक्तृत्वकला आणि प्रेझेंटेशन कौशल्य खूपच प्रभावी आहे. तुमचा आत्मविश्वास आणि स्पष्टता खूप प्रेरणादायक आहे.\",\n            \"Mummy, aapki cooking amazing hai! Sab friends hamesha tareef karte hain aur bolte hai ki aapke jaisa khana aaj tak nahi khaya unhone hehehe.\",\n            \"Your explanation on the new topic in today’s lecture was very clear. It’s rare to see this level of clarity.We need such proffesors like you who can make learning physics this easy\"\n            \"Yaar, tu har sports event mein lead karta hai. Teri energy aur commitment ekdum top-notch hai!..You should  be a proffesional athlete mere bhai tu india ka naam roshan karega dekh lena meri intution hai\",\n             \"तुझ्या स्केचिंगचं कौशल्य खरंच अप्रतिम आहे. तुला प्रत्येक नजरेचं बारकाईने निरीक्षण करण्याची कला आहे.\",\"Tere dancing ka jo ekdum smooth flow hai na, woh crowd ko ekdum engage kar leta hai. Amazing!\",\n            \"तुम्ही केलेली कामं पाहून खूप आनंद झाला. तुमच्या प्रयत्नांमुळे शाळेत एक नवीन ऊर्जा निर्माण झाली आहे. तुमच्या योगदानामुळे सर्व विद्यार्थ्यांना नवीन शिकण्याची प्रेरणा मिळाली आहे. तुम्ही जी मेहनत घेत आहात, ती खूपच प्रेरणादायी आहे. तुमच्या या कौशल्यामुळे अनेक लोकांना मदत होईल. तुमचे काम एक आदर्श ठरले आहे, आणि तुमचं समर्पण आणि मेहनत खूप कौतुकास्पद आहे.\",\n            \"तुमचं काम खूपच उत्कृष्ट आहे! तुम्ही सगळ्या समस्यांचे समाधान शोधून त्यावर काम केलं आहे. तुम्ही केवळ उत्कृष्ट कार्य केलेच नाही, तर इतरांना देखील मदतीचा हात दिला आहे. तुमचं समर्पण आणि मेहनत नेहमीच उदाहरण ठरते.\",\n            \"तुम्ही गेल्या काही महिन्यांत केलेल्या कामामुळे आम्हाला खूप सकारात्मक बदल दिसले. तुमचं नेतृत्व खूप प्रेरणादायक आहे, आणि तुमच्यामुळे सर्वांना योग्य दिशा मिळाली आहे. तुम्ही दिलेल्या मार्गदर्शनामुळे सगळ्यांचा आत्मविश्वास वाढला आहे.\",\n            \"तुमचं काम फारच प्रभावशाली आहे. तुमच्या मेहनतीमुळे अनेक लोकांचं जीवन सुधारलं आहे. तुम्ही जे काम करत आहात, त्याची समाजाला खूप आवश्यकता आहे. तुमचं कार्य निश्चितच इतरांना प्रेरित करेल.\",\n            \"तुमच्या कौशल्याने आणि ज्ञानाने खूप लोकांना मदत केली आहे. तुमच्या दिलेल्या मार्गदर्शनामुळे अनेक जण उत्तम कार्य करीत आहेत. तुमचं काम हे खरंच एक आदर्श आहे. तुमच्यामुळे खूप लोकांच्या जीवनात सकारात्मक बदल झाला आहे.\",\n            \"आपके द्वारा किए गए कार्य ने सचमुच हम सबको प्रेरित किया है। आपकी मेहनत और समर्पण ने टीम को एक नई दिशा दी है। आपने न केवल उत्कृष्ट परिणाम हासिल किए हैं, बल्कि दूसरों को भी सीखने और आगे बढ़ने के लिए प्रेरित किया है। आपका कार्य सभी के लिए एक आदर्श बन गया है।\",\n            \"आपका काम शानदार है! आपने जो भी किया, उसे न केवल बखूबी अंजाम दिया, बल्कि दूसरों को भी मदद की है। आपका समर्पण और परिश्रम वाकई काबिले तारीफ है। आपके योगदान ने सबको एक नई दिशा दी है।\",\n            \"आपकी मेहनत और समर्पण ने संस्था में कई सकारात्मक बदलाव लाए हैं। आपके मार्गदर्शन से कई लोग अपनी क्षमताओं को पहचान पाए हैं और अपने कार्य में सुधार कर पा रहे हैं। आपके कार्य से हम सभी को प्रेरणा मिलती है।\",\n            \"आपका काम अत्यधिक सराहनीय है। आपने अपनी मेहनत और ईमानदारी से हर एक चुनौती का सामना किया है। आपके योगदान ने न केवल टीम को मजबूती दी है, बल्कि हम सबको और भी बेहतर करने के लिए प्रेरित किया है।\",\n            \"आपने जिस तरह से पिछले कुछ महीनों में कठिन परिस्थितियों को संभाला है, वह सराहनीय है। आपकी प्रेरणा और नेतृत्व से टीम ने बेहतरीन परिणाम हासिल किए हैं। आपका काम सभी के लिए प्रेरणा का स्रोत बन गया है।\",\n        ]\n    }\n}\n\n\n\ndef add_generated_examples(df, text_examples):\n    new_data = []\n    \n    id_start = max(df['id'].max(), 0) + 1 if not df.empty else 1\n\n    for label, languages in text_examples.items():\n        for lang, texts in languages.items():\n            for text in texts:\n                row = {\n                    \"id\": id_start,\n                    \"text\": text,\n                    \"praise\": 1 if label ==\"praise\" else 0,\n                    \"demands\": 1 if label == \"demands\" else 0,\n                    \"complaint\": 1 if label ==\"complaint\" else 0,\n                    \"questions\": 1 if label == \"questions\" else 0\n                }\n                new_data.append(row)\n                id_start += 1\n                \n    random.shuffle(new_data)\n    return pd.concat([df, pd.DataFrame(new_data)], ignore_index=True)\n\n\n\ndf1 = add_generated_examples(df, text_examples)\n\n# Display the updated dataframe\nprint(df1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:10.207216Z","iopub.execute_input":"2025-01-20T10:29:10.207560Z","iopub.status.idle":"2025-01-20T10:29:10.267122Z","shell.execute_reply.started":"2025-01-20T10:29:10.207498Z","shell.execute_reply":"2025-01-20T10:29:10.266430Z"}},"outputs":[{"name":"stdout","text":"                 id                                               text  \\\n0      500796286320  Wow! From what I've observed from this documen...   \n1      838906157157  काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...   \n2     1011026626743  अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...   \n3     1068853499446  She's saying that \"doing her own research\" led...   \n4      502772748919  That is not Karen, that is perfectly reasonabl...   \n...             ...                                                ...   \n4164  1099483387630  Classroom की condition देखी है तुमने? Fans तो ...   \n4165  1099483387641  पाणीपुरवठा खंडित झालाय, सकाळपासून पाणी नाही. ल...   \n4166  1099483387664  मुझे आपके द्वारा दी गई इंटरनेट सेवा से बहुत पर...   \n4167  1099483387581  क्याआप बता सकते हैं कि मैंने पिछली बार जो कंप्...   \n4168  1099483387644  The road near our colony is in such bad condit...   \n\n      complaint  demands  praise  questions  \n0           0.0      0.0     1.0        0.0  \n1           1.0      0.0     0.0        0.0  \n2           0.0      0.0     1.0        0.0  \n3           1.0      0.0     0.0        0.0  \n4           1.0      0.0     0.0        0.0  \n...         ...      ...     ...        ...  \n4164        1.0      0.0     0.0        0.0  \n4165        1.0      0.0     0.0        0.0  \n4166        1.0      0.0     0.0        0.0  \n4167        0.0      0.0     0.0        1.0  \n4168        1.0      0.0     0.0        0.0  \n\n[4169 rows x 6 columns]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T08:15:27.047133Z","iopub.execute_input":"2025-01-20T08:15:27.047546Z","iopub.status.idle":"2025-01-20T08:15:27.065652Z","shell.execute_reply.started":"2025-01-20T08:15:27.047513Z","shell.execute_reply":"2025-01-20T08:15:27.064562Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                 id                                               text  \\\n0      500796286320  Wow! From what I've observed from this documen...   \n1      838906157157  काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...   \n2     1011026626743  अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...   \n3     1068853499446  She's saying that \"doing her own research\" led...   \n4      502772748919  That is not Karen, that is perfectly reasonabl...   \n...             ...                                                ...   \n4165  1099483387686  आपका काम शानदार है! आपने जो भी किया, उसे न केव...   \n4166  1099483387671  The train services at the railway station are ...   \n4167  1099483387650  Library में books का arrangement इतना खराब है ...   \n4168  1099483387668  Tumhari food delivery service ne mujhe kaafi d...   \n4169  1099483387608  do you see any media houses on reddit, or cele...   \n\n      complaint  demands  praise  questions  \n0           0.0      0.0     1.0        0.0  \n1           1.0      0.0     0.0        0.0  \n2           0.0      0.0     1.0        0.0  \n3           1.0      0.0     0.0        0.0  \n4           1.0      0.0     0.0        0.0  \n...         ...      ...     ...        ...  \n4165        0.0      0.0     1.0        0.0  \n4166        1.0      0.0     0.0        0.0  \n4167        1.0      0.0     0.0        0.0  \n4168        1.0      0.0     0.0        0.0  \n4169        0.0      0.0     0.0        1.0  \n\n[4170 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500796286320</td>\n      <td>Wow! From what I've observed from this documen...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>838906157157</td>\n      <td>काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1011026626743</td>\n      <td>अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1068853499446</td>\n      <td>She's saying that \"doing her own research\" led...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>502772748919</td>\n      <td>That is not Karen, that is perfectly reasonabl...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4165</th>\n      <td>1099483387686</td>\n      <td>आपका काम शानदार है! आपने जो भी किया, उसे न केव...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4166</th>\n      <td>1099483387671</td>\n      <td>The train services at the railway station are ...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4167</th>\n      <td>1099483387650</td>\n      <td>Library में books का arrangement इतना खराब है ...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4168</th>\n      <td>1099483387668</td>\n      <td>Tumhari food delivery service ne mujhe kaafi d...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4169</th>\n      <td>1099483387608</td>\n      <td>do you see any media houses on reddit, or cele...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4170 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"stopwords = set([\"the\", \"and\", \"to\", \"of\", \"a\", \"in\", \"for\", \"on\", \"with\", \"by\", \"an\", \"it\", \"from\", \"as\", \"be\", \"that\",\"are\",\"ka\"])  \n\ndef remove_stopwords(text):\n    text = text.lower()\n    return \" \".join([word for word in text.split() if word not in stopwords])\n\ndf1['text'] = df1['text'].apply(remove_stopwords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:31:32.698021Z","iopub.execute_input":"2025-01-20T09:31:32.698300Z","iopub.status.idle":"2025-01-20T09:31:32.752703Z","shell.execute_reply.started":"2025-01-20T09:31:32.698280Z","shell.execute_reply":"2025-01-20T09:31:32.752084Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T09:31:32.882863Z","iopub.execute_input":"2025-01-20T09:31:32.883125Z","iopub.status.idle":"2025-01-20T09:31:32.895503Z","shell.execute_reply.started":"2025-01-20T09:31:32.883106Z","shell.execute_reply":"2025-01-20T09:31:32.894782Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 id                                               text  \\\n0      500796286320  wow! what i've observed this documentary natio...   \n1      838906157157  काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...   \n2     1011026626743  अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...   \n3     1068853499446  she's saying \"doing her own research\" led diff...   \n4      502772748919  is not karen, is perfectly reasonable. she los...   \n...             ...                                                ...   \n4166  1099483387687  आपका काम शानदार है! आपने जो भी किया, उसे न केव...   \n4167  1099483387657  gym equipment our society is poor condition, s...   \n4168  1099483387631  shazam bro i am your big fan if you see this m...   \n4169  1099483387554  tujhe office supplies urgent chahiye kya , kal...   \n4170  1099483387602  can you provide some insight into why delivery...   \n\n      complaint  demands  praise  questions  \n0           0.0      0.0     1.0        0.0  \n1           1.0      0.0     0.0        0.0  \n2           0.0      0.0     1.0        0.0  \n3           1.0      0.0     0.0        0.0  \n4           1.0      0.0     0.0        0.0  \n...         ...      ...     ...        ...  \n4166        0.0      0.0     1.0        0.0  \n4167        1.0      0.0     0.0        0.0  \n4168        1.0      0.0     0.0        0.0  \n4169        0.0      1.0     0.0        0.0  \n4170        0.0      0.0     0.0        1.0  \n\n[4171 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500796286320</td>\n      <td>wow! what i've observed this documentary natio...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>838906157157</td>\n      <td>काय रे dungnat मेंदु असणाऱ्या आंधभक्ता तुझा आई...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1011026626743</td>\n      <td>अजित दादा आणि प्रफुल्ल पटेल यांनी केलेल्या काम...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1068853499446</td>\n      <td>she's saying \"doing her own research\" led diff...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>502772748919</td>\n      <td>is not karen, is perfectly reasonable. she los...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4166</th>\n      <td>1099483387687</td>\n      <td>आपका काम शानदार है! आपने जो भी किया, उसे न केव...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4167</th>\n      <td>1099483387657</td>\n      <td>gym equipment our society is poor condition, s...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4168</th>\n      <td>1099483387631</td>\n      <td>shazam bro i am your big fan if you see this m...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4169</th>\n      <td>1099483387554</td>\n      <td>tujhe office supplies urgent chahiye kya , kal...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4170</th>\n      <td>1099483387602</td>\n      <td>can you provide some insight into why delivery...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4171 rows × 6 columns</p>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def get_language(text):\n    return Detector(\"\".join(x for x in text if x.isprintable()), quiet=True).languages[0].name\n\ndf1[\"language\"] = df1[\"text\"].progress_apply(get_language)\ndf\ndf2[\"language\"]=df2[\"text\"].progress_apply(get_language)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:13:01.649092Z","iopub.execute_input":"2025-01-19T13:13:01.649424Z","iopub.status.idle":"2025-01-19T13:13:01.742578Z","shell.execute_reply.started":"2025-01-19T13:13:01.649399Z","shell.execute_reply":"2025-01-19T13:13:01.741557Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-c22889c5241d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misprintable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"language\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'progress_apply'"],"ename":"AttributeError","evalue":"'Series' object has no attribute 'progress_apply'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T15:37:00.919994Z","iopub.execute_input":"2025-01-17T15:37:00.920271Z","iopub.status.idle":"2025-01-17T15:37:00.933399Z","shell.execute_reply.started":"2025-01-17T15:37:00.920251Z","shell.execute_reply":"2025-01-17T15:37:00.932546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ENGLISH vs NON_ENGLISH","metadata":{}},{"cell_type":"code","source":"lang_list = sorted(list(set(df1[\"language\"])))\ncounts = [list(df1[\"language\"]).count(cont) for cont in lang_list]\ndf = pd.DataFrame(np.transpose([lang_list, counts]))\ndf.columns = [\"Languages\", \"Count\"]\ndf[\"Count\"] = df[\"Count\"].apply(int)\n\ndf_en = pd.DataFrame(np.transpose([[\"English\", \"Non-English\"], [max(counts), sum(counts) - max(counts)]]))\ndf_en.columns = [\"Languages\", \"Count\"]\n\nfig = px.bar(df_en, x=\"Languages\", y=\"Count\", title=\"Language of comments\", color=\"Languages\", text=\"Count\")\nfig.update_layout(template=\"plotly_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig.data[0].textfont.color = \"black\"\nfig.data[0].textposition = \"outside\"\nfig.data[1].textfont.color = \"black\"\nfig.data[1].textposition = \"outside\"\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:00:30.672659Z","iopub.execute_input":"2025-01-15T18:00:30.673081Z","iopub.status.idle":"2025-01-15T18:00:30.770139Z","shell.execute_reply.started":"2025-01-15T18:00:30.673048Z","shell.execute_reply":"2025-01-15T18:00:30.769165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = px.bar(df.query(\"Languages != 'English' and Languages != 'un'\").query(\"Count >= 0\"),\n             y=\"Languages\", x=\"Count\", title=\"Language of non-English comments\", template=\"plotly_white\", color=\"Languages\", text=\"Count\", orientation=\"h\")\nfig.update_traces(marker=dict(line=dict(width=0.75,\n                                        color='black')),  textposition=\"outside\")\nfig.update_layout(showlegend=False)\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:00:30.868967Z","iopub.execute_input":"2025-01-15T18:00:30.869295Z","iopub.status.idle":"2025-01-15T18:00:31.038817Z","shell.execute_reply.started":"2025-01-15T18:00:30.869271Z","shell.execute_reply":"2025-01-15T18:00:31.038077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lang_list = sorted(list(set(df2[\"language\"])))\ncounts = [list(df2[\"language\"]).count(cont) for cont in lang_list]\ndf = pd.DataFrame(np.transpose([lang_list, counts]))\ndf.columns = [\"Languages\", \"Count\"]\ndf[\"Count\"] = df[\"Count\"].apply(int)\n\ndf_en = pd.DataFrame(np.transpose([[\"English\", \"Non-English\"], [max(counts), sum(counts) - max(counts)]]))\ndf_en.columns = [\"Languages\", \"Count\"]\n\nfig = px.bar(df.query(\"Languages != 'English' and Languages != 'un'\").query(\"Count >= 0\"),\n             y=\"Languages\", x=\"Count\", title=\"Language of non-English comments\", template=\"plotly_white\", color=\"Languages\", text=\"Count\", orientation=\"h\")\nfig.update_traces(marker=dict(line=dict(width=0.75,\n                                        color='black')),  textposition=\"outside\")\nfig.update_layout(showlegend=False)\nfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:00:31.063438Z","iopub.execute_input":"2025-01-15T18:00:31.063784Z","iopub.status.idle":"2025-01-15T18:00:31.168786Z","shell.execute_reply.started":"2025-01-15T18:00:31.063760Z","shell.execute_reply":"2025-01-15T18:00:31.167916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**After looking through the data we can see that some texts listed has english language has some hinglish text containing hindi words written in english like kya bhadiya etc**","metadata":{}},{"cell_type":"markdown","source":"**looks like test set contains some data rows with languages that are not present in training set.... needs to handle this somehow**","metadata":{}},{"cell_type":"code","source":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\ndf1[\"comment_words\"] = df1[\"text\"].apply(new_len)\nnums = df1.query(\"comment_words != 0 and comment_words < 200\").sample(frac=0.1)[\"comment_words\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All comments\"],\n                         colors=[\"coral\"])\n\nfig.update_layout(title_text=\"Comment words\", xaxis_title=\"Comment words\", template=\"simple_white\", showlegend=False)\nfig.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:00:39.489433Z","iopub.execute_input":"2025-01-15T18:00:39.489814Z","iopub.status.idle":"2025-01-15T18:00:39.578158Z","shell.execute_reply.started":"2025-01-15T18:00:39.489782Z","shell.execute_reply":"2025-01-15T18:00:39.575590Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Visualization of most occuring words in all the four categories of classfication**","metadata":{}},{"cell_type":"code","source":"# Function to get all text for a specific category\ndef get_text_by_category(df, category_column):\n    \"\"\"\n    Combine all the text data corresponding to a specific binary category column.\n    \"\"\"\n    texts = df[df[category_column] == 1]['text'].values\n    return \" \".join(texts)\n\n# Function to get frequent words\ndef get_frequent_words(text, top_n=20):\n    \"\"\"\n    Get the top N frequent words from the text.\n    \"\"\"\n    words = text.split()\n    word_counts = Counter(words)\n    return word_counts.most_common(top_n)\n\ndef generate_word_cloud(text, category_name):\n    \"\"\"\n    Generate and display a word cloud for a given category.\n    \"\"\"\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(f\"Word Cloud for {category_name}\", fontsize=16)\n    plt.show()\n    \ndef plot_top_words(text, category_name, top_n=20):\n    \"\"\"\n    Plot a bar chart of the most frequent words for a given category.\n    \"\"\"\n    frequent_words = get_frequent_words(text, top_n=top_n)\n    words, counts = zip(*frequent_words)\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=list(counts), y=list(words), palette=\"viridis\")\n    plt.title(f\"Top {top_n} Words in {category_name}\", fontsize=16)\n    plt.xlabel(\"Frequency\")\n    plt.ylabel(\"Words\")\n    plt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T15:08:22.164947Z","iopub.execute_input":"2025-01-15T15:08:22.165512Z","iopub.status.idle":"2025-01-15T15:08:22.174230Z","shell.execute_reply.started":"2025-01-15T15:08:22.165468Z","shell.execute_reply":"2025-01-15T15:08:22.173046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of category columns\ncategory_columns = ['complaint','demands','praise','questions']\n\nfor category in category_columns:\n    print(f\"Processing category: {category}\")\n    \n    # Get text for this category\n    category_text = get_text_by_category(df1, category)\n    \n    # Generate Word Cloud\n    generate_word_cloud(category_text, category)\n    \n    # Plot Top Words\n    plot_top_words(category_text, category, top_n=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:21:54.284057Z","iopub.execute_input":"2025-01-14T11:21:54.284411Z","iopub.status.idle":"2025-01-14T11:22:01.707545Z","shell.execute_reply.started":"2025-01-14T11:21:54.284384Z","shell.execute_reply":"2025-01-14T11:22:01.706383Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_text_by_language_and_category(df, category, language):\n    \"\"\"\n    Get all text data for a specific category and language.\n    \"\"\"\n    texts = df[(df[category] == 1) & (df['language'] == language)]['text'].values\n    return \" \".join(texts)\n\n# Example for generating word clouds for all languages\nlanguagess = ['Urdu', 'Malayalam', 'Sinhala',\n       'Uzbek', 'Afar', 'Persian', 'Czech', 'Southern Sotho', 'un',\n       'Tagalog', 'Indonesian', 'Manx', 'Akan', 'Tamil',\n\n              'Scottish Gaelic']  # Add detected languages here\n\nfor category in category_columns:\n    for language in languagess:\n        print(f\"Category: {category}, Language: {language}\")\n        \n        # Get text for this language and category\n        language_category_text = get_text_by_language_and_category(df1, category, language)\n        \n        if language_category_text:  # Check if there is text for this combination\n            # Generate Word Cloud\n            generate_word_cloud(language_category_text, f\"{category} ({language})\")\n            \n            # Plot Top Words\n            plot_top_words(language_category_text, f\"{category} ({language})\", top_n=20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T11:22:31.637468Z","iopub.execute_input":"2025-01-14T11:22:31.637884Z","iopub.status.idle":"2025-01-14T11:22:50.719337Z","shell.execute_reply.started":"2025-01-14T11:22:31.637825Z","shell.execute_reply":"2025-01-14T11:22:50.718212Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_remove=['Uzbek', 'Afar', 'Czech', 'Southern Sotho','Urdu','Tamil','Akan','Persian','Scottish Gaelic','Manx']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T18:35:30.115139Z","iopub.execute_input":"2025-01-14T18:35:30.115567Z","iopub.status.idle":"2025-01-14T18:35:30.120465Z","shell.execute_reply.started":"2025-01-14T18:35:30.115525Z","shell.execute_reply":"2025-01-14T18:35:30.119184Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1_filtered = df1[~df1['language'].isin(labels_remove)]\ndf1_filtered['language'].unique()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install indic-nlp-library\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T13:36:03.427546Z","iopub.execute_input":"2025-01-14T13:36:03.427884Z","iopub.status.idle":"2025-01-14T13:36:10.006839Z","shell.execute_reply.started":"2025-01-14T13:36:03.427860Z","shell.execute_reply":"2025-01-14T13:36:10.005880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import string\nfrom indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\nfrom indicnlp.tokenize import indic_tokenize\n\n\ndef process_english_text(text):\n    \"\"\"\n    Preprocess English text: remove stopwords, lowercase, and strip punctuation.\n    \"\"\"\n    stop_words = set(stopwords.words('english'))\n    tokens = word_tokenize(text.lower()) \n    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n    return \" \".join(tokens)\n\n\n#def process_hinglish_text(text):\n    \"\"\"\n    Preprocess Hinglish text by transliterating and cleaning it.\n    \"\"\"\n   # try:\n        #transliterated = UnicodeIndicTransliterator.transliterate(text, 'h', 'en')\n       # tokens = word_tokenize(transliterated.lower())\n      #  tokens = [word for word in tokens if word not in string.punctuation]\n     #   return \" \".join(tokens)\n   # except Exception as e:\n    #    return text.lower()\n\ndef process_hindi_text(text):\n    \"\"\"\n    Preprocess Hindi text: tokenize and remove punctuations.\n    \"\"\"\n    try:\n        tokens = list(indic_tokenize.trivial_tokenize(text))\n        tokens = [word for word in tokens if word not in string.punctuation]\n        return \" \".join(tokens)\n    except Exception as e:\n        return text\n        \ndef process_marathi_text(text):\n    \"\"\"\n    Preprocess Marathi text: tokenize and remove punctuations.\n    \"\"\"\n    try:\n        tokens = list(indic_tokenize.trivial_tokenize(text))\n        tokens = [word for word in tokens if word not in string.punctuation]\n        return \" \".join(tokens)\n    except Exception as e:\n        return text\ndef process_minor_language_text(text):\n    \"\"\"\n    Preprocess text in minority languages by removing punctuation and normalizing.\n    \"\"\"\n    tokens = word_tokenize(text.lower())\n    tokens = [word for word in tokens if word not in string.punctuation]\n    return \" \".join(tokens)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T13:36:10.954750Z","iopub.execute_input":"2025-01-14T13:36:10.955092Z","iopub.status.idle":"2025-01-14T13:36:10.971466Z","shell.execute_reply.started":"2025-01-14T13:36:10.955063Z","shell.execute_reply":"2025-01-14T13:36:10.970466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_text_by_language(row):\n    \"\"\"\n    Process text based on the detected language.\n    \"\"\"\n    lang = row['language']\n    text = row['text']\n\n    if lang == 'English':\n        return process_english_text(text)\n    #elif lang == 'Hinglish':\n        #return process_hinglish_text(text)\n    elif lang == 'Hindi':\n        return process_hindi_text(text)\n    elif lang == 'Marathi':\n        return process_marathi_text(text)\n    elif lang in ['Urdu', 'Malayalam', 'Sinhala',\n       'Uzbek', 'Afar', 'Persian', 'Czech', 'Southern Sotho', 'un',\n       'Tagalog', 'Indonesian', 'Manx', 'Akan', 'Tamil',\n       'Scottish Gaelic']:\n        return process_minor_language_text(text)\n    else:\n        return text  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T13:36:15.197731Z","iopub.execute_input":"2025-01-14T13:36:15.198063Z","iopub.status.idle":"2025-01-14T13:36:15.202967Z","shell.execute_reply.started":"2025-01-14T13:36:15.198034Z","shell.execute_reply":"2025-01-14T13:36:15.202063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\n\n\nnltk.download('stopwords')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T13:36:17.198173Z","iopub.execute_input":"2025-01-14T13:36:17.198501Z","iopub.status.idle":"2025-01-14T13:36:17.261643Z","shell.execute_reply.started":"2025-01-14T13:36:17.198471Z","shell.execute_reply":"2025-01-14T13:36:17.260895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1['text'] = df1.apply(process_text_by_language, axis=1)\ndf2['text']=df2.apply(process_text_by_language,axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T13:36:20.478249Z","iopub.execute_input":"2025-01-14T13:36:20.478553Z","iopub.status.idle":"2025-01-14T13:36:22.273482Z","shell.execute_reply.started":"2025-01-14T13:36:20.478528Z","shell.execute_reply":"2025-01-14T13:36:22.272824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T13:36:23.616325Z","iopub.execute_input":"2025-01-14T13:36:23.616682Z","iopub.status.idle":"2025-01-14T13:36:23.631850Z","shell.execute_reply.started":"2025-01-14T13:36:23.616651Z","shell.execute_reply":"2025-01-14T13:36:23.630952Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Tokenization**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(max_features=4000)\nX = tfidf_vectorizer.fit_transform(df1['text'])\nXt = tfidf_vectorizer.fit_transform(df2['text'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T15:20:08.276634Z","iopub.execute_input":"2025-01-15T15:20:08.277145Z","iopub.status.idle":"2025-01-15T15:20:08.715201Z","shell.execute_reply.started":"2025-01-15T15:20:08.277098Z","shell.execute_reply":"2025-01-15T15:20:08.713725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Xt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T15:20:11.505662Z","iopub.execute_input":"2025-01-15T15:20:11.506054Z","iopub.status.idle":"2025-01-15T15:20:11.514017Z","shell.execute_reply.started":"2025-01-15T15:20:11.506024Z","shell.execute_reply":"2025-01-15T15:20:11.512720Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Embedding**","metadata":{}},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:45:07.401798Z","iopub.execute_input":"2025-01-16T17:45:07.402123Z","iopub.status.idle":"2025-01-16T17:45:10.946479Z","shell.execute_reply.started":"2025-01-16T17:45:07.402097Z","shell.execute_reply":"2025-01-16T17:45:10.945619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nembedder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  \nX = embedder.encode(df1['text'].tolist())\nXt = embedder.encode(df2['text'].tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:45:12.539248Z","iopub.execute_input":"2025-01-16T17:45:12.539573Z","iopub.status.idle":"2025-01-16T17:45:30.128003Z","shell.execute_reply.started":"2025-01-16T17:45:12.539521Z","shell.execute_reply":"2025-01-16T17:45:30.127302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Xt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T15:27:09.333180Z","iopub.execute_input":"2025-01-15T15:27:09.333602Z","iopub.status.idle":"2025-01-15T15:27:09.342391Z","shell.execute_reply.started":"2025-01-15T15:27:09.333542Z","shell.execute_reply":"2025-01-15T15:27:09.341061Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\nmodel_name = \"xlm-roberta-base\"  # or \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\nmodel.eval() \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:45:52.255765Z","iopub.execute_input":"2025-01-16T17:45:52.256047Z","iopub.status.idle":"2025-01-16T17:45:54.492465Z","shell.execute_reply.started":"2025-01-16T17:45:52.256024Z","shell.execute_reply":"2025-01-16T17:45:54.491549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef batch_embed(texts, tokenizer, model, batch_size=16, max_length=128):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch_texts = texts[i:i + batch_size]\n        tokens = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=max_length,\n            return_tensors=\"pt\"\n        )\n        tokens = {key: value.to(device) for key, value in tokens.items()}\n        with torch.no_grad():\n            outputs = model(**tokens)\n            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        embeddings.append(batch_embeddings)\n    return np.vstack(embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:45:54.594218Z","iopub.execute_input":"2025-01-16T17:45:54.594629Z","iopub.status.idle":"2025-01-16T17:45:54.600770Z","shell.execute_reply.started":"2025-01-16T17:45:54.594578Z","shell.execute_reply":"2025-01-16T17:45:54.599735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_texts = df1[\"text\"].tolist()\ntest_texts = df2[\"text\"].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:45:56.755440Z","iopub.execute_input":"2025-01-16T17:45:56.755789Z","iopub.status.idle":"2025-01-16T17:45:56.761076Z","shell.execute_reply.started":"2025-01-16T17:45:56.755760Z","shell.execute_reply":"2025-01-16T17:45:56.760153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T19:04:57.162344Z","iopub.execute_input":"2025-01-17T19:04:57.162698Z","iopub.status.idle":"2025-01-17T19:04:57.167117Z","shell.execute_reply.started":"2025-01-17T19:04:57.162671Z","shell.execute_reply":"2025-01-17T19:04:57.166081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_train_features = batch_embed(train_texts, tokenizer, model)\nX_test_features = batch_embed(test_texts, tokenizer, model)\nX_train_features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:45:58.913381Z","iopub.execute_input":"2025-01-16T17:45:58.913715Z","iopub.status.idle":"2025-01-16T17:46:44.511338Z","shell.execute_reply.started":"2025-01-16T17:45:58.913686Z","shell.execute_reply":"2025-01-16T17:46:44.510377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX1_train, X1_test, y1_train, y1_test = train_test_split(X_train_features, df1[['praise', 'complaint', 'demands', 'questions']], test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T17:47:40.557972Z","iopub.execute_input":"2025-01-16T17:47:40.558299Z","iopub.status.idle":"2025-01-16T17:47:40.568022Z","shell.execute_reply.started":"2025-01-16T17:47:40.558270Z","shell.execute_reply":"2025-01-16T17:47:40.567052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y1_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_loss_function(y_true, y_pred):\n    \"\"\"\n    Custom loss function for multilabel classification with XGBoost.\n    We combine Binary Cross-Entropy (BCE) with an entropy regularization term.\n    \"\"\"\n    # Sigmoid transformation to convert logits to probabilities\n    preds = 1.0 / (1.0 + np.exp(-y_pred))  # Apply sigmoid function\n    \n    # BCE loss: binary cross-entropy for each label\n    bce_loss = -y_true * np.log(preds + 1e-15) - (1 - y_true) * np.log(1 - preds + 1e-15)\n    bce_loss = np.sum(bce_loss, axis=1)  # Sum over all labels\n    \n    # Regularization term: Encourage diversity in predictions\n    # This is the entropy of the predicted probabilities for each instance\n    entropy_loss = -np.sum(preds * np.log(preds + 1e-15), axis=1)  # Entropy across labels\n    \n    # Total loss: BCE loss + entropy regularization\n    total_loss = np.mean(bce_loss + 0.1 * entropy_loss)  # 0.1 is a weight for regularization\n    \n    # Compute gradients and hessians (used by XGBoost)\n    grad = preds - y_true\n    hess = preds * (1 - preds)  # Sigmoid derivative\n    \n    return grad, hess","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:20:32.663509Z","iopub.execute_input":"2025-01-16T18:20:32.663896Z","iopub.status.idle":"2025-01-16T18:20:32.669261Z","shell.execute_reply.started":"2025-01-16T18:20:32.663865Z","shell.execute_reply":"2025-01-16T18:20:32.668301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\n# Prepare storage for predictions and scores\ny_pred = {}\n#auc_scores = {}\n\n# Train and evaluate for each label independently\nfor col in ['complaint','demands', 'praise','questions']:\n    # Prepare data for the current label\n    dtrain = xgb.DMatrix(X_train_features, label=df1[col])\n    dtest = xgb.DMatrix(X_test_features)\n    \n    # Define XGBoost parameters\n    params = {\n        'objective':custom_loss_function,  # Binary classification\n        'eval_metric': 'auc',           # AUC for evaluation\n        'learning_rate': 0.1,\n        'max_depth': 3,\n        'subsample': 0.8,\n        'lambda': 1,\n        'alpha': 1\n    }\n    \n    # Train the model\n    xgb_model= xgb.train(params, dtrain, num_boost_round=500)\n    \n    for col in ['complaint', 'demands', 'praise', 'questions']:\n        \n        dtest = xgb.DMatrix(X_test_features) \n        y_pred[col] =xgb_model.predict(dtest)\n       # preds = xgb_model.predict(dtest)\n        \n    \n    \n    #auc_scores[col] = roc_auc_score(y1_test[col], preds)\n\n    #print(\"AUC Scores:\", auc_scores)\n    output_df = pd.DataFrame({\n    'id': df2['id'],\n    'complaint': y_pred['complaint'],\n    'demands': y_pred['demands'],\n    'praise': y_pred['praise'],\n    'questions': y_pred['questions']\n})\n\n# Save the predictions to a CSV file\noutput_df.to_csv('prediction2.csv', index=False)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:21:06.348960Z","iopub.execute_input":"2025-01-16T18:21:06.349264Z","iopub.status.idle":"2025-01-16T18:21:06.417860Z","shell.execute_reply.started":"2025-01-16T18:21:06.349242Z","shell.execute_reply":"2025-01-16T18:21:06.416673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:15:51.947285Z","iopub.execute_input":"2025-01-16T18:15:51.947611Z","iopub.status.idle":"2025-01-16T18:15:51.953593Z","shell.execute_reply.started":"2025-01-16T18:15:51.947582Z","shell.execute_reply":"2025-01-16T18:15:51.952857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:15:56.951663Z","iopub.execute_input":"2025-01-16T18:15:56.951982Z","iopub.status.idle":"2025-01-16T18:15:56.962943Z","shell.execute_reply.started":"2025-01-16T18:15:56.951957Z","shell.execute_reply":"2025-01-16T18:15:56.961930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T18:00:17.182376Z","iopub.execute_input":"2025-01-16T18:00:17.182736Z","iopub.status.idle":"2025-01-16T18:00:17.188499Z","shell.execute_reply.started":"2025-01-16T18:00:17.182704Z","shell.execute_reply":"2025-01-16T18:00:17.187656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_predd = {}\n\n# Process and predict for each label\nfor col in ['complaint', 'demands', 'praise', 'questions']:\n    # Prepare the test data as XGBoost DMatrix\n    model1 = xgb.Booster()\n    model1.load_model(f\"/kaggle/working/{col}_xgb_model.json\")\n    dtestt = xgb.DMatrix(X_test_features) \n    y_predd[col] =model1.predict(dtestt)\n\n# Combine predictions into a DataFrame\noutput_df = pd.DataFrame({\n    'id': df2['id'],\n    'complaint': y_predd['complaint'],\n    'demands': y_predd['demands'],\n    'praise': y_predd['praise'],\n    'questions': y_predd['questions']\n})\n\n# Save the predictions to a CSV file\noutput_df.to_csv('prediction2.csv', index=False)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:34:42.290873Z","iopub.execute_input":"2025-01-15T19:34:42.291211Z","iopub.status.idle":"2025-01-15T19:34:42.607730Z","shell.execute_reply.started":"2025-01-15T19:34:42.291186Z","shell.execute_reply":"2025-01-15T19:34:42.606867Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model1.get_dump()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:44:16.416692Z","iopub.execute_input":"2025-01-15T19:44:16.417054Z","iopub.status.idle":"2025-01-15T19:44:16.478909Z","shell.execute_reply.started":"2025-01-15T19:44:16.417028Z","shell.execute_reply":"2025-01-15T19:44:16.478031Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d=pd.read_csv(\"/kaggle/working/predictions.csv\")\nd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T19:34:57.540015Z","iopub.execute_input":"2025-01-15T19:34:57.540339Z","iopub.status.idle":"2025-01-15T19:34:57.557326Z","shell.execute_reply.started":"2025-01-15T19:34:57.540317Z","shell.execute_reply":"2025-01-15T19:34:57.556514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_predd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T18:33:15.584378Z","iopub.execute_input":"2025-01-15T18:33:15.584760Z","iopub.status.idle":"2025-01-15T18:33:15.591188Z","shell.execute_reply.started":"2025-01-15T18:33:15.584714Z","shell.execute_reply":"2025-01-15T18:33:15.590234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers datasets torch scikit-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T10:11:07.479975Z","iopub.execute_input":"2025-01-17T10:11:07.480312Z","iopub.status.idle":"2025-01-17T10:11:10.713036Z","shell.execute_reply.started":"2025-01-17T10:11:07.480282Z","shell.execute_reply":"2025-01-17T10:11:10.711942Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DistilBertTokenizer\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n\n\ndef tokenize_data(texts, max_length=128):\n    return tokenizer(\n        texts.tolist(),\n        padding='max_length',\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n\n\n#X_train, X_test, y_train, y_test = train_test_split(\n #   df1['text'], df1[['complaint', 'demands', 'praise', 'questions']],\n  #  test_size=0.2,\n   # random_state=42\n#)\n\n\ntrain_tokens = tokenize_data(df1['text'])\ntest_tokens = tokenize_data(df1['text'])\n\ny_train_tensor = torch.tensor(df1[['complaint', 'demands', 'praise', 'questions']].values, dtype=torch.float32)\n#y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n#y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:21:33.814715Z","iopub.execute_input":"2025-01-17T17:21:33.815045Z","iopub.status.idle":"2025-01-17T17:21:43.045049Z","shell.execute_reply.started":"2025-01-17T17:21:33.815019Z","shell.execute_reply":"2025-01-17T17:21:43.044283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import DistilBertModel\nimport torch.nn as nn\n\nclass DistilBERTMultiLabel(nn.Module):\n    def __init__(self, num_labels):\n        super(DistilBERTMultiLabel, self).__init__()\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-multilingual-cased')\n        self.classifier = nn.Sequential(\n            nn.Linear(self.distilbert.config.hidden_size, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_labels),\n            nn.Sigmoid()  \n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n        logits = self.classifier(hidden_state)\n        return logits\n\n\nmodel = DistilBERTMultiLabel(num_labels=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:21:43.046040Z","iopub.execute_input":"2025-01-17T17:21:43.046271Z","iopub.status.idle":"2025-01-17T17:21:46.178385Z","shell.execute_reply.started":"2025-01-17T17:21:43.046251Z","shell.execute_reply":"2025-01-17T17:21:46.177747Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AdamW\nfrom torch.optim import lr_scheduler\n\n# Prepare data loaders\ntrain_data = TensorDataset(\n    train_tokens['input_ids'], train_tokens['attention_mask'], y_train_tensor\n)\ntest_data = TensorDataset(\n    test_tokens['input_ids'], test_tokens['attention_mask']\n)\n\ntrain_loader = DataLoader(train_data, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=16)\n\n# Optimizer and Scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5,weight_decay=0.01)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.2)\n\n# Loss function\ncriterion = nn.BCELoss()  # Binary cross-entropy for multi-label\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:22:15.286821Z","iopub.execute_input":"2025-01-17T17:22:15.287198Z","iopub.status.idle":"2025-01-17T17:22:15.313373Z","shell.execute_reply.started":"2025-01-17T17:22:15.287164Z","shell.execute_reply":"2025-01-17T17:22:15.312154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Input IDs size: {train_tokens['input_ids'].size()}\")\nprint(f\"Attention Mask size: {train_tokens['attention_mask'].size()}\")\nprint(f\"Labels size: {y_train_tensor.size()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:22:04.936638Z","iopub.execute_input":"2025-01-17T17:22:04.937041Z","iopub.status.idle":"2025-01-17T17:22:05.330261Z","shell.execute_reply.started":"2025-01-17T17:22:04.937010Z","shell.execute_reply":"2025-01-17T17:22:05.329401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:22:12.037845Z","iopub.execute_input":"2025-01-17T17:22:12.038131Z","iopub.status.idle":"2025-01-17T17:22:12.041792Z","shell.execute_reply.started":"2025-01-17T17:22:12.038111Z","shell.execute_reply":"2025-01-17T17:22:12.040912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n    \n    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader)}\")\n    scheduler.step()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T16:09:51.427457Z","iopub.execute_input":"2025-01-17T16:09:51.427995Z","iopub.status.idle":"2025-01-17T16:14:30.256916Z","shell.execute_reply.started":"2025-01-17T16:09:51.427966Z","shell.execute_reply":"2025-01-17T16:14:30.255854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d=pd.read_csv(\"/kaggle/working/submission3.csv\")\nd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T13:23:46.502277Z","iopub.execute_input":"2025-01-16T13:23:46.502603Z","iopub.status.idle":"2025-01-16T13:23:46.517148Z","shell.execute_reply.started":"2025-01-16T13:23:46.502574Z","shell.execute_reply":"2025-01-16T13:23:46.516394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n        outputs = model(input_ids, attention_mask)\n        all_preds.append(outputs.cpu())\n        all_labels.append(labels.cpu())\n\nall_preds = torch.cat(all_preds).numpy()\nall_labels = torch.cat(all_labels).numpy()\n\nauc_scores = {}\nfor i, col in enumerate(['complaint', 'demands', 'praise', 'questions']):\n    auc_scores[col] = roc_auc_score(all_labels[:, i], all_preds[:, i])\n\nprint(\"AUC Scores:\", auc_scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T15:17:06.398013Z","iopub.execute_input":"2025-01-17T15:17:06.398412Z","iopub.status.idle":"2025-01-17T15:17:06.422809Z","shell.execute_reply.started":"2025-01-17T15:17:06.398378Z","shell.execute_reply":"2025-01-17T15:17:06.421300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\n\n# Prepare test dataset and dataloader\ntest_dataset = TensorDataset(test_tokens['input_ids'], test_tokens['attention_mask'])\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # Use a small batch size\n\n# Initialize device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Predict in batches\ntest_preds = []\n\nmodel.eval()  # Set model to evaluation mode\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids, attention_mask = [x.to(device) for x in batch]\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        test_preds.append(outputs.cpu().numpy())\n\n# Concatenate predictions from all batches\ntest_preds = np.vstack(test_preds)  # Convert list of arrays to a single array\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': df2['id'],\n    'complaint': test_preds[:, 0],\n    'demands': test_preds[:, 1],\n    'praise': test_preds[:, 2],\n    'questions': test_preds[:, 3]\n})\n\nsubmission.to_csv('submission4.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T16:14:38.417710Z","iopub.execute_input":"2025-01-17T16:14:38.418038Z","iopub.status.idle":"2025-01-17T16:14:56.665122Z","shell.execute_reply.started":"2025-01-17T16:14:38.418013Z","shell.execute_reply":"2025-01-17T16:14:56.664048Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on test data\n#test_token = tokenize_data(df2['text'])\ntest_inputs = {\n    'input_ids': test_tokens['input_ids'].to(device),\n    'attention_mask': test_tokens['attention_mask'].to(device)\n}\n\n# Move model to device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nwith torch.no_grad():\n    test_preds = model(**test_inputs).cpu().numpy()\n\n# Create submission file\nsubmission = pd.DataFrame({\n    'id': df2['id'],\n    'complaint': test_preds[:, 0],\n    'demands': test_preds[:, 1],\n    'praise': test_preds[:, 2],\n    'questions': test_preds[:, 3]\n})\n\nsubmission.to_csv('submission4.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:20:07.198029Z","iopub.execute_input":"2025-01-17T17:20:07.198323Z","iopub.status.idle":"2025-01-17T17:20:07.216497Z","shell.execute_reply.started":"2025-01-17T17:20:07.198299Z","shell.execute_reply":"2025-01-17T17:20:07.215333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:19:15.559274Z","iopub.execute_input":"2025-01-17T17:19:15.559618Z","iopub.status.idle":"2025-01-17T17:19:15.564036Z","shell.execute_reply.started":"2025-01-17T17:19:15.559588Z","shell.execute_reply":"2025-01-17T17:19:15.562775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dd=pd.read_csv(\"/kaggle/working/submission.csv\")\ndd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-14T12:39:01.583389Z","iopub.execute_input":"2025-01-14T12:39:01.583726Z","iopub.status.idle":"2025-01-14T12:39:01.601244Z","shell.execute_reply.started":"2025-01-14T12:39:01.583698Z","shell.execute_reply":"2025-01-14T12:39:01.600368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import XLMRobertaTokenizer, BertTokenizer\n\n# Load tokenizers\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n#mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\ndef preprocess_text(texts, tokenizer, max_len=128):\n    encodings = tokenizer.batch_encode_plus(\n        texts.tolist(),\n        max_length=max_len,\n        padding=\"max_length\",\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    return encodings\n\nxlm_encodings = preprocess_text(df1['text'], xlm_tokenizer)\n#mbert_encodings = preprocess_text(df1['text'], mbert_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:14:56.617705Z","iopub.execute_input":"2025-01-17T17:14:56.618026Z","iopub.status.idle":"2025-01-17T17:15:00.689414Z","shell.execute_reply.started":"2025-01-17T17:14:56.617998Z","shell.execute_reply":"2025-01-17T17:15:00.688739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\n\n\nclass TransformerClassifier(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super(TransformerClassifier, self).__init__()\n        self.transformer = AutoModel.from_pretrained(model_name)\n        \n        # Classifier head\n        self.classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(128, num_labels),\n            nn.Sigmoid()  # Normalize probabilities across all labels\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = outputs.last_hidden_state[:, 0, :]  # CLS token\n        logits = self.classifier(hidden_state)\n        return logits\n\n\n# Initialize models\nxlm_model = TransformerClassifier(\"xlm-roberta-base\", num_labels=4)\n#mbert_model = TransformerClassifier(\"bert-base-multilingual-cased\", num_labels=4)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:15:31.687039Z","iopub.execute_input":"2025-01-17T17:15:31.687327Z","iopub.status.idle":"2025-01-17T17:15:31.830529Z","shell.execute_reply.started":"2025-01-17T17:15:31.687304Z","shell.execute_reply":"2025-01-17T17:15:31.829882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\n\nclass TextDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\n# Define targets\ny = df1[['complaint', 'demands', 'praise', 'questions']].values\n\n# Prepare datasets and loaders\ntrain_dataset = TextDataset(xlm_encodings, y)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:15:34.686081Z","iopub.execute_input":"2025-01-17T17:15:34.686386Z","iopub.status.idle":"2025-01-17T17:15:34.693639Z","shell.execute_reply.started":"2025-01-17T17:15:34.686360Z","shell.execute_reply":"2025-01-17T17:15:34.692496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom transformers import AdamW\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, train_loader, epochs=5, lr=1e-3):\n    model.to(device)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = AdamW(model.parameters(), lr=lr,weight_decay=0.01)\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        all_labels, all_preds = [], []\n        \n        for batch in train_loader:\n            optimizer.zero_grad()\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            all_labels.append(labels.cpu().numpy())\n            all_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n        \n        # Calculate AUC-ROC\n        all_labels = np.vstack(all_labels)\n        all_preds = np.vstack(all_preds)\n        auc_scores = {col: roc_auc_score(all_labels[:, i], all_preds[:, i]) \n                      for i, col in enumerate(['complaint', 'demands', 'praise', 'questions'])}\n        \n        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, AUC-ROC: {auc_scores}\")\n\n# Train XLM-RoBERTa\ntrain_model(xlm_model, train_loader)\n\n# Train mBERT\n#train_model(mbert_model, train_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T17:15:35.265841Z","iopub.execute_input":"2025-01-17T17:15:35.266113Z","iopub.status.idle":"2025-01-17T17:17:33.268924Z","shell.execute_reply.started":"2025-01-17T17:15:35.266093Z","shell.execute_reply":"2025-01-17T17:17:33.267760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Collect predictions for meta-classifier\nxlm_preds = xlm_model(xlm_encodings['input_ids'].to(device), xlm_encodings['attention_mask'].to(device)).detach().cpu().numpy()\n#mbert_preds = mbert_model(mbert_encodings['input_ids'].to(device), mbert_encodings['attention_mask'].to(device)).detach().cpu().numpy()\n\n#meta_features = np.hstack([xlm_preds, mbert_preds])\n#meta_clf = LogisticRegression()\n#meta_clf.fit(meta_features, y)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test_df = pd.read_csv(\"test.csv\")\ntest_encodings = preprocess_text(df2['text'], xlm_tokenizer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"xlm_test_preds = xlm_model(test_encodings['input_ids'].to(device), test_encodings['attention_mask'].to(device)).detach().cpu().numpy()\n#mbert_test_preds = mbert_model(test_encodings['input_ids'].to(device), test_encodings['attention_mask'].to(device)).detach().cpu().numpy()\n\n# Combine predictions\n#test_meta_features = np.hstack([xlm_test_preds, mbert_test_preds])\n#final_preds = meta_clf.predict_proba(test_meta_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"id\": df2[\"id\"],\n    \"complaint\": final_preds[:, 0],\n    \"demands\": final_preds[:, 1],\n    \"praise\": final_preds[:, 2],\n    \"questions\": final_preds[:, 3]\n})\nsubmission.to_csv(\"submission.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:33.813944Z","iopub.execute_input":"2025-01-20T10:29:33.814281Z","iopub.status.idle":"2025-01-20T10:29:41.761680Z","shell.execute_reply.started":"2025-01-20T10:29:33.814254Z","shell.execute_reply":"2025-01-20T10:29:41.760767Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_texts = df1['text']  \ntrain_labels = df1[['complaint', 'demands', 'praise', 'questions']].values\n\ntest_texts = df2['text']  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:41.763026Z","iopub.execute_input":"2025-01-20T10:29:41.763531Z","iopub.status.idle":"2025-01-20T10:29:41.770043Z","shell.execute_reply.started":"2025-01-20T10:29:41.763504Z","shell.execute_reply":"2025-01-20T10:29:41.768822Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n\n\n# Tokenize data\ndef tokenize_texts(texts, max_length=128):\n    return tokenizer(\n        list(texts),\n        padding=True,\n        truncation=True,\n        max_length=max_length,\n        return_tensors=\"pt\"\n    )\n\ntrain_encodings = tokenize_texts(train_texts)\n\ntest_encodings = tokenize_texts(test_texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:41.772111Z","iopub.execute_input":"2025-01-20T10:29:41.772461Z","iopub.status.idle":"2025-01-20T10:29:48.496836Z","shell.execute_reply.started":"2025-01-20T10:29:41.772428Z","shell.execute_reply":"2025-01-20T10:29:48.496150Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bd04f9ccc11436aa80e25b0e9a38ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8450c494c4e14a33856778e5af3ed2cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd0128a30f6642e8962478ccd97640e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"314b009e5f5f47e1b08df0ed981a45f4"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Dataset class\nclass MultilabelDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n        return item\n\n# Create datasets\ntrain_dataset = MultilabelDataset(train_encodings, train_labels)\ntest_dataset = MultilabelDataset(test_encodings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:48.497992Z","iopub.execute_input":"2025-01-20T10:29:48.498463Z","iopub.status.idle":"2025-01-20T10:29:48.503613Z","shell.execute_reply.started":"2025-01-20T10:29:48.498438Z","shell.execute_reply":"2025-01-20T10:29:48.502904Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:48.504445Z","iopub.execute_input":"2025-01-20T10:29:48.504677Z","iopub.status.idle":"2025-01-20T10:29:48.523999Z","shell.execute_reply.started":"2025-01-20T10:29:48.504658Z","shell.execute_reply":"2025-01-20T10:29:48.523356Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from torch.nn import BCEWithLogitsLoss\n\ndef train_model(model, dataloader, optimizer, epochs=9):\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0\n        for batch in dataloader:\n            inputs = {key: val.to(\"cuda\") for key, val in batch.items() if key != \"labels\"}\n            labels = batch[\"labels\"].to(\"cuda\")\n\n            optimizer.zero_grad()\n            outputs = model(**inputs).logits\n            loss = BCEWithLogitsLoss()(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:48.524774Z","iopub.execute_input":"2025-01-20T10:29:48.524971Z","iopub.status.idle":"2025-01-20T10:29:48.540408Z","shell.execute_reply.started":"2025-01-20T10:29:48.524945Z","shell.execute_reply":"2025-01-20T10:29:48.539830Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-large\", num_labels=4, problem_type=\"multi_label_classification\").to(\"cuda\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:29:48.541224Z","iopub.execute_input":"2025-01-20T10:29:48.541481Z","iopub.status.idle":"2025-01-20T10:30:01.048068Z","shell.execute_reply.started":"2025-01-20T10:29:48.541452Z","shell.execute_reply":"2025-01-20T10:30:01.047037Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b58cb61c203468fb12deb8f58abb197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2fbe204cca4e61be37f6faee167708"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=1.5e-5)\n\n# Add warmup and decay\n#num_training_steps = len(train_loader) * 6\n#scheduler = get_linear_schedule_with_warmup(\n #   optimizer,\n  #  num_warmup_steps=num_training_steps * 0.05,  # 10% warmup\n   # num_training_steps=num_training_steps\n#)\n\n\n# Train the model\ntrain_model(model, train_loader, optimizer, epochs=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T10:30:09.879660Z","iopub.execute_input":"2025-01-20T10:30:09.879975Z","iopub.status.idle":"2025-01-20T11:16:28.782691Z","shell.execute_reply.started":"2025-01-20T10:30:09.879948Z","shell.execute_reply":"2025-01-20T11:16:28.781836Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n<ipython-input-7-2acccb9edd82>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/8, Loss: 0.4022\nEpoch 2/8, Loss: 0.2412\nEpoch 3/8, Loss: 0.1707\nEpoch 4/8, Loss: 0.1218\nEpoch 5/8, Loss: 0.0940\nEpoch 6/8, Loss: 0.0679\nEpoch 7/8, Loss: 0.0495\nEpoch 8/8, Loss: 0.0392\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def predict(model, dataloader):\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for batch in dataloader:\n            inputs = {key: val.to(\"cuda\") for key, val in batch.items()}\n            outputs = model(**inputs).logits\n            preds = torch.sigmoid(outputs).cpu().numpy()\n            predictions.extend(preds)\n    return np.array(predictions)\n\n# Get predictions\ntest_predictions = predict(model, test_loader)\n\n# Normalize predictions to sum to 1\nnormalized_predictions = test_predictions / test_predictions.sum(axis=1, keepdims=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T11:16:28.784155Z","iopub.execute_input":"2025-01-20T11:16:28.784609Z","iopub.status.idle":"2025-01-20T11:17:23.147220Z","shell.execute_reply.started":"2025-01-20T11:16:28.784586Z","shell.execute_reply":"2025-01-20T11:17:23.146546Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-2acccb9edd82>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Create a DataFrame for submission\nsubmission = pd.DataFrame({\n    'id': df2['id'],  # Add the test IDs\n    'complaint': normalized_predictions[:, 0],\n    'demands': normalized_predictions[:, 1],\n    'praise': normalized_predictions[:, 2],\n    'questions': normalized_predictions[:, 3],\n})\n\n# Save to CSV\nsubmission.to_csv('submission11.csv', index=False)\nprint(\"Submission file saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T11:17:23.148494Z","iopub.execute_input":"2025-01-20T11:17:23.148828Z","iopub.status.idle":"2025-01-20T11:17:23.177299Z","shell.execute_reply.started":"2025-01-20T11:17:23.148795Z","shell.execute_reply":"2025-01-20T11:17:23.176705Z"}},"outputs":[{"name":"stdout","text":"Submission file saved as submission.csv\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"d=pd.read_csv(\"/kaggle/working/submission4.csv\")\nd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T11:11:28.004592Z","iopub.execute_input":"2025-01-18T11:11:28.004981Z","iopub.status.idle":"2025-01-18T11:11:28.020719Z","shell.execute_reply.started":"2025-01-18T11:11:28.004954Z","shell.execute_reply":"2025-01-18T11:11:28.019886Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"                 id  complaint   demands    praise  questions\n0     1041016773991   0.001530  0.990229  0.006878   0.001363\n1      109362481297   0.000938  0.001041  0.997162   0.000859\n2      985019053532   0.996231  0.001143  0.001784   0.000842\n3      436629695381   0.996317  0.001138  0.000639   0.001907\n4      585196067684   0.002049  0.991497  0.002880   0.003574\n...             ...        ...       ...       ...        ...\n1995   148770317688   0.996688  0.001272  0.001087   0.000953\n1996   288582831883   0.000644  0.002317  0.996471   0.000568\n1997   285086247879   0.000942  0.000979  0.997126   0.000953\n1998   114758927004   0.006517  0.002385  0.002276   0.988822\n1999   417819392797   0.001338  0.000815  0.997063   0.000783\n\n[2000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041016773991</td>\n      <td>0.001530</td>\n      <td>0.990229</td>\n      <td>0.006878</td>\n      <td>0.001363</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>109362481297</td>\n      <td>0.000938</td>\n      <td>0.001041</td>\n      <td>0.997162</td>\n      <td>0.000859</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>985019053532</td>\n      <td>0.996231</td>\n      <td>0.001143</td>\n      <td>0.001784</td>\n      <td>0.000842</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>436629695381</td>\n      <td>0.996317</td>\n      <td>0.001138</td>\n      <td>0.000639</td>\n      <td>0.001907</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>585196067684</td>\n      <td>0.002049</td>\n      <td>0.991497</td>\n      <td>0.002880</td>\n      <td>0.003574</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>148770317688</td>\n      <td>0.996688</td>\n      <td>0.001272</td>\n      <td>0.001087</td>\n      <td>0.000953</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>288582831883</td>\n      <td>0.000644</td>\n      <td>0.002317</td>\n      <td>0.996471</td>\n      <td>0.000568</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>285086247879</td>\n      <td>0.000942</td>\n      <td>0.000979</td>\n      <td>0.997126</td>\n      <td>0.000953</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>114758927004</td>\n      <td>0.006517</td>\n      <td>0.002385</td>\n      <td>0.002276</td>\n      <td>0.988822</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>417819392797</td>\n      <td>0.001338</td>\n      <td>0.000815</td>\n      <td>0.997063</td>\n      <td>0.000783</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-20T11:22:35.856551Z","iopub.execute_input":"2025-01-20T11:22:35.856847Z","iopub.status.idle":"2025-01-20T11:22:35.875850Z","shell.execute_reply.started":"2025-01-20T11:22:35.856824Z","shell.execute_reply":"2025-01-20T11:22:35.874980Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                 id  complaint   demands    praise  questions\n0     1041016773991   0.006667  0.985395  0.004397   0.003541\n1      109362481297   0.002438  0.001627  0.995204   0.000731\n2      985019053532   0.903841  0.001412  0.094509   0.000238\n3      436629695381   0.991679  0.000561  0.006366   0.001394\n4      585196067684   0.004007  0.983747  0.005093   0.007153\n...             ...        ...       ...       ...        ...\n1995   148770317688   0.990831  0.000781  0.007598   0.000790\n1996   288582831883   0.002646  0.003159  0.993678   0.000517\n1997   285086247879   0.002626  0.001765  0.994819   0.000790\n1998   114758927004   0.971603  0.000917  0.006551   0.020929\n1999   417819392797   0.002189  0.001785  0.995246   0.000780\n\n[2000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041016773991</td>\n      <td>0.006667</td>\n      <td>0.985395</td>\n      <td>0.004397</td>\n      <td>0.003541</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>109362481297</td>\n      <td>0.002438</td>\n      <td>0.001627</td>\n      <td>0.995204</td>\n      <td>0.000731</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>985019053532</td>\n      <td>0.903841</td>\n      <td>0.001412</td>\n      <td>0.094509</td>\n      <td>0.000238</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>436629695381</td>\n      <td>0.991679</td>\n      <td>0.000561</td>\n      <td>0.006366</td>\n      <td>0.001394</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>585196067684</td>\n      <td>0.004007</td>\n      <td>0.983747</td>\n      <td>0.005093</td>\n      <td>0.007153</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>148770317688</td>\n      <td>0.990831</td>\n      <td>0.000781</td>\n      <td>0.007598</td>\n      <td>0.000790</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>288582831883</td>\n      <td>0.002646</td>\n      <td>0.003159</td>\n      <td>0.993678</td>\n      <td>0.000517</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>285086247879</td>\n      <td>0.002626</td>\n      <td>0.001765</td>\n      <td>0.994819</td>\n      <td>0.000790</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>114758927004</td>\n      <td>0.971603</td>\n      <td>0.000917</td>\n      <td>0.006551</td>\n      <td>0.020929</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>417819392797</td>\n      <td>0.002189</td>\n      <td>0.001785</td>\n      <td>0.995246</td>\n      <td>0.000780</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"ddd=pd.read_csv(\"/kaggle/input/submission1-5/submission1.5.csv\")\nddd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T09:54:44.608881Z","iopub.execute_input":"2025-01-18T09:54:44.609157Z","iopub.status.idle":"2025-01-18T09:54:44.632594Z","shell.execute_reply.started":"2025-01-18T09:54:44.609135Z","shell.execute_reply":"2025-01-18T09:54:44.631901Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                 id  complaint   demands    praise  questions\n0     1041016773991   0.028546  0.920891  0.032976   0.017587\n1      109362481297   0.007149  0.008302  0.975564   0.008985\n2      985019053532   0.950405  0.011815  0.035136   0.002644\n3      436629695381   0.968888  0.011756  0.011268   0.008087\n4      585196067684   0.030264  0.913229  0.029869   0.026638\n...             ...        ...       ...       ...        ...\n1995   148770317688   0.966109  0.013035  0.015271   0.005585\n1996   288582831883   0.006159  0.782621  0.202552   0.008667\n1997   285086247879   0.014307  0.007581  0.974061   0.004051\n1998   114758927004   0.175441  0.007197  0.012809   0.804553\n1999   417819392797   0.024599  0.007640  0.965110   0.002652\n\n[2000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041016773991</td>\n      <td>0.028546</td>\n      <td>0.920891</td>\n      <td>0.032976</td>\n      <td>0.017587</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>109362481297</td>\n      <td>0.007149</td>\n      <td>0.008302</td>\n      <td>0.975564</td>\n      <td>0.008985</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>985019053532</td>\n      <td>0.950405</td>\n      <td>0.011815</td>\n      <td>0.035136</td>\n      <td>0.002644</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>436629695381</td>\n      <td>0.968888</td>\n      <td>0.011756</td>\n      <td>0.011268</td>\n      <td>0.008087</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>585196067684</td>\n      <td>0.030264</td>\n      <td>0.913229</td>\n      <td>0.029869</td>\n      <td>0.026638</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>148770317688</td>\n      <td>0.966109</td>\n      <td>0.013035</td>\n      <td>0.015271</td>\n      <td>0.005585</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>288582831883</td>\n      <td>0.006159</td>\n      <td>0.782621</td>\n      <td>0.202552</td>\n      <td>0.008667</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>285086247879</td>\n      <td>0.014307</td>\n      <td>0.007581</td>\n      <td>0.974061</td>\n      <td>0.004051</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>114758927004</td>\n      <td>0.175441</td>\n      <td>0.007197</td>\n      <td>0.012809</td>\n      <td>0.804553</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>417819392797</td>\n      <td>0.024599</td>\n      <td>0.007640</td>\n      <td>0.965110</td>\n      <td>0.002652</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"d4=pd.read_csv('/kaggle/working/submission4.csv')\nd4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-18T10:35:21.944991Z","iopub.execute_input":"2025-01-18T10:35:21.945294Z","iopub.status.idle":"2025-01-18T10:35:21.962561Z","shell.execute_reply.started":"2025-01-18T10:35:21.945272Z","shell.execute_reply":"2025-01-18T10:35:21.961421Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                 id  complaint   demands    praise  questions\n0     1041016773991   0.001530  0.990229  0.006878   0.001363\n1      109362481297   0.000938  0.001041  0.997162   0.000859\n2      985019053532   0.996231  0.001143  0.001784   0.000842\n3      436629695381   0.996317  0.001138  0.000639   0.001907\n4      585196067684   0.002049  0.991497  0.002880   0.003574\n...             ...        ...       ...       ...        ...\n1995   148770317688   0.996688  0.001272  0.001087   0.000953\n1996   288582831883   0.000644  0.002317  0.996471   0.000568\n1997   285086247879   0.000942  0.000979  0.997126   0.000953\n1998   114758927004   0.006517  0.002385  0.002276   0.988822\n1999   417819392797   0.001338  0.000815  0.997063   0.000783\n\n[2000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>complaint</th>\n      <th>demands</th>\n      <th>praise</th>\n      <th>questions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1041016773991</td>\n      <td>0.001530</td>\n      <td>0.990229</td>\n      <td>0.006878</td>\n      <td>0.001363</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>109362481297</td>\n      <td>0.000938</td>\n      <td>0.001041</td>\n      <td>0.997162</td>\n      <td>0.000859</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>985019053532</td>\n      <td>0.996231</td>\n      <td>0.001143</td>\n      <td>0.001784</td>\n      <td>0.000842</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>436629695381</td>\n      <td>0.996317</td>\n      <td>0.001138</td>\n      <td>0.000639</td>\n      <td>0.001907</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>585196067684</td>\n      <td>0.002049</td>\n      <td>0.991497</td>\n      <td>0.002880</td>\n      <td>0.003574</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>148770317688</td>\n      <td>0.996688</td>\n      <td>0.001272</td>\n      <td>0.001087</td>\n      <td>0.000953</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>288582831883</td>\n      <td>0.000644</td>\n      <td>0.002317</td>\n      <td>0.996471</td>\n      <td>0.000568</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>285086247879</td>\n      <td>0.000942</td>\n      <td>0.000979</td>\n      <td>0.997126</td>\n      <td>0.000953</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>114758927004</td>\n      <td>0.006517</td>\n      <td>0.002385</td>\n      <td>0.002276</td>\n      <td>0.988822</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>417819392797</td>\n      <td>0.001338</td>\n      <td>0.000815</td>\n      <td>0.997063</td>\n      <td>0.000783</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:56:09.052244Z","iopub.execute_input":"2025-01-19T13:56:09.052537Z","iopub.status.idle":"2025-01-19T13:56:09.068735Z","shell.execute_reply.started":"2025-01-19T13:56:09.052514Z","shell.execute_reply":"2025-01-19T13:56:09.067472Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-e983f374794d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"],"ename":"NameError","evalue":"name 'd' is not defined","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}