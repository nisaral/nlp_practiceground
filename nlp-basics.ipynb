{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP BASICS ","metadata":{}},{"cell_type":"markdown","source":"NLP Tutorial\nNLP - or Natural Language Processing - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.","metadata":{}},{"cell_type":"markdown","source":"We’ll then turn to a set of tasks collectively called text normalization, in which text\n**normalization**\nregular expressions play an important part.\nNormalizing text means converting it\nto a more convenient, standard form. For example, most of what we are going to\ndo with language relies on first separating out or **tokenizing** words from running\n**tokenization** text, the task of tokenization.\nEnglish words are often separated from each otherby whitespace, but whitespace is not always sufficient. New York and rock ’n’ rollare sometimes treated as large words despite the fact that they contain spaces, while sometimes we’ll need to separate I’m into the two words I and am. \nFor processing tweets or texts we’ll need to tokenize emoticons like :) or hashtags like #nlproc.","metadata":{}},{"cell_type":"markdown","source":"Another part of text normalization is lemmatization, the task of determining\nthat two words have the same root, despite their surface differences. For example,\nthe words sang, sung, and sings are forms of the verb sing. The word sing is the\ncommon lemma of these words, and a lemmatizer maps from all of these to sing.\nLemmatization is essential for processing morphologically complex languages like\nstemming Arabic. Stemming refers to a simpler version of lemmatization in which we mainly\njust strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like sentence\nsegmentation\nperiods or exclamation points.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-05T13:24:05.664841Z","iopub.execute_input":"2024-10-05T13:24:05.665294Z","iopub.status.idle":"2024-10-05T13:24:07.000920Z","shell.execute_reply.started":"2024-10-05T13:24:05.665253Z","shell.execute_reply":"2024-10-05T13:24:06.999158Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.003792Z","iopub.execute_input":"2024-10-05T13:24:07.004455Z","iopub.status.idle":"2024-10-05T13:24:07.011059Z","shell.execute_reply.started":"2024-10-05T13:24:07.004408Z","shell.execute_reply":"2024-10-05T13:24:07.009461Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\ntrain_df.tail(12)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.013222Z","iopub.execute_input":"2024-10-05T13:24:07.013764Z","iopub.status.idle":"2024-10-05T13:24:07.127004Z","shell.execute_reply.started":"2024-10-05T13:24:07.013707Z","shell.execute_reply":"2024-10-05T13:24:07.125801Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shuffle training dataframe\ntrain_df_shuffled = train_df.sample(frac=1, random_state=42) \ntrain_df_shuffled.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.129698Z","iopub.execute_input":"2024-10-05T13:24:07.130123Z","iopub.status.idle":"2024-10-05T13:24:07.155677Z","shell.execute_reply.started":"2024-10-05T13:24:07.130077Z","shell.execute_reply":"2024-10-05T13:24:07.154220Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"let's see the class distribution ","metadata":{}},{"cell_type":"code","source":"classdis=train_df.target\nclassdis.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.157747Z","iopub.execute_input":"2024-10-05T13:24:07.158418Z","iopub.status.idle":"2024-10-05T13:24:07.176471Z","shell.execute_reply.started":"2024-10-05T13:24:07.158372Z","shell.execute_reply":"2024-10-05T13:24:07.174733Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we got a good amount of distribution for both classes ","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.178551Z","iopub.execute_input":"2024-10-05T13:24:07.179023Z","iopub.status.idle":"2024-10-05T13:24:07.206965Z","shell.execute_reply.started":"2024-10-05T13:24:07.178978Z","shell.execute_reply":"2024-10-05T13:24:07.205478Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"lets return some characteristics of text length for all the rows using anonymous(lambda) fuction ","metadata":{}},{"cell_type":"code","source":"train_df[\"length\"]= train_df[\"text\"].apply(lambda x:len(x))\ntest_df[\"length\"]=test_df[\"text\"].apply(lambda x:len(x))\\\n\nprint(\"training data characteristics\")\nprint(train_df[\"length\"].describe())\n\nprint(\"testing data characteristics\")\nprint(test_df[\"length\"].describe())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.209148Z","iopub.execute_input":"2024-10-05T13:24:07.209567Z","iopub.status.idle":"2024-10-05T13:24:07.239609Z","shell.execute_reply.started":"2024-10-05T13:24:07.209524Z","shell.execute_reply":"2024-10-05T13:24:07.237746Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"let's look at some random training examples","metadata":{}},{"cell_type":"code","source":"print(f\"Text:{train_df['text'].tail(10)}\",f\"Target:{train_df['target'].tail(10)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.241656Z","iopub.execute_input":"2024-10-05T13:24:07.242132Z","iopub.status.idle":"2024-10-05T13:24:07.251056Z","shell.execute_reply.started":"2024-10-05T13:24:07.242087Z","shell.execute_reply":"2024-10-05T13:24:07.249442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**split data into training and validation set**\nso we can check our model performance while training on training set and as test set doesn't have labels thus we need to make a vallidation set\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n                                                                            train_df_shuffled[\"target\"].to_numpy(),\n                                                                            test_size=0.1, \n                                                                            random_state=42)\n   ","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:07.253512Z","iopub.execute_input":"2024-10-05T13:24:07.254078Z","iopub.status.idle":"2024-10-05T13:24:08.822925Z","shell.execute_reply.started":"2024-10-05T13:24:07.253992Z","shell.execute_reply":"2024-10-05T13:24:08.821417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_sentences),len(val_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:08.827227Z","iopub.execute_input":"2024-10-05T13:24:08.827786Z","iopub.status.idle":"2024-10-05T13:24:08.836884Z","shell.execute_reply.started":"2024-10-05T13:24:08.827745Z","shell.execute_reply":"2024-10-05T13:24:08.835274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data preprocessing- Converting text into numbers\nWhen dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to numbers.\n\nThere are a few ways to do this, namely:\n\nTokenziation - direct mapping of token (a token could be a word or a character) to number\nEmbedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)","metadata":{}},{"cell_type":"markdown","source":"**Text vectorization**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\n\n# Initialize the TextVectorization layer with corrected parameters\ntext_vectorizer = TextVectorization(\n    max_tokens=None,  # Maximum size of the vocabulary (None means no limit)\n    standardize=\"lower_and_strip_punctuation\",  # Standardization method as a string\n    split=\"whitespace\",  # Tokenize based on whitespace\n    ngrams=None,  # No n-gram creation\n    output_mode=\"int\",  # Map tokens to integers\n    output_sequence_length=None  # Length of the output sequences (None means variable length)\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:08.839211Z","iopub.execute_input":"2024-10-05T13:24:08.840148Z","iopub.status.idle":"2024-10-05T13:24:25.127200Z","shell.execute_reply.started":"2024-10-05T13:24:08.840088Z","shell.execute_reply":"2024-10-05T13:24:25.125493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It's often beneficial to set this to a specific number (e.g., max_tokens=10000) to limit the vocabulary size, which can help with model performance and prevent overfitting.","metadata":{}},{"cell_type":"code","source":"# Setup text vectorization variables\nmax_vocab_length = 10000 # max number of words to have in our vocabulary\nmax_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does a model see?)\n\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                    output_mode=\"int\",\n                                    output_sequence_length=max_length)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:25.129612Z","iopub.execute_input":"2024-10-05T13:24:25.130690Z","iopub.status.idle":"2024-10-05T13:24:25.146300Z","shell.execute_reply.started":"2024-10-05T13:24:25.130623Z","shell.execute_reply":"2024-10-05T13:24:25.144628Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fit the text vectorizer instance to the training data using the adapt() method\ntext_vectorizer.adapt(train_sentences)","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:25.148427Z","iopub.execute_input":"2024-10-05T13:24:25.148850Z","iopub.status.idle":"2024-10-05T13:24:25.332815Z","shell.execute_reply.started":"2024-10-05T13:24:25.148809Z","shell.execute_reply":"2024-10-05T13:24:25.331554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n# Choose a random sentence every time u run from the training dataset and tokenize it\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n        \\n\\nVectorized version:\")\ntext_vectorizer([random_sentence])","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:25.334517Z","iopub.execute_input":"2024-10-05T13:24:25.334950Z","iopub.status.idle":"2024-10-05T13:24:25.386325Z","shell.execute_reply.started":"2024-10-05T13:24:25.334902Z","shell.execute_reply":"2024-10-05T13:24:25.385049Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the unique words in the vocabulary\nwords_in_vocab = text_vectorizer.get_vocabulary()\ntop_10_words = words_in_vocab[:10] # the most common words in the vocab\nbottom_10_words = words_in_vocab[-10:] # the least common words in the vocab\nprint(f\"Most common words in vocab: {top_10_words}\")\nprint(f\"Least common words in vocab: {bottom_10_words}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:24:25.387879Z","iopub.execute_input":"2024-10-05T13:24:25.388366Z","iopub.status.idle":"2024-10-05T13:24:25.447827Z","shell.execute_reply.started":"2024-10-05T13:24:25.388324Z","shell.execute_reply":"2024-10-05T13:24:25.446361Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Creating an Embedding using an Embedding Layer**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers \n\nembedding = layers.Embedding(input_dim=max_vocab_length, # set the input shape\n                             output_dim=128, # set the size of the embedding vector\n                             embeddings_initializer=\"uniform\", # default, initialize embedding vectors randomly\n                             input_length=max_length # how long is each input\n                             )\n\nembedding","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:26:13.361462Z","iopub.execute_input":"2024-10-05T13:26:13.361894Z","iopub.status.idle":"2024-10-05T13:26:13.375129Z","shell.execute_reply.started":"2024-10-05T13:26:13.361852Z","shell.execute_reply":"2024-10-05T13:26:13.373708Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a random sentence from training set\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n        \\n\\nEmbedded version:\")\n\n# Embed the random sentence (turn it into numerical representation, aka tokenization first)\nsample_embed = embedding(text_vectorizer([random_sentence]))\nsample_embed","metadata":{"execution":{"iopub.status.busy":"2024-10-05T13:26:35.705526Z","iopub.execute_input":"2024-10-05T13:26:35.706013Z","iopub.status.idle":"2024-10-05T13:26:35.769508Z","shell.execute_reply.started":"2024-10-05T13:26:35.705970Z","shell.execute_reply":"2024-10-05T13:26:35.768133Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"trial over bitch","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}